{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 006\n",
    "\n",
    "In this experiment, we try out an architecture involving a locally-connected RNN layer to see if it improves the block landing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from custom_layers import PointwiseRNN2d\n",
    "\n",
    "# If running this from VS Code, launch a TensorBoard session on the folder runs/experiment_006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockLandingDataset(Dataset):\n",
    "    def __init__(self, path: str):\n",
    "        self.path = path\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError()\n",
    "        with os.scandir(self.path) as it:\n",
    "            entry: os.DirEntry = next(iter(it))\n",
    "            _, self.ext = os.path.splitext(entry.name)\n",
    "            self.highest_index = max((int(Path(file.path).stem) for file in it))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.highest_index + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = os.path.join(self.path, f\"{idx}{self.ext}\")\n",
    "        if not os.path.exists(file):\n",
    "            raise IndexError()\n",
    "        boards = np.load(file)\n",
    "        assert boards.shape[0] == 4\n",
    "        x = boards[:3] # Just take the first two frames as that's what will be input to the main model\n",
    "        b1 = boards[2] # We can identify a block landing by the fact that a block spawns in the next time step\n",
    "        b2 = boards[3]\n",
    "        y = (np.all(b1[0] == 0) & np.any(b2[0] == 1)).astype(np.float32)\n",
    "        x, y = torch.tensor(x), torch.tensor(y)\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 22, 10]) torch.int32\n",
      "torch.Size([4]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BlockLandingDataset(os.path.join(\"data\", \"block_landing\", \"train\"))\n",
    "test_dataset = BlockLandingDataset(os.path.join(\"data\", \"block_landing\", \"test\"))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "print(x.shape, x.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockLandingModel(nn.Module):\n",
    "    \"\"\"Predicts whether a block has landed.\n",
    "\n",
    "    Inputs:\n",
    "        x: Tensor of int32 of shape (batch_size, seq_length, height, width). height = 22 and width = 10 are the dimensions of the game\n",
    "           board. seq_length = 2 is the number of game board inputs. The entries should be 0 for empty cells and 1 for blocks.\n",
    "    \n",
    "    Returns: float32 scalar, logit for block landing prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 5, 3)\n",
    "        self.rnn = nn.RNN(5, 5)\n",
    "        self.norm = nn.BatchNorm1d(5)\n",
    "        self.lin = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((1, 0, 4, 2, 3)) # Move sequence to dimension 0 and channels/classes to dimension 2\n",
    "\n",
    "        xs = [] # Split up the timesteps so we can apply convolution to them separately\n",
    "        for i in range(x.shape[0]):\n",
    "            x_i = x[i]\n",
    "            x_i = F.relu(self.conv(F.pad(x_i, (1, 1, 1, 1)))) # Pad height and width (the last 2 dimensions) with zeroes to represent the board boundaries\n",
    "            x_i = F.avg_pool2d(x_i, kernel_size=x.shape[-2:]).squeeze(-1).squeeze(-1)\n",
    "            x_i = x_i.unsqueeze(0)\n",
    "            xs.append(x_i)\n",
    "\n",
    "        x = torch.concat(xs)\n",
    "        x, rnn_state = self.rnn(x)\n",
    "        x = x[-1] # Just take last predicted state\n",
    "        x = self.norm(x)\n",
    "        logits = self.lin(x).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlockLandingModel(\n",
      "  (conv): Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (rnn): RNN(5, 5)\n",
      "  (norm): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n",
      "Predicted states: 0.5695329308509827\n"
     ]
    }
   ],
   "source": [
    "model = BlockLandingModel().to(device)\n",
    "print(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    logits = model(X)[0]\n",
    "    preds = F.sigmoid(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    avg_loss = 0.0\n",
    "    correct = 0.0\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metric calculations\n",
    "        avg_loss += loss.item()\n",
    "        classes = (pred >= 0).type(torch.float)\n",
    "        correct += (classes == y).type(torch.float).mean().item()\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    avg_loss /= num_batches\n",
    "    correct /= num_batches\n",
    "    print(f\"Training accuracy: {(100*correct):>0.1f}%\")\n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"acc\": correct,\n",
    "    }\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            classes = (pred >= 0).type(torch.float)\n",
    "            correct += (classes == y).type(torch.float).mean().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= num_batches\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return {\n",
    "        \"loss\": test_loss,\n",
    "        \"acc\": correct,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithPointwiseRnn(nn.Module):\n",
    "    \"\"\"This model has a locally connected RNN instead of a dense one operating on global average pools.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 5, 3)\n",
    "        self.rnn = PointwiseRNN2d(5, 5)\n",
    "        #self.norm = nn.BatchNorm1d(5)\n",
    "        self.lin = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((1, 0, 4, 2, 3)) # Move sequence to dimension 0 and channels/classes to dimension 2\n",
    "        x1, x2, x3 = x # Split up the two timesteps so we can apply convolution to them both separately\n",
    "\n",
    "        x1 = F.relu(self.conv(F.pad(x1, (1, 1, 1, 1)))) # Pad height and width (the last 2 dimensions) with zeroes to represent the board boundaries\n",
    "        x2 = F.relu(self.conv(F.pad(x2, (1, 1, 1, 1))))\n",
    "        x3 = F.relu(self.conv(F.pad(x3, (1, 1, 1, 1))))\n",
    "\n",
    "        x = torch.concat((x1.unsqueeze(0), x2.unsqueeze(0), x3.unsqueeze(0)))\n",
    "        x, rnn_state = self.rnn(x)\n",
    "        x = x[-1] # Just take last predicted state\n",
    "        #x = self.norm(x)\n",
    "        x = F.avg_pool2d(x, kernel_size=x.shape[-2:]).squeeze(-1).squeeze(-1)\n",
    "        logits = self.lin(x).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'baseline' has 171 parameters.\n",
      "Model 'pointwise_rnn' has 161 parameters.\n",
      "Training model 'baseline'...\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "loss: 0.836069  [    4/  183]\n",
      "loss: 0.662130  [   84/  183]\n",
      "loss: 0.478500  [  164/  183]\n",
      "Training accuracy: 65.0%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.711740 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.419497  [    4/  183]\n",
      "loss: 0.678149  [   84/  183]\n",
      "loss: 0.438411  [  164/  183]\n",
      "Training accuracy: 69.9%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.605260 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.425502  [    4/  183]\n",
      "loss: 0.610328  [   84/  183]\n",
      "loss: 0.743086  [  164/  183]\n",
      "Training accuracy: 69.0%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.567052 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.394889  [    4/  183]\n",
      "loss: 0.347516  [   84/  183]\n",
      "loss: 0.447395  [  164/  183]\n",
      "Training accuracy: 71.0%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.584078 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.419473  [    4/  183]\n",
      "loss: 0.451510  [   84/  183]\n",
      "loss: 0.556072  [  164/  183]\n",
      "Training accuracy: 70.5%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.533381 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.692167  [    4/  183]\n",
      "loss: 0.603095  [   84/  183]\n",
      "loss: 0.470812  [  164/  183]\n",
      "Training accuracy: 72.3%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.495070 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.562869  [    4/  183]\n",
      "loss: 0.598430  [   84/  183]\n",
      "loss: 0.467460  [  164/  183]\n",
      "Training accuracy: 73.7%\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.447732 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.396609  [    4/  183]\n",
      "loss: 0.424229  [   84/  183]\n",
      "loss: 0.343483  [  164/  183]\n",
      "Training accuracy: 77.7%\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.457260 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.456873  [    4/  183]\n",
      "loss: 0.484223  [   84/  183]\n",
      "loss: 0.342206  [  164/  183]\n",
      "Training accuracy: 80.4%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.964088 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.641003  [    4/  183]\n",
      "loss: 0.338374  [   84/  183]\n",
      "loss: 0.448372  [  164/  183]\n",
      "Training accuracy: 81.5%\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.762362 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.399083  [    4/  183]\n",
      "loss: 0.294256  [   84/  183]\n",
      "loss: 0.784170  [  164/  183]\n",
      "Training accuracy: 81.9%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.612165 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.541734  [    4/  183]\n",
      "loss: 0.860989  [   84/  183]\n",
      "loss: 0.298901  [  164/  183]\n",
      "Training accuracy: 77.5%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 1.522488 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.903090  [    4/  183]\n",
      "loss: 0.658294  [   84/  183]\n",
      "loss: 0.640752  [  164/  183]\n",
      "Training accuracy: 78.4%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.363607 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.751937  [    4/  183]\n",
      "loss: 0.787057  [   84/  183]\n",
      "loss: 0.374796  [  164/  183]\n",
      "Training accuracy: 85.3%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 3.649085 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.234552  [    4/  183]\n",
      "loss: 0.321707  [   84/  183]\n",
      "loss: 0.437458  [  164/  183]\n",
      "Training accuracy: 82.4%\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 1.467653 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.182860  [    4/  183]\n",
      "loss: 0.330718  [   84/  183]\n",
      "loss: 0.244147  [  164/  183]\n",
      "Training accuracy: 84.2%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.116532 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.807140  [    4/  183]\n",
      "loss: 0.321074  [   84/  183]\n",
      "loss: 0.144521  [  164/  183]\n",
      "Training accuracy: 93.5%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.457308 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.267879  [    4/  183]\n",
      "loss: 0.378960  [   84/  183]\n",
      "loss: 0.250713  [  164/  183]\n",
      "Training accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.208371 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.085714  [    4/  183]\n",
      "loss: 0.737689  [   84/  183]\n",
      "loss: 0.089605  [  164/  183]\n",
      "Training accuracy: 88.0%\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.388457 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.232358  [    4/  183]\n",
      "loss: 0.098477  [   84/  183]\n",
      "loss: 0.278146  [  164/  183]\n",
      "Training accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.158962 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.077672  [    4/  183]\n",
      "loss: 0.204980  [   84/  183]\n",
      "loss: 0.606339  [  164/  183]\n",
      "Training accuracy: 89.5%\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.247083 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.153848  [    4/  183]\n",
      "loss: 0.328383  [   84/  183]\n",
      "loss: 0.298530  [  164/  183]\n",
      "Training accuracy: 87.0%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.160270 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.194105  [    4/  183]\n",
      "loss: 0.243785  [   84/  183]\n",
      "loss: 0.139339  [  164/  183]\n",
      "Training accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.094904 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.142736  [    4/  183]\n",
      "loss: 0.164450  [   84/  183]\n",
      "loss: 0.896676  [  164/  183]\n",
      "Training accuracy: 89.7%\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.464014 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.051562  [    4/  183]\n",
      "loss: 0.593715  [   84/  183]\n",
      "loss: 0.179282  [  164/  183]\n",
      "Training accuracy: 92.4%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.093985 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.300723  [    4/  183]\n",
      "loss: 0.722057  [   84/  183]\n",
      "loss: 0.089329  [  164/  183]\n",
      "Training accuracy: 87.9%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.078421 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.973817  [    4/  183]\n",
      "loss: 0.261044  [   84/  183]\n",
      "loss: 0.084511  [  164/  183]\n",
      "Training accuracy: 86.8%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.108392 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.220899  [    4/  183]\n",
      "loss: 0.131964  [   84/  183]\n",
      "loss: 0.657199  [  164/  183]\n",
      "Training accuracy: 85.0%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.158466 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.037554  [    4/  183]\n",
      "loss: 0.201634  [   84/  183]\n",
      "loss: 0.148935  [  164/  183]\n",
      "Training accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.087316 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.204923  [    4/  183]\n",
      "loss: 0.769574  [   84/  183]\n",
      "loss: 0.290717  [  164/  183]\n",
      "Training accuracy: 88.6%\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.401944 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.175090  [    4/  183]\n",
      "loss: 0.080621  [   84/  183]\n",
      "loss: 0.048434  [  164/  183]\n",
      "Training accuracy: 92.9%\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.246667 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.506094  [    4/  183]\n",
      "loss: 0.047490  [   84/  183]\n",
      "loss: 0.134064  [  164/  183]\n",
      "Training accuracy: 94.0%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.102223 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.549956  [    4/  183]\n",
      "loss: 0.948267  [   84/  183]\n",
      "loss: 0.103033  [  164/  183]\n",
      "Training accuracy: 89.9%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.089596 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.267563  [    4/  183]\n",
      "loss: 0.152977  [   84/  183]\n",
      "loss: 0.105580  [  164/  183]\n",
      "Training accuracy: 94.0%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.077451 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.104747  [    4/  183]\n",
      "loss: 0.145369  [   84/  183]\n",
      "loss: 0.243318  [  164/  183]\n",
      "Training accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.109840 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.177771  [    4/  183]\n",
      "loss: 0.946889  [   84/  183]\n",
      "loss: 0.117674  [  164/  183]\n",
      "Training accuracy: 88.9%\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.260334 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.075813  [    4/  183]\n",
      "loss: 0.727717  [   84/  183]\n",
      "loss: 0.033221  [  164/  183]\n",
      "Training accuracy: 89.7%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.053947 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.099571  [    4/  183]\n",
      "loss: 0.108713  [   84/  183]\n",
      "loss: 0.023098  [  164/  183]\n",
      "Training accuracy: 94.0%\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.193188 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.107366  [    4/  183]\n",
      "loss: 0.026160  [   84/  183]\n",
      "loss: 0.023199  [  164/  183]\n",
      "Training accuracy: 88.4%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.051210 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.678075  [    4/  183]\n",
      "loss: 0.079227  [   84/  183]\n",
      "loss: 0.028580  [  164/  183]\n",
      "Training accuracy: 87.3%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.087318 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.191644  [    4/  183]\n",
      "loss: 0.127013  [   84/  183]\n",
      "loss: 0.100704  [  164/  183]\n",
      "Training accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.043032 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.099579  [    4/  183]\n",
      "loss: 0.091375  [   84/  183]\n",
      "loss: 0.109979  [  164/  183]\n",
      "Training accuracy: 93.5%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.043217 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.107945  [    4/  183]\n",
      "loss: 0.017287  [   84/  183]\n",
      "loss: 0.118142  [  164/  183]\n",
      "Training accuracy: 95.5%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.104712 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.083078  [    4/  183]\n",
      "loss: 1.090482  [   84/  183]\n",
      "loss: 0.121693  [  164/  183]\n",
      "Training accuracy: 88.0%\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.142643 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.078305  [    4/  183]\n",
      "loss: 0.121691  [   84/  183]\n",
      "loss: 0.024819  [  164/  183]\n",
      "Training accuracy: 95.7%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.081606 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.104716  [    4/  183]\n",
      "loss: 0.082086  [   84/  183]\n",
      "loss: 0.021849  [  164/  183]\n",
      "Training accuracy: 92.4%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.430663 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.057655  [    4/  183]\n",
      "loss: 0.032811  [   84/  183]\n",
      "loss: 0.013456  [  164/  183]\n",
      "Training accuracy: 94.6%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.030974 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.825232  [    4/  183]\n",
      "loss: 0.718074  [   84/  183]\n",
      "loss: 0.090884  [  164/  183]\n",
      "Training accuracy: 88.0%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.071868 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.173487  [    4/  183]\n",
      "loss: 0.202679  [   84/  183]\n",
      "loss: 0.858790  [  164/  183]\n",
      "Training accuracy: 92.9%\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.157552 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.013928  [    4/  183]\n",
      "loss: 0.091891  [   84/  183]\n",
      "loss: 0.081970  [  164/  183]\n",
      "Training accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.048284 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.160668  [    4/  183]\n",
      "loss: 0.017036  [   84/  183]\n",
      "loss: 0.015618  [  164/  183]\n",
      "Training accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.687029 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.201276  [    4/  183]\n",
      "loss: 0.017723  [   84/  183]\n",
      "loss: 0.041619  [  164/  183]\n",
      "Training accuracy: 94.6%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.048362 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.229844  [    4/  183]\n",
      "loss: 0.116707  [   84/  183]\n",
      "loss: 0.150766  [  164/  183]\n",
      "Training accuracy: 90.2%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.124857 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.079523  [    4/  183]\n",
      "loss: 0.058358  [   84/  183]\n",
      "loss: 0.068441  [  164/  183]\n",
      "Training accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.069425 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.225018  [    4/  183]\n",
      "loss: 0.097218  [   84/  183]\n",
      "loss: 0.169518  [  164/  183]\n",
      "Training accuracy: 92.4%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.057038 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.218639  [    4/  183]\n",
      "loss: 0.061299  [   84/  183]\n",
      "loss: 0.022408  [  164/  183]\n",
      "Training accuracy: 93.5%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.046960 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.086587  [    4/  183]\n",
      "loss: 0.871296  [   84/  183]\n",
      "loss: 0.011574  [  164/  183]\n",
      "Training accuracy: 94.6%\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.250125 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.008583  [    4/  183]\n",
      "loss: 0.085879  [   84/  183]\n",
      "loss: 0.767788  [  164/  183]\n",
      "Training accuracy: 94.0%\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.205269 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.065879  [    4/  183]\n",
      "loss: 0.010311  [   84/  183]\n",
      "loss: 0.666171  [  164/  183]\n",
      "Training accuracy: 92.4%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.026471 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.016929  [    4/  183]\n",
      "loss: 0.074820  [   84/  183]\n",
      "loss: 1.186129  [  164/  183]\n",
      "Training accuracy: 97.3%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.721879 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.040312  [    4/  183]\n",
      "loss: 0.091221  [   84/  183]\n",
      "loss: 0.053989  [  164/  183]\n",
      "Training accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.120460 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.106639  [    4/  183]\n",
      "loss: 0.134037  [   84/  183]\n",
      "loss: 0.650153  [  164/  183]\n",
      "Training accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.076559 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.156977  [    4/  183]\n",
      "loss: 0.096282  [   84/  183]\n",
      "loss: 0.367670  [  164/  183]\n",
      "Training accuracy: 88.6%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.049232 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.355833  [    4/  183]\n",
      "loss: 0.020190  [   84/  183]\n",
      "loss: 0.496226  [  164/  183]\n",
      "Training accuracy: 94.0%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.084633 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.020998  [    4/  183]\n",
      "loss: 0.719171  [   84/  183]\n",
      "loss: 0.120471  [  164/  183]\n",
      "Training accuracy: 93.5%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.048702 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.159784  [    4/  183]\n",
      "loss: 0.664782  [   84/  183]\n",
      "loss: 0.016327  [  164/  183]\n",
      "Training accuracy: 85.1%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.053564 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.020659  [    4/  183]\n",
      "loss: 0.636568  [   84/  183]\n",
      "loss: 0.006523  [  164/  183]\n",
      "Training accuracy: 92.9%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.019041 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.168699  [    4/  183]\n",
      "loss: 0.009521  [   84/  183]\n",
      "loss: 0.007621  [  164/  183]\n",
      "Training accuracy: 96.0%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.036684 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.008246  [    4/  183]\n",
      "loss: 0.061537  [   84/  183]\n",
      "loss: 0.012331  [  164/  183]\n",
      "Training accuracy: 94.0%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.034991 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.091956  [    4/  183]\n",
      "loss: 0.081232  [   84/  183]\n",
      "loss: 0.072636  [  164/  183]\n",
      "Training accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.079324 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.066237  [    4/  183]\n",
      "loss: 0.047584  [   84/  183]\n",
      "loss: 0.007766  [  164/  183]\n",
      "Training accuracy: 96.7%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.031620 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.029048  [    4/  183]\n",
      "loss: 1.118282  [   84/  183]\n",
      "loss: 0.329794  [  164/  183]\n",
      "Training accuracy: 88.0%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.017692 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.072972  [    4/  183]\n",
      "loss: 0.058952  [   84/  183]\n",
      "loss: 0.093486  [  164/  183]\n",
      "Training accuracy: 95.1%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.018822 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.068705  [    4/  183]\n",
      "loss: 0.007350  [   84/  183]\n",
      "loss: 0.055376  [  164/  183]\n",
      "Training accuracy: 96.6%\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.439267 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.149471  [    4/  183]\n",
      "loss: 0.085443  [   84/  183]\n",
      "loss: 0.056699  [  164/  183]\n",
      "Training accuracy: 92.9%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.029081 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.195301  [    4/  183]\n",
      "loss: 0.008911  [   84/  183]\n",
      "loss: 0.113939  [  164/  183]\n",
      "Training accuracy: 92.4%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.017675 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.050278  [    4/  183]\n",
      "loss: 0.057856  [   84/  183]\n",
      "loss: 0.055467  [  164/  183]\n",
      "Training accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.739719 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.074836  [    4/  183]\n",
      "loss: 0.285606  [   84/  183]\n",
      "loss: 0.006953  [  164/  183]\n",
      "Training accuracy: 94.6%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.020532 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.113372  [    4/  183]\n",
      "loss: 0.191164  [   84/  183]\n",
      "loss: 1.261829  [  164/  183]\n",
      "Training accuracy: 94.6%\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.387789 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.004580  [    4/  183]\n",
      "loss: 0.248715  [   84/  183]\n",
      "loss: 0.092537  [  164/  183]\n",
      "Training accuracy: 87.5%\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.134780 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.043951  [    4/  183]\n",
      "loss: 0.975966  [   84/  183]\n",
      "loss: 0.020440  [  164/  183]\n",
      "Training accuracy: 93.5%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.037003 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.044376  [    4/  183]\n",
      "loss: 0.064740  [   84/  183]\n",
      "loss: 0.058223  [  164/  183]\n",
      "Training accuracy: 96.2%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.022872 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.115016  [    4/  183]\n",
      "loss: 0.060429  [   84/  183]\n",
      "loss: 0.140002  [  164/  183]\n",
      "Training accuracy: 93.8%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.865819 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.115509  [    4/  183]\n",
      "loss: 1.017674  [   84/  183]\n",
      "loss: 0.007217  [  164/  183]\n",
      "Training accuracy: 95.7%\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.581385 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.067762  [    4/  183]\n",
      "loss: 0.012930  [   84/  183]\n",
      "loss: 0.132129  [  164/  183]\n",
      "Training accuracy: 95.7%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.015274 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.057731  [    4/  183]\n",
      "loss: 0.056320  [   84/  183]\n",
      "loss: 0.007894  [  164/  183]\n",
      "Training accuracy: 93.1%\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.105185 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.317685  [    4/  183]\n",
      "loss: 0.049398  [   84/  183]\n",
      "loss: 0.470897  [  164/  183]\n",
      "Training accuracy: 89.7%\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.172842 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.079542  [    4/  183]\n",
      "loss: 0.057449  [   84/  183]\n",
      "loss: 0.040936  [  164/  183]\n",
      "Training accuracy: 95.7%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.040834 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.031950  [    4/  183]\n",
      "loss: 0.050532  [   84/  183]\n",
      "loss: 0.056265  [  164/  183]\n",
      "Training accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.441128 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.177132  [    4/  183]\n",
      "loss: 0.143730  [   84/  183]\n",
      "loss: 0.061171  [  164/  183]\n",
      "Training accuracy: 90.6%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 4.198810 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.089398  [    4/  183]\n",
      "loss: 0.099619  [   84/  183]\n",
      "loss: 0.222770  [  164/  183]\n",
      "Training accuracy: 86.6%\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.184059 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.021942  [    4/  183]\n",
      "loss: 0.051983  [   84/  183]\n",
      "loss: 0.099158  [  164/  183]\n",
      "Training accuracy: 94.2%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.126199 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.107122  [    4/  183]\n",
      "loss: 0.028714  [   84/  183]\n",
      "loss: 0.067966  [  164/  183]\n",
      "Training accuracy: 96.2%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.480850 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.021568  [    4/  183]\n",
      "loss: 0.005599  [   84/  183]\n",
      "loss: 0.143352  [  164/  183]\n",
      "Training accuracy: 97.8%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.076985 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.045596  [    4/  183]\n",
      "loss: 0.041749  [   84/  183]\n",
      "loss: 0.901688  [  164/  183]\n",
      "Training accuracy: 94.0%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.014908 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.012492  [    4/  183]\n",
      "loss: 0.046894  [   84/  183]\n",
      "loss: 0.039400  [  164/  183]\n",
      "Training accuracy: 95.7%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.019079 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.026527  [    4/  183]\n",
      "loss: 0.055093  [   84/  183]\n",
      "loss: 1.101481  [  164/  183]\n",
      "Training accuracy: 89.9%\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.021407 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.038530  [    4/  183]\n",
      "loss: 0.046869  [   84/  183]\n",
      "loss: 0.416305  [  164/  183]\n",
      "Training accuracy: 96.7%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.099428 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.035151  [    4/  183]\n",
      "loss: 0.006174  [   84/  183]\n",
      "loss: 0.369107  [  164/  183]\n",
      "Training accuracy: 92.9%\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.335567 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.008518  [    4/  183]\n",
      "loss: 0.045739  [   84/  183]\n",
      "loss: 0.041108  [  164/  183]\n",
      "Training accuracy: 90.4%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.161830 \n",
      "\n",
      "Done!\n",
      "Training model 'pointwise_rnn'...\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "loss: 0.635610  [    4/  183]\n",
      "loss: 0.644760  [   84/  183]\n",
      "loss: 0.800639  [  164/  183]\n",
      "Training accuracy: 51.1%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.705041 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.693681  [    4/  183]\n",
      "loss: 0.743950  [   84/  183]\n",
      "loss: 0.719036  [  164/  183]\n",
      "Training accuracy: 50.5%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.699133 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.690896  [    4/  183]\n",
      "loss: 0.692111  [   84/  183]\n",
      "loss: 0.745967  [  164/  183]\n",
      "Training accuracy: 50.9%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.697329 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.674156  [    4/  183]\n",
      "loss: 0.690663  [   84/  183]\n",
      "loss: 0.711299  [  164/  183]\n",
      "Training accuracy: 50.7%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.695033 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.677041  [    4/  183]\n",
      "loss: 0.693893  [   84/  183]\n",
      "loss: 0.696184  [  164/  183]\n",
      "Training accuracy: 50.9%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.694618 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.678551  [    4/  183]\n",
      "loss: 0.678588  [   84/  183]\n",
      "loss: 0.691022  [  164/  183]\n",
      "Training accuracy: 50.9%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.694648 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.658970  [    4/  183]\n",
      "loss: 0.711622  [   84/  183]\n",
      "loss: 0.700104  [  164/  183]\n",
      "Training accuracy: 50.7%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.693539 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.676200  [    4/  183]\n",
      "loss: 0.685414  [   84/  183]\n",
      "loss: 0.703336  [  164/  183]\n",
      "Training accuracy: 50.7%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.692211 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.685143  [    4/  183]\n",
      "loss: 0.688226  [   84/  183]\n",
      "loss: 0.694286  [  164/  183]\n",
      "Training accuracy: 54.2%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.690399 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.668021  [    4/  183]\n",
      "loss: 0.732608  [   84/  183]\n",
      "loss: 0.704798  [  164/  183]\n",
      "Training accuracy: 50.5%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.692062 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.668363  [    4/  183]\n",
      "loss: 0.686023  [   84/  183]\n",
      "loss: 0.691470  [  164/  183]\n",
      "Training accuracy: 58.5%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.689581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.684963  [    4/  183]\n",
      "loss: 0.723572  [   84/  183]\n",
      "loss: 0.705029  [  164/  183]\n",
      "Training accuracy: 50.7%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.692271 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.692834  [    4/  183]\n",
      "loss: 0.688562  [   84/  183]\n",
      "loss: 0.685564  [  164/  183]\n",
      "Training accuracy: 51.3%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.691695 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.693201  [    4/  183]\n",
      "loss: 0.687295  [   84/  183]\n",
      "loss: 0.681743  [  164/  183]\n",
      "Training accuracy: 51.8%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.691459 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.693728  [    4/  183]\n",
      "loss: 0.701668  [   84/  183]\n",
      "loss: 0.678686  [  164/  183]\n",
      "Training accuracy: 54.9%\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.691845 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.687837  [    4/  183]\n",
      "loss: 0.658518  [   84/  183]\n",
      "loss: 0.694915  [  164/  183]\n",
      "Training accuracy: 55.8%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.688982 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.696877  [    4/  183]\n",
      "loss: 0.731458  [   84/  183]\n",
      "loss: 0.670064  [  164/  183]\n",
      "Training accuracy: 50.4%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.687950 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.678951  [    4/  183]\n",
      "loss: 0.687486  [   84/  183]\n",
      "loss: 0.688379  [  164/  183]\n",
      "Training accuracy: 56.3%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.688017 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.690289  [    4/  183]\n",
      "loss: 0.687185  [   84/  183]\n",
      "loss: 0.718834  [  164/  183]\n",
      "Training accuracy: 50.5%\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.689814 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.689754  [    4/  183]\n",
      "loss: 0.674800  [   84/  183]\n",
      "loss: 0.716203  [  164/  183]\n",
      "Training accuracy: 56.0%\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.690299 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.659610  [    4/  183]\n",
      "loss: 0.688860  [   84/  183]\n",
      "loss: 0.660903  [  164/  183]\n",
      "Training accuracy: 56.2%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.686536 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.674161  [    4/  183]\n",
      "loss: 0.717647  [   84/  183]\n",
      "loss: 0.680265  [  164/  183]\n",
      "Training accuracy: 53.4%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.687102 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.657277  [    4/  183]\n",
      "loss: 0.675669  [   84/  183]\n",
      "loss: 0.668519  [  164/  183]\n",
      "Training accuracy: 56.0%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.686690 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.669273  [    4/  183]\n",
      "loss: 0.692627  [   84/  183]\n",
      "loss: 0.670186  [  164/  183]\n",
      "Training accuracy: 59.1%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.685523 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.705326  [    4/  183]\n",
      "loss: 0.699664  [   84/  183]\n",
      "loss: 0.716867  [  164/  183]\n",
      "Training accuracy: 59.8%\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.688138 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.677228  [    4/  183]\n",
      "loss: 0.645511  [   84/  183]\n",
      "loss: 0.709196  [  164/  183]\n",
      "Training accuracy: 53.3%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.684842 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.672826  [    4/  183]\n",
      "loss: 0.684858  [   84/  183]\n",
      "loss: 0.699900  [  164/  183]\n",
      "Training accuracy: 60.7%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.684333 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.679401  [    4/  183]\n",
      "loss: 0.675621  [   84/  183]\n",
      "loss: 0.671846  [  164/  183]\n",
      "Training accuracy: 63.8%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.684534 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.692048  [    4/  183]\n",
      "loss: 0.661782  [   84/  183]\n",
      "loss: 0.674101  [  164/  183]\n",
      "Training accuracy: 64.5%\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.682959 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.700266  [    4/  183]\n",
      "loss: 0.666817  [   84/  183]\n",
      "loss: 0.688050  [  164/  183]\n",
      "Training accuracy: 54.3%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.682617 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.668482  [    4/  183]\n",
      "loss: 0.684828  [   84/  183]\n",
      "loss: 0.672983  [  164/  183]\n",
      "Training accuracy: 62.7%\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.686235 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.684790  [    4/  183]\n",
      "loss: 0.669430  [   84/  183]\n",
      "loss: 0.682984  [  164/  183]\n",
      "Training accuracy: 56.7%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.682657 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.662701  [    4/  183]\n",
      "loss: 0.705171  [   84/  183]\n",
      "loss: 0.693583  [  164/  183]\n",
      "Training accuracy: 57.6%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.684455 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.698110  [    4/  183]\n",
      "loss: 0.657426  [   84/  183]\n",
      "loss: 0.677461  [  164/  183]\n",
      "Training accuracy: 56.3%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.679221 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.648473  [    4/  183]\n",
      "loss: 0.682113  [   84/  183]\n",
      "loss: 0.686860  [  164/  183]\n",
      "Training accuracy: 59.8%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.680726 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.702573  [    4/  183]\n",
      "loss: 0.695586  [   84/  183]\n",
      "loss: 0.649034  [  164/  183]\n",
      "Training accuracy: 61.8%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.680641 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.647292  [    4/  183]\n",
      "loss: 0.680785  [   84/  183]\n",
      "loss: 0.690212  [  164/  183]\n",
      "Training accuracy: 61.1%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.677801 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.673667  [    4/  183]\n",
      "loss: 0.697458  [   84/  183]\n",
      "loss: 0.662634  [  164/  183]\n",
      "Training accuracy: 62.5%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.677229 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.646119  [    4/  183]\n",
      "loss: 0.696128  [   84/  183]\n",
      "loss: 0.710874  [  164/  183]\n",
      "Training accuracy: 56.7%\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.679316 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.699557  [    4/  183]\n",
      "loss: 0.697711  [   84/  183]\n",
      "loss: 0.660248  [  164/  183]\n",
      "Training accuracy: 60.1%\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.675896 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.643768  [    4/  183]\n",
      "loss: 0.681222  [   84/  183]\n",
      "loss: 0.684666  [  164/  183]\n",
      "Training accuracy: 66.3%\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.686687 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.633830  [    4/  183]\n",
      "loss: 0.627383  [   84/  183]\n",
      "loss: 0.667812  [  164/  183]\n",
      "Training accuracy: 59.8%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.676522 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.704334  [    4/  183]\n",
      "loss: 0.596319  [   84/  183]\n",
      "loss: 0.655784  [  164/  183]\n",
      "Training accuracy: 65.6%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.675600 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.692783  [    4/  183]\n",
      "loss: 0.664450  [   84/  183]\n",
      "loss: 0.633644  [  164/  183]\n",
      "Training accuracy: 61.8%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.677216 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.646144  [    4/  183]\n",
      "loss: 0.692167  [   84/  183]\n",
      "loss: 0.612189  [  164/  183]\n",
      "Training accuracy: 59.2%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.674004 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.686414  [    4/  183]\n",
      "loss: 0.650890  [   84/  183]\n",
      "loss: 0.666465  [  164/  183]\n",
      "Training accuracy: 65.4%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.673926 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.681534  [    4/  183]\n",
      "loss: 0.698520  [   84/  183]\n",
      "loss: 0.690177  [  164/  183]\n",
      "Training accuracy: 62.9%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.672004 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.657396  [    4/  183]\n",
      "loss: 0.649514  [   84/  183]\n",
      "loss: 0.678688  [  164/  183]\n",
      "Training accuracy: 65.2%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.677771 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.704370  [    4/  183]\n",
      "loss: 0.615094  [   84/  183]\n",
      "loss: 0.690206  [  164/  183]\n",
      "Training accuracy: 62.0%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.670791 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.653773  [    4/  183]\n",
      "loss: 0.673344  [   84/  183]\n",
      "loss: 0.645603  [  164/  183]\n",
      "Training accuracy: 63.8%\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.674897 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.661623  [    4/  183]\n",
      "loss: 0.689293  [   84/  183]\n",
      "loss: 0.675310  [  164/  183]\n",
      "Training accuracy: 62.9%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.665634 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.636934  [    4/  183]\n",
      "loss: 0.730815  [   84/  183]\n",
      "loss: 0.625652  [  164/  183]\n",
      "Training accuracy: 64.5%\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.673268 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.676036  [    4/  183]\n",
      "loss: 0.699800  [   84/  183]\n",
      "loss: 0.646752  [  164/  183]\n",
      "Training accuracy: 65.0%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.662369 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.669420  [    4/  183]\n",
      "loss: 0.636748  [   84/  183]\n",
      "loss: 0.672946  [  164/  183]\n",
      "Training accuracy: 66.1%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.668863 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.635929  [    4/  183]\n",
      "loss: 0.668709  [   84/  183]\n",
      "loss: 0.636476  [  164/  183]\n",
      "Training accuracy: 65.6%\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.670078 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.611815  [    4/  183]\n",
      "loss: 0.596109  [   84/  183]\n",
      "loss: 0.681350  [  164/  183]\n",
      "Training accuracy: 63.9%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.667673 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.642345  [    4/  183]\n",
      "loss: 0.738961  [   84/  183]\n",
      "loss: 0.705885  [  164/  183]\n",
      "Training accuracy: 63.8%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.670730 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.638840  [    4/  183]\n",
      "loss: 0.629807  [   84/  183]\n",
      "loss: 0.613180  [  164/  183]\n",
      "Training accuracy: 67.4%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.662536 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.682939  [    4/  183]\n",
      "loss: 0.595563  [   84/  183]\n",
      "loss: 0.669611  [  164/  183]\n",
      "Training accuracy: 61.1%\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.661573 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.636208  [    4/  183]\n",
      "loss: 0.610151  [   84/  183]\n",
      "loss: 0.628302  [  164/  183]\n",
      "Training accuracy: 67.8%\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.675160 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.617902  [    4/  183]\n",
      "loss: 0.677619  [   84/  183]\n",
      "loss: 0.605749  [  164/  183]\n",
      "Training accuracy: 66.7%\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.679639 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.688042  [    4/  183]\n",
      "loss: 0.739164  [   84/  183]\n",
      "loss: 0.671331  [  164/  183]\n",
      "Training accuracy: 65.2%\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.651457 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.695789  [    4/  183]\n",
      "loss: 0.683260  [   84/  183]\n",
      "loss: 0.661742  [  164/  183]\n",
      "Training accuracy: 64.3%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.661511 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.661889  [    4/  183]\n",
      "loss: 0.781710  [   84/  183]\n",
      "loss: 0.830975  [  164/  183]\n",
      "Training accuracy: 63.4%\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.661766 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.663820  [    4/  183]\n",
      "loss: 0.564268  [   84/  183]\n",
      "loss: 0.769023  [  164/  183]\n",
      "Training accuracy: 68.8%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.656696 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.605664  [    4/  183]\n",
      "loss: 0.553100  [   84/  183]\n",
      "loss: 0.669246  [  164/  183]\n",
      "Training accuracy: 62.0%\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.650387 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.624517  [    4/  183]\n",
      "loss: 0.740663  [   84/  183]\n",
      "loss: 0.704067  [  164/  183]\n",
      "Training accuracy: 68.1%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.650267 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.673083  [    4/  183]\n",
      "loss: 0.611763  [   84/  183]\n",
      "loss: 0.563338  [  164/  183]\n",
      "Training accuracy: 69.2%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.653512 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.650098  [    4/  183]\n",
      "loss: 0.690700  [   84/  183]\n",
      "loss: 0.584984  [  164/  183]\n",
      "Training accuracy: 69.6%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.650683 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.593396  [    4/  183]\n",
      "loss: 0.669539  [   84/  183]\n",
      "loss: 0.637875  [  164/  183]\n",
      "Training accuracy: 66.8%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.653437 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.686956  [    4/  183]\n",
      "loss: 0.741492  [   84/  183]\n",
      "loss: 0.683508  [  164/  183]\n",
      "Training accuracy: 71.2%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.645161 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.693444  [    4/  183]\n",
      "loss: 0.645744  [   84/  183]\n",
      "loss: 0.697850  [  164/  183]\n",
      "Training accuracy: 66.5%\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.638630 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.707018  [    4/  183]\n",
      "loss: 0.664153  [   84/  183]\n",
      "loss: 0.710137  [  164/  183]\n",
      "Training accuracy: 70.1%\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.648057 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.658226  [    4/  183]\n",
      "loss: 0.558170  [   84/  183]\n",
      "loss: 0.651009  [  164/  183]\n",
      "Training accuracy: 67.4%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.636305 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.648897  [    4/  183]\n",
      "loss: 0.741564  [   84/  183]\n",
      "loss: 0.654494  [  164/  183]\n",
      "Training accuracy: 67.2%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.648439 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.611996  [    4/  183]\n",
      "loss: 0.596244  [   84/  183]\n",
      "loss: 0.706780  [  164/  183]\n",
      "Training accuracy: 66.8%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.641606 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.650213  [    4/  183]\n",
      "loss: 0.490109  [   84/  183]\n",
      "loss: 0.674975  [  164/  183]\n",
      "Training accuracy: 67.9%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.654289 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.639779  [    4/  183]\n",
      "loss: 0.698004  [   84/  183]\n",
      "loss: 0.654779  [  164/  183]\n",
      "Training accuracy: 64.3%\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.643300 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.640229  [    4/  183]\n",
      "loss: 0.700145  [   84/  183]\n",
      "loss: 0.665710  [  164/  183]\n",
      "Training accuracy: 70.1%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.629478 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.693713  [    4/  183]\n",
      "loss: 0.636243  [   84/  183]\n",
      "loss: 0.473866  [  164/  183]\n",
      "Training accuracy: 66.7%\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.660368 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.574432  [    4/  183]\n",
      "loss: 0.556073  [   84/  183]\n",
      "loss: 0.596503  [  164/  183]\n",
      "Training accuracy: 68.3%\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.636526 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.773951  [    4/  183]\n",
      "loss: 0.688001  [   84/  183]\n",
      "loss: 0.682649  [  164/  183]\n",
      "Training accuracy: 67.2%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.635449 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.636009  [    4/  183]\n",
      "loss: 0.553673  [   84/  183]\n",
      "loss: 0.670867  [  164/  183]\n",
      "Training accuracy: 66.5%\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.661906 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.555782  [    4/  183]\n",
      "loss: 0.571677  [   84/  183]\n",
      "loss: 0.722947  [  164/  183]\n",
      "Training accuracy: 65.4%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.633857 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.653650  [    4/  183]\n",
      "loss: 0.680166  [   84/  183]\n",
      "loss: 0.711985  [  164/  183]\n",
      "Training accuracy: 69.2%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.619878 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.541701  [    4/  183]\n",
      "loss: 0.795150  [   84/  183]\n",
      "loss: 0.548701  [  164/  183]\n",
      "Training accuracy: 69.4%\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.660674 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.583728  [    4/  183]\n",
      "loss: 0.558865  [   84/  183]\n",
      "loss: 0.568057  [  164/  183]\n",
      "Training accuracy: 69.2%\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.637884 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.625541  [    4/  183]\n",
      "loss: 0.593766  [   84/  183]\n",
      "loss: 0.544297  [  164/  183]\n",
      "Training accuracy: 70.3%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.619606 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.580665  [    4/  183]\n",
      "loss: 0.745037  [   84/  183]\n",
      "loss: 0.609980  [  164/  183]\n",
      "Training accuracy: 69.0%\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.618798 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.503160  [    4/  183]\n",
      "loss: 0.545919  [   84/  183]\n",
      "loss: 0.758519  [  164/  183]\n",
      "Training accuracy: 65.6%\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.644749 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.662071  [    4/  183]\n",
      "loss: 0.584670  [   84/  183]\n",
      "loss: 0.593917  [  164/  183]\n",
      "Training accuracy: 69.4%\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.623774 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.516643  [    4/  183]\n",
      "loss: 0.712817  [   84/  183]\n",
      "loss: 0.542796  [  164/  183]\n",
      "Training accuracy: 67.2%\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.646988 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.910207  [    4/  183]\n",
      "loss: 0.475657  [   84/  183]\n",
      "loss: 0.458574  [  164/  183]\n",
      "Training accuracy: 69.0%\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.615693 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.647515  [    4/  183]\n",
      "loss: 0.705379  [   84/  183]\n",
      "loss: 0.591287  [  164/  183]\n",
      "Training accuracy: 67.8%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.629564 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.494684  [    4/  183]\n",
      "loss: 0.586387  [   84/  183]\n",
      "loss: 0.662145  [  164/  183]\n",
      "Training accuracy: 69.4%\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.637756 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.627153  [    4/  183]\n",
      "loss: 0.694466  [   84/  183]\n",
      "loss: 0.814001  [  164/  183]\n",
      "Training accuracy: 68.5%\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.628446 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.692025  [    4/  183]\n",
      "loss: 0.590213  [   84/  183]\n",
      "loss: 0.636875  [  164/  183]\n",
      "Training accuracy: 70.3%\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.600958 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.726751  [    4/  183]\n",
      "loss: 0.555868  [   84/  183]\n",
      "loss: 0.493451  [  164/  183]\n",
      "Training accuracy: 69.2%\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.619230 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.693995  [    4/  183]\n",
      "loss: 0.721395  [   84/  183]\n",
      "loss: 0.518472  [  164/  183]\n",
      "Training accuracy: 71.2%\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.630259 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.716622  [    4/  183]\n",
      "loss: 0.527183  [   84/  183]\n",
      "loss: 0.595353  [  164/  183]\n",
      "Training accuracy: 69.4%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.616274 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "batch_size = 4\n",
    "epochs = 100\n",
    "\n",
    "models = {\n",
    "    \"baseline\": BlockLandingModel().to(device),\n",
    "    \"pointwise_rnn\": ModelWithPointwiseRnn().to(device),\n",
    "}\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Model '{name}' has {count_parameters(model)} parameters.\")\n",
    "\n",
    "log_dir = os.path.join(\"runs\", \"experiment_006\")\n",
    "shutil.rmtree(log_dir, ignore_errors=True)\n",
    "\n",
    "for name, model in models.items():\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"Training model '{name}'...\")\n",
    "    log_subdir = os.path.join(log_dir, name + \"_\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    tb = SummaryWriter(log_subdir)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t}\\n-------------------------------\")\n",
    "        train_metrics = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_metrics = test_loop(test_dataloader, model, loss_fn)\n",
    "        tb.add_scalar(\"Loss/train\", train_metrics[\"loss\"], t)\n",
    "        tb.add_scalar(\"Accuracy/train\", train_metrics[\"acc\"], t)\n",
    "        tb.add_scalar(\"Loss/test\", test_metrics[\"loss\"], t)\n",
    "        tb.add_scalar(\"Accuracy/test\", test_metrics[\"acc\"], t)\n",
    "        for name, weight in model.named_parameters():\n",
    "            tb.add_histogram(f\"Weights/{name}\", weight, t)\n",
    "            tb.add_histogram(f\"Gradients/{name}\", weight.grad, t)\n",
    "\n",
    "    tb.close()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pointwise RNN takes a long time to train and takes many epochs to improve loss and accuracy. Ideally we should try varying the learning rate and other properties, but to do this in a reasonable time, we need a faster pointwise RNN implementation.\n",
    "\n",
    "Meanwhile I have had some other ideas about the possible architecture which I might try first. For example, we could try just using convolutions on the last frame to distil the spatial information, then a linear layer or two at the end. If we need to operate on multiple frames, we can just concatenate the tensors instead of treating them as a sequence. Avoiding RNNs may circumvent the training instability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
