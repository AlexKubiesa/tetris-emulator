{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 013\n",
    "\n",
    "In this experiment, we'll add random noise as an additional input to the Tetris emulator and see if it allows us to threshold the model output and still get believable, stochastic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordingDataset(Dataset):\n",
    "    def __init__(self, path: str):\n",
    "        self.path = path\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError()\n",
    "        with os.scandir(self.path) as it:\n",
    "            entry: os.DirEntry = next(iter(it))\n",
    "            _, self.ext = os.path.splitext(entry.name)\n",
    "            self.highest_index = max((int(Path(file.path).stem) for file in it), default=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.highest_index + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = os.path.join(self.path, f\"{idx}{self.ext}\")\n",
    "        if not os.path.exists(file):\n",
    "            raise IndexError()\n",
    "        boards = np.load(file)\n",
    "        x = torch.tensor(boards[-2]) # Ignore all boards except the last two\n",
    "        y = torch.tensor(boards[-1], dtype=torch.long)\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 22, 10]) torch.int32\n",
      "torch.Size([4, 22, 10]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "train_dataset = RecordingDataset(os.path.join(\"data\", \"tetris_emulator\", \"train\"))\n",
    "test_dataset = RecordingDataset(os.path.join(\"data\", \"tetris_emulator\", \"test\"))\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "print(x.shape, x.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrisModel(nn.Module):\n",
    "    \"\"\"Predicts the next state of the cells.\n",
    "\n",
    "    Inputs:\n",
    "        x: Tensor of int32 of shape (batch_size, height, width). height = 22 and width = 10 are the dimensions of the game\n",
    "           board. The entries should be 0 for empty cells and 1 for blocks.\n",
    "        z: Tensor of float32 of shape (batch_size,). Random noise, which this model discards.\n",
    "    \n",
    "    Returns: Tensor of float32 of shape (batch_size, height, width), logits for the new cells. Probabilities close to 0 (negative logits)\n",
    "             correspond to empty cells, and probabilities close to 1 (positive logits) correspond to blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Sequential(\n",
    "            nn.Conv2d(2, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.glob = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(160, 10)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(26, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 2, 1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, height, width = x.shape\n",
    "        \n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        x = self.loc(x)\n",
    "\n",
    "        x_glob = self.glob(x)\n",
    "        x_glob = x_glob[:, :, None, None] # Expand dims\n",
    "        x_glob = x_glob.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, x_glob), dim=1)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithRandomness(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.glob = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(160, 10)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(26, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 2, 1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, height, width = x.shape\n",
    "        \n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        z = z[:, None, None, None] # Expand dims to match x\n",
    "        z = z.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, z), dim=1)\n",
    "\n",
    "        x = self.loc(x)\n",
    "\n",
    "        x_glob = self.glob(x)\n",
    "        x_glob = x_glob[:, :, None, None] # Expand dims\n",
    "        x_glob = x_glob.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, x_glob), dim=1)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TetrisModel has 12828 parameters.\n",
      "ModelWithRandomness has 12972 parameters.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "for model_class in [TetrisModel, ModelWithRandomness]:\n",
    "    print(f\"{model_class.__name__} has {count_parameters(model_class())} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.3112, 0.4302, 0.3874, 0.3541, 0.3541, 0.3541, 0.3541, 0.3655,\n",
      "          0.3859, 0.4142],\n",
      "         [0.2824, 0.4075, 0.3468, 0.3324, 0.3324, 0.3324, 0.3324, 0.3311,\n",
      "          0.3320, 0.4735],\n",
      "         [0.2813, 0.4611, 0.3815, 0.4337, 0.4337, 0.4337, 0.4337, 0.4239,\n",
      "          0.4651, 0.4286],\n",
      "         [0.2441, 0.4674, 0.3870, 0.4244, 0.4244, 0.4244, 0.4244, 0.4631,\n",
      "          0.4360, 0.4361],\n",
      "         [0.2441, 0.4674, 0.3870, 0.4244, 0.4244, 0.4244, 0.4244, 0.4631,\n",
      "          0.4360, 0.4361],\n",
      "         [0.2441, 0.4674, 0.3870, 0.4244, 0.4244, 0.4244, 0.4244, 0.4631,\n",
      "          0.4360, 0.4361],\n",
      "         [0.2441, 0.4674, 0.3870, 0.4244, 0.4244, 0.4244, 0.4244, 0.4631,\n",
      "          0.4360, 0.4361],\n",
      "         [0.2441, 0.4674, 0.3870, 0.4244, 0.4244, 0.4244, 0.4244, 0.4631,\n",
      "          0.4360, 0.4361],\n",
      "         [0.2441, 0.4674, 0.3870, 0.4244, 0.4244, 0.4244, 0.4244, 0.4631,\n",
      "          0.4360, 0.4361],\n",
      "         [0.2293, 0.4477, 0.4055, 0.4012, 0.4058, 0.3811, 0.4254, 0.4631,\n",
      "          0.4360, 0.4361],\n",
      "         [0.2258, 0.4482, 0.3501, 0.5033, 0.4311, 0.3606, 0.4220, 0.4456,\n",
      "          0.4346, 0.4361],\n",
      "         [0.2854, 0.3984, 0.3742, 0.3871, 0.5044, 0.3352, 0.4382, 0.4447,\n",
      "          0.4609, 0.4361],\n",
      "         [0.2616, 0.4205, 0.4035, 0.3723, 0.3785, 0.3953, 0.3639, 0.3904,\n",
      "          0.4398, 0.4361],\n",
      "         [0.2224, 0.4240, 0.4390, 0.5511, 0.3251, 0.3250, 0.3522, 0.4267,\n",
      "          0.4277, 0.4361],\n",
      "         [0.2341, 0.4668, 0.4620, 0.5322, 0.3794, 0.4032, 0.3744, 0.4478,\n",
      "          0.4222, 0.4361],\n",
      "         [0.2413, 0.4257, 0.2875, 0.3892, 0.3497, 0.3016, 0.3868, 0.4678,\n",
      "          0.4294, 0.4361],\n",
      "         [0.2439, 0.4718, 0.3688, 0.4168, 0.3827, 0.4014, 0.4044, 0.4478,\n",
      "          0.4338, 0.4361],\n",
      "         [0.2441, 0.4674, 0.3870, 0.4244, 0.4244, 0.4244, 0.4244, 0.4631,\n",
      "          0.4360, 0.4361],\n",
      "         [0.2441, 0.4674, 0.3870, 0.4244, 0.4244, 0.4244, 0.4244, 0.4631,\n",
      "          0.4360, 0.4361],\n",
      "         [0.2368, 0.5105, 0.4764, 0.4232, 0.4232, 0.4232, 0.4232, 0.4404,\n",
      "          0.4248, 0.4519],\n",
      "         [0.2312, 0.4098, 0.4093, 0.3996, 0.3996, 0.3996, 0.3996, 0.4589,\n",
      "          0.4404, 0.3892],\n",
      "         [0.3150, 0.4183, 0.4226, 0.3849, 0.3849, 0.3849, 0.3849, 0.4031,\n",
      "          0.4648, 0.4803]],\n",
      "\n",
      "        [[0.6888, 0.5698, 0.6126, 0.6459, 0.6459, 0.6459, 0.6459, 0.6345,\n",
      "          0.6141, 0.5858],\n",
      "         [0.7176, 0.5925, 0.6532, 0.6676, 0.6676, 0.6676, 0.6676, 0.6689,\n",
      "          0.6680, 0.5265],\n",
      "         [0.7187, 0.5389, 0.6185, 0.5663, 0.5663, 0.5663, 0.5663, 0.5761,\n",
      "          0.5349, 0.5714],\n",
      "         [0.7559, 0.5326, 0.6130, 0.5756, 0.5756, 0.5756, 0.5756, 0.5369,\n",
      "          0.5640, 0.5639],\n",
      "         [0.7559, 0.5326, 0.6130, 0.5756, 0.5756, 0.5756, 0.5756, 0.5369,\n",
      "          0.5640, 0.5639],\n",
      "         [0.7559, 0.5326, 0.6130, 0.5756, 0.5756, 0.5756, 0.5756, 0.5369,\n",
      "          0.5640, 0.5639],\n",
      "         [0.7559, 0.5326, 0.6130, 0.5756, 0.5756, 0.5756, 0.5756, 0.5369,\n",
      "          0.5640, 0.5639],\n",
      "         [0.7559, 0.5326, 0.6130, 0.5756, 0.5756, 0.5756, 0.5756, 0.5369,\n",
      "          0.5640, 0.5639],\n",
      "         [0.7559, 0.5326, 0.6130, 0.5756, 0.5756, 0.5756, 0.5756, 0.5369,\n",
      "          0.5640, 0.5639],\n",
      "         [0.7707, 0.5523, 0.5945, 0.5988, 0.5942, 0.6189, 0.5746, 0.5369,\n",
      "          0.5640, 0.5639],\n",
      "         [0.7742, 0.5518, 0.6499, 0.4967, 0.5689, 0.6394, 0.5780, 0.5544,\n",
      "          0.5654, 0.5639],\n",
      "         [0.7146, 0.6016, 0.6258, 0.6129, 0.4956, 0.6648, 0.5618, 0.5553,\n",
      "          0.5391, 0.5639],\n",
      "         [0.7384, 0.5795, 0.5965, 0.6277, 0.6215, 0.6047, 0.6361, 0.6096,\n",
      "          0.5602, 0.5639],\n",
      "         [0.7776, 0.5760, 0.5610, 0.4489, 0.6749, 0.6750, 0.6478, 0.5733,\n",
      "          0.5723, 0.5639],\n",
      "         [0.7659, 0.5332, 0.5380, 0.4678, 0.6206, 0.5968, 0.6256, 0.5522,\n",
      "          0.5778, 0.5639],\n",
      "         [0.7587, 0.5743, 0.7125, 0.6108, 0.6503, 0.6984, 0.6132, 0.5322,\n",
      "          0.5706, 0.5639],\n",
      "         [0.7561, 0.5282, 0.6312, 0.5832, 0.6173, 0.5986, 0.5956, 0.5522,\n",
      "          0.5662, 0.5639],\n",
      "         [0.7559, 0.5326, 0.6130, 0.5756, 0.5756, 0.5756, 0.5756, 0.5369,\n",
      "          0.5640, 0.5639],\n",
      "         [0.7559, 0.5326, 0.6130, 0.5756, 0.5756, 0.5756, 0.5756, 0.5369,\n",
      "          0.5640, 0.5639],\n",
      "         [0.7632, 0.4895, 0.5236, 0.5768, 0.5768, 0.5768, 0.5768, 0.5596,\n",
      "          0.5752, 0.5481],\n",
      "         [0.7688, 0.5902, 0.5907, 0.6004, 0.6004, 0.6004, 0.6004, 0.5411,\n",
      "          0.5596, 0.6108],\n",
      "         [0.6850, 0.5817, 0.5774, 0.6151, 0.6151, 0.6151, 0.6151, 0.5969,\n",
      "          0.5352, 0.5197]]])\n"
     ]
    }
   ],
   "source": [
    "model = ModelWithRandomness().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.rand(batch_size)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_SPAWN_TYPES = 7\n",
    "\n",
    "def get_spawn_type(y):\n",
    "    if (y[1, :] == 0).all():\n",
    "        return 0 # I\n",
    "    if y[0, 3] == 1:\n",
    "        if y[1, 3] == 1:\n",
    "            return 1 # J\n",
    "        else:\n",
    "            if y[0, 5] == 1:\n",
    "                return 2 # T\n",
    "            else:\n",
    "                return 3 # Z\n",
    "    else:\n",
    "        if y[1, 3] == 1:\n",
    "            if y[0, 4] == 1:\n",
    "                return 4 # S\n",
    "            else:\n",
    "                return 5 # L\n",
    "        else:\n",
    "            return 6 # O\n",
    "\n",
    "\n",
    "def get_explanatory_randomness(X, y, use_oracle, onehot):\n",
    "    \"\"\"Given a batch of training examples X, returns a batch of random numbers that explain the block spawn type.\"\"\"\n",
    "    batch_size, height, width = X.shape\n",
    "\n",
    "    # Allocate output tensor\n",
    "    z_shape = (batch_size, NUM_SPAWN_TYPES) if onehot else (batch_size,)\n",
    "    z = torch.zeros(z_shape)\n",
    "\n",
    "    # Identify block spawns\n",
    "    spawns = (X[:, 0, :] == 0).all(-1) & (y[:, 0, :] == 1).any(-1)\n",
    "\n",
    "    # For each example, set randomness based on block spawn or at random\n",
    "    for i in range(batch_size):\n",
    "        spawn_type = get_spawn_type(y[i]) if (spawns[i] and use_oracle) else random.randrange(NUM_SPAWN_TYPES)\n",
    "        if onehot:\n",
    "            z[i, spawn_type] = 1.0\n",
    "        else:\n",
    "            z[i] = (z[i] + spawn_type) / 7.0\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, use_oracle, onehot):\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        z = get_explanatory_randomness(X, y, use_oracle, onehot)\n",
    "        pred = model(X, z)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test_loop(split_name, dataloader, model, loss_fn, use_oracle, onehot):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    board_accuracy = 0.0\n",
    "    spawn_recall = 0.0\n",
    "    num_spawns = 0.0\n",
    "    spawn_precision = 0.0\n",
    "    num_predicted_spawns = 0.0\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            z = get_explanatory_randomness(X, y, use_oracle, onehot)\n",
    "            pred = model(X, z)\n",
    "            epoch_loss += loss_fn(pred, y).item()\n",
    "            classes = torch.argmax(pred, dim=1)\n",
    "            board_accuracy += (classes == y).all(-1).all(-1).type(torch.float).mean().item()\n",
    "\n",
    "            actual_spawns = (X[:, 0, :] == 0).all(-1) & (y[:, 0, :] == 1).any(-1)\n",
    "            predicted_spawns = (classes[:, 0, :] == 1).any(-1)\n",
    "            num_true_positives = (actual_spawns & predicted_spawns).type(torch.float).sum().item()\n",
    "            spawn_recall += num_true_positives\n",
    "            num_spawns += actual_spawns.type(torch.float).sum().item()\n",
    "            spawn_precision += num_true_positives\n",
    "            num_predicted_spawns += predicted_spawns.type(torch.float).sum().item()\n",
    "\n",
    "    epoch_loss /= num_batches\n",
    "    board_accuracy /= num_batches\n",
    "    spawn_recall /= num_spawns\n",
    "    spawn_precision = np.nan if num_predicted_spawns == 0.0 else spawn_precision / num_predicted_spawns\n",
    "    \n",
    "    return {\n",
    "        \"loss\": epoch_loss,\n",
    "        \"acc_board\": board_accuracy,\n",
    "        \"Spawn recall\": spawn_recall,\n",
    "        \"Spawn precision\": spawn_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_interesting_examples(dataset):\n",
    "    for x, y in dataset:\n",
    "        # In our case, \"interesting\" currently means block spawns\n",
    "        if (x[0] == 0).all() & (y[1] == 1).any():\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prediction(x, pred, y):\n",
    "    \"\"\"Renders an example and prediction into a single-image array.\n",
    "    \n",
    "    Inputs:\n",
    "        x: Tensor of shape (height, width), the model input.\n",
    "        pred: Tensor of shape (height, width), the model prediction.\n",
    "        y: Tensor of shape (height, width), the target.\n",
    "    \"\"\"\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == pred.shape\n",
    "    assert x.shape == y.shape\n",
    "    height, width = x.shape\n",
    "    with torch.no_grad():\n",
    "        separator = torch.ones(height, 1, dtype=x.dtype)\n",
    "        return torch.cat((x, separator, pred, separator, y), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def train(model, learning_rate=1e-1, epochs=100, use_oracle=False, onehot=False):\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    log_dir = os.path.join(\"runs\", \"experiment_013\")\n",
    "    log_subdir = os.path.join(log_dir, f'{model.__class__.__name__}_{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "    tb = SummaryWriter(log_subdir)\n",
    "\n",
    "    train_examples = list(itertools.islice(find_interesting_examples(train_dataset), 3))\n",
    "    test_examples = list(itertools.islice(find_interesting_examples(test_dataset), 3))\n",
    "\n",
    "    print(f\"Training model {model.__class__.__name__}...\")\n",
    "    for t in range(epochs):\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, use_oracle, onehot)\n",
    "        train_metrics = test_loop(\"Train\", train_dataloader, model, loss_fn, use_oracle, onehot)\n",
    "        test_metrics = test_loop(\"Test\", test_dataloader, model, loss_fn, use_oracle, onehot)\n",
    "        tb.add_scalar(\"Loss/train\", train_metrics[\"loss\"], t)\n",
    "        tb.add_scalar(\"Board accuracy/train\", train_metrics[\"acc_board\"], t)\n",
    "        tb.add_scalar(\"Spawn recall/train\", train_metrics[\"Spawn recall\"], t)\n",
    "        tb.add_scalar(\"Spawn precision/train\", train_metrics[\"Spawn precision\"], t)\n",
    "        tb.add_scalar(\"Loss/test\", test_metrics[\"loss\"], t)\n",
    "        tb.add_scalar(\"Board accuracy/test\", test_metrics[\"acc_board\"], t)\n",
    "        tb.add_scalar(\"Spawn recall/test\", test_metrics[\"Spawn recall\"], t)\n",
    "        tb.add_scalar(\"Spawn precision/test\", test_metrics[\"Spawn precision\"], t)\n",
    "        for name, weight in model.named_parameters():\n",
    "            tb.add_histogram(f\"Weights/{name}\", weight, t)\n",
    "            tb.add_histogram(f\"Gradients/{name}\", weight.grad, t)\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(train_examples):\n",
    "                z = get_explanatory_randomness(x.unsqueeze(0), y.unsqueeze(0), use_oracle, onehot)\n",
    "                pred = model(x.unsqueeze(0), z).squeeze(0).argmax(dim=0)\n",
    "                img = render_prediction(x, pred, y)\n",
    "                tb.add_image(f\"Predictions/train/{i}\", img, t, dataformats=\"HW\")\n",
    "            for i, (x, y) in enumerate(test_examples):\n",
    "                z = get_explanatory_randomness(x.unsqueeze(0), y.unsqueeze(0), use_oracle, onehot)\n",
    "                pred = model(x.unsqueeze(0), z).squeeze(0).argmax(dim=0)\n",
    "                img = render_prediction(x, pred, y)\n",
    "                tb.add_image(f\"Predictions/test/{i}\", img, t, dataformats=\"HW\")\n",
    "\n",
    "    tb.close()\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return {\n",
    "        \"train_accuracy\": train_metrics[\"acc_board\"],\n",
    "        \"train_spawn_recall\": train_metrics[\"Spawn recall\"],\n",
    "        \"test_accuracy\": test_metrics[\"acc_board\"],\n",
    "        \"test_spawn_recall\": test_metrics[\"Spawn recall\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def train_and_compare_architectures(model_classes, repeats=6, use_oracle=False, onehot=False):\n",
    "    architectures = [{\"class\": model_class} for model_class in model_classes]\n",
    "\n",
    "    for a in architectures:\n",
    "        a[\"train_accuracy\"] = np.zeros(repeats)\n",
    "        a[\"train_spawn_recall\"] = np.zeros(repeats)\n",
    "        a[\"test_accuracy\"] = np.zeros(repeats)\n",
    "        a[\"test_spawn_recall\"] = np.zeros(repeats)\n",
    "\n",
    "    for repeat in range(repeats):\n",
    "        for architecture in architectures:\n",
    "            model = architecture[\"class\"]().to(device)\n",
    "            repeat_results = train(model, use_oracle=use_oracle, onehot=onehot)\n",
    "            architecture[\"train_accuracy\"][repeat] = repeat_results[\"train_accuracy\"]\n",
    "            architecture[\"train_spawn_recall\"][repeat] = repeat_results[\"train_spawn_recall\"]\n",
    "            architecture[\"test_accuracy\"][repeat] = repeat_results[\"test_accuracy\"]\n",
    "            architecture[\"test_spawn_recall\"][repeat] = repeat_results[\"test_spawn_recall\"]\n",
    "        \n",
    "    print(f\"Results:\")\n",
    "    for architecture in architectures:\n",
    "        print(f\"Class: {architecture['class'].__name__}\")\n",
    "        print(f\"  Train accuracy mean {architecture['train_accuracy'].mean():.5f}, std {architecture['train_accuracy'].std():.5f}\")\n",
    "        print(f\"  Test accuracy mean {architecture['test_accuracy'].mean():.5f}, std {architecture['test_accuracy'].std():.5f}\")\n",
    "        print(f\"  Train spawn recall mean {architecture['train_spawn_recall'].mean():.5f}, std {architecture['train_spawn_recall'].std():.5f}\")\n",
    "        print(f\"  Test spawn recall mean {architecture['test_spawn_recall'].mean():.5f}, std {architecture['test_spawn_recall'].std():.5f}\")\n",
    "        print()\n",
    "\n",
    "    for architecture in architectures[1:]:\n",
    "        baseline = architectures[0]\n",
    "        print(f\"Performing t-tests of {architecture['class'].__name__} against {baseline['class'].__name__}\")\n",
    "        accuracy_ttest = ttest_ind(baseline[\"test_accuracy\"], architecture[\"test_accuracy\"], equal_var=False)\n",
    "        spawn_recall_ttest = ttest_ind(baseline[\"test_spawn_recall\"], architecture[\"test_spawn_recall\"], equal_var=False)\n",
    "        print(\"Accuracy t-test results:\")\n",
    "        print(accuracy_ttest)\n",
    "        print(\"Spawn recall t-test results:\")\n",
    "        print(spawn_recall_ttest)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ModelWithRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Results:\n",
      "Class: ModelWithRandomness\n",
      "  Train accuracy mean 0.94180, std 0.00071\n",
      "  Test accuracy mean 0.91439, std 0.00169\n",
      "  Train spawn recall mean 0.47333, std 0.14543\n",
      "  Test spawn recall mean 0.47083, std 0.16293\n",
      "\n",
      "Class: TetrisModel\n",
      "  Train accuracy mean 0.94180, std 0.00042\n",
      "  Test accuracy mean 0.91439, std 0.00169\n",
      "  Train spawn recall mean 0.50000, std 0.11764\n",
      "  Test spawn recall mean 0.46250, std 0.09100\n",
      "\n",
      "Performing t-tests of TetrisModel against ModelWithRandomness\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=1.0362610227313026e-13, pvalue=0.9999999999999194)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=0.09985033665845854, pvalue=0.9229681619085626)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_compare_architectures([TetrisModel, ModelWithRandomness])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the random input isn't helping by itself. The model still learns to predict the most common type of block spawn, and the training curves for the two model architectures look very similar.\n",
    "\n",
    "Let's assess the best-case scenario: let's modify the random numbers so that they perfectly explain the block spawn type, and see if that benefits the model. If even this gives no benefit, then it's evidence that the model is underfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best-case scenario: block spawn type oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ModelWithRandomness...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Results:\n",
      "Class: ModelWithRandomness\n",
      "  Train accuracy mean 0.94208, std 0.00039\n",
      "  Test accuracy mean 0.91667, std 0.00107\n",
      "  Train spawn recall mean 0.40133, std 0.03984\n",
      "  Test spawn recall mean 0.43750, std 0.11434\n",
      "\n",
      "Class: TetrisModel\n",
      "  Train accuracy mean 0.94104, std 0.00185\n",
      "  Test accuracy mean 0.91364, std 0.00186\n",
      "  Train spawn recall mean 0.47600, std 0.08845\n",
      "  Test spawn recall mean 0.45833, std 0.07169\n",
      "\n",
      "Performing t-tests of TetrisModel against ModelWithRandomness\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=3.1622776601682827, pvalue=0.01334906342602041)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=-0.34519719844613717, pvalue=0.7384268984761414)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_compare_architectures([TetrisModel, ModelWithRandomness], use_oracle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems not to benefit from having a block spawn type oracle, so maybe the model is just underfitting and needs more capacity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.3696, 0.4127, 0.4992, 0.4408, 0.4408, 0.4408, 0.4408, 0.3562,\n",
      "          0.4209, 0.1969],\n",
      "         [0.4434, 0.2622, 0.3944, 0.3677, 0.3677, 0.3677, 0.3677, 0.3279,\n",
      "          0.4091, 0.1607],\n",
      "         [0.4093, 0.3593, 0.4029, 0.4253, 0.4253, 0.4253, 0.4253, 0.3938,\n",
      "          0.3846, 0.1297],\n",
      "         [0.4099, 0.3028, 0.4368, 0.4307, 0.4307, 0.4307, 0.4307, 0.3549,\n",
      "          0.3598, 0.1235],\n",
      "         [0.4099, 0.3028, 0.4368, 0.4307, 0.4307, 0.4307, 0.4307, 0.3549,\n",
      "          0.3598, 0.1235],\n",
      "         [0.4099, 0.3028, 0.4368, 0.4307, 0.4307, 0.4307, 0.4307, 0.3549,\n",
      "          0.3598, 0.1235],\n",
      "         [0.4099, 0.3028, 0.4368, 0.4307, 0.4307, 0.4307, 0.4307, 0.3549,\n",
      "          0.3598, 0.1235],\n",
      "         [0.4099, 0.3028, 0.4368, 0.4307, 0.4307, 0.4307, 0.4307, 0.3549,\n",
      "          0.3598, 0.1235],\n",
      "         [0.4099, 0.3028, 0.4368, 0.4307, 0.4307, 0.4307, 0.4307, 0.3549,\n",
      "          0.3598, 0.1235],\n",
      "         [0.4099, 0.3028, 0.4368, 0.4307, 0.4307, 0.4307, 0.4307, 0.3549,\n",
      "          0.3598, 0.1235],\n",
      "         [0.4099, 0.3028, 0.4368, 0.4307, 0.4307, 0.4307, 0.4307, 0.3549,\n",
      "          0.3598, 0.1235],\n",
      "         [0.4099, 0.3028, 0.4368, 0.4307, 0.4307, 0.4307, 0.4307, 0.3549,\n",
      "          0.3598, 0.1235],\n",
      "         [0.4211, 0.3484, 0.4597, 0.5432, 0.3886, 0.3943, 0.4623, 0.3505,\n",
      "          0.3598, 0.1235],\n",
      "         [0.4174, 0.3631, 0.5104, 0.4559, 0.6449, 0.4388, 0.3396, 0.3408,\n",
      "          0.3642, 0.1235],\n",
      "         [0.4816, 0.1970, 0.1321, 0.4246, 0.4005, 0.6215, 0.5243, 0.3482,\n",
      "          0.3789, 0.1235],\n",
      "         [0.3895, 0.2126, 0.2571, 0.3310, 0.6506, 0.3531, 0.4838, 0.4585,\n",
      "          0.3953, 0.1235],\n",
      "         [0.4186, 0.2916, 0.3011, 0.3633, 0.5731, 0.2844, 0.2050, 0.4859,\n",
      "          0.4626, 0.1235],\n",
      "         [0.4275, 0.2883, 0.4524, 0.1646, 0.2681, 0.5450, 0.3387, 0.3923,\n",
      "          0.4158, 0.1235],\n",
      "         [0.4094, 0.3387, 0.5257, 0.5267, 0.3289, 0.2983, 0.3904, 0.4180,\n",
      "          0.3761, 0.1235],\n",
      "         [0.4473, 0.4481, 0.5684, 0.4640, 0.5188, 0.6362, 0.2699, 0.2711,\n",
      "          0.4502, 0.1207],\n",
      "         [0.4258, 0.2511, 0.3175, 0.3540, 0.3703, 0.3229, 0.5419, 0.4045,\n",
      "          0.5638, 0.1582],\n",
      "         [0.3996, 0.2129, 0.4845, 0.3858, 0.2315, 0.4413, 0.4253, 0.3366,\n",
      "          0.3590, 0.2122]],\n",
      "\n",
      "        [[0.6304, 0.5873, 0.5008, 0.5592, 0.5592, 0.5592, 0.5592, 0.6438,\n",
      "          0.5791, 0.8031],\n",
      "         [0.5566, 0.7378, 0.6056, 0.6323, 0.6323, 0.6323, 0.6323, 0.6721,\n",
      "          0.5909, 0.8393],\n",
      "         [0.5907, 0.6407, 0.5971, 0.5747, 0.5747, 0.5747, 0.5747, 0.6062,\n",
      "          0.6154, 0.8703],\n",
      "         [0.5901, 0.6972, 0.5632, 0.5693, 0.5693, 0.5693, 0.5693, 0.6451,\n",
      "          0.6402, 0.8765],\n",
      "         [0.5901, 0.6972, 0.5632, 0.5693, 0.5693, 0.5693, 0.5693, 0.6451,\n",
      "          0.6402, 0.8765],\n",
      "         [0.5901, 0.6972, 0.5632, 0.5693, 0.5693, 0.5693, 0.5693, 0.6451,\n",
      "          0.6402, 0.8765],\n",
      "         [0.5901, 0.6972, 0.5632, 0.5693, 0.5693, 0.5693, 0.5693, 0.6451,\n",
      "          0.6402, 0.8765],\n",
      "         [0.5901, 0.6972, 0.5632, 0.5693, 0.5693, 0.5693, 0.5693, 0.6451,\n",
      "          0.6402, 0.8765],\n",
      "         [0.5901, 0.6972, 0.5632, 0.5693, 0.5693, 0.5693, 0.5693, 0.6451,\n",
      "          0.6402, 0.8765],\n",
      "         [0.5901, 0.6972, 0.5632, 0.5693, 0.5693, 0.5693, 0.5693, 0.6451,\n",
      "          0.6402, 0.8765],\n",
      "         [0.5901, 0.6972, 0.5632, 0.5693, 0.5693, 0.5693, 0.5693, 0.6451,\n",
      "          0.6402, 0.8765],\n",
      "         [0.5901, 0.6972, 0.5632, 0.5693, 0.5693, 0.5693, 0.5693, 0.6451,\n",
      "          0.6402, 0.8765],\n",
      "         [0.5789, 0.6516, 0.5403, 0.4568, 0.6114, 0.6057, 0.5377, 0.6495,\n",
      "          0.6402, 0.8765],\n",
      "         [0.5826, 0.6369, 0.4896, 0.5441, 0.3551, 0.5612, 0.6604, 0.6592,\n",
      "          0.6358, 0.8765],\n",
      "         [0.5184, 0.8030, 0.8679, 0.5754, 0.5995, 0.3785, 0.4757, 0.6518,\n",
      "          0.6211, 0.8765],\n",
      "         [0.6105, 0.7874, 0.7429, 0.6690, 0.3494, 0.6469, 0.5162, 0.5415,\n",
      "          0.6047, 0.8765],\n",
      "         [0.5814, 0.7084, 0.6989, 0.6367, 0.4269, 0.7156, 0.7950, 0.5141,\n",
      "          0.5374, 0.8765],\n",
      "         [0.5725, 0.7117, 0.5476, 0.8354, 0.7319, 0.4550, 0.6613, 0.6077,\n",
      "          0.5842, 0.8765],\n",
      "         [0.5906, 0.6613, 0.4743, 0.4733, 0.6711, 0.7017, 0.6096, 0.5820,\n",
      "          0.6239, 0.8765],\n",
      "         [0.5527, 0.5519, 0.4316, 0.5360, 0.4812, 0.3638, 0.7301, 0.7289,\n",
      "          0.5498, 0.8793],\n",
      "         [0.5742, 0.7489, 0.6825, 0.6460, 0.6297, 0.6771, 0.4581, 0.5955,\n",
      "          0.4362, 0.8418],\n",
      "         [0.6004, 0.7871, 0.5155, 0.6142, 0.7685, 0.5587, 0.5747, 0.6634,\n",
      "          0.6410, 0.7878]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "class WiderLocalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.glob = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(160, 10)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(42, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 2, 1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, height, width = x.shape\n",
    "        \n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        z = z[:, None, None, None] # Expand dims to match x\n",
    "        z = z.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, z), dim=1)\n",
    "\n",
    "        x = self.loc(x)\n",
    "\n",
    "        x_glob = self.glob(x)\n",
    "        x_glob = x_glob[:, :, None, None] # Expand dims\n",
    "        x_glob = x_glob.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, x_glob), dim=1)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = WiderLocalModel().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.rand(batch_size)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.6459, 0.6785, 0.7665, 0.7721, 0.7721, 0.7721, 0.7721, 0.7578,\n",
      "          0.7437, 0.7193],\n",
      "         [0.6656, 0.5422, 0.6360, 0.6231, 0.6231, 0.6231, 0.6231, 0.6361,\n",
      "          0.5276, 0.5421],\n",
      "         [0.6955, 0.6783, 0.6351, 0.6747, 0.6747, 0.6747, 0.6747, 0.7097,\n",
      "          0.6246, 0.6556],\n",
      "         [0.6940, 0.6009, 0.6206, 0.6541, 0.6541, 0.6541, 0.6541, 0.6743,\n",
      "          0.6102, 0.6148],\n",
      "         [0.6940, 0.6009, 0.6206, 0.6541, 0.6541, 0.6541, 0.6541, 0.6743,\n",
      "          0.6102, 0.6148],\n",
      "         [0.6940, 0.5980, 0.6386, 0.6479, 0.6904, 0.6681, 0.6444, 0.6731,\n",
      "          0.6115, 0.6148],\n",
      "         [0.6940, 0.5893, 0.5861, 0.6496, 0.7078, 0.6757, 0.5940, 0.6518,\n",
      "          0.6168, 0.6148],\n",
      "         [0.6940, 0.5664, 0.6089, 0.6975, 0.7255, 0.7657, 0.7240, 0.6692,\n",
      "          0.6181, 0.6148],\n",
      "         [0.6940, 0.5520, 0.6364, 0.6712, 0.7689, 0.7510, 0.6297, 0.6686,\n",
      "          0.6207, 0.6148],\n",
      "         [0.6940, 0.5699, 0.5990, 0.5448, 0.6299, 0.6832, 0.5221, 0.5761,\n",
      "          0.6380, 0.6148],\n",
      "         [0.7018, 0.6064, 0.6300, 0.6570, 0.5406, 0.5796, 0.4912, 0.6672,\n",
      "          0.5943, 0.6148],\n",
      "         [0.6547, 0.5735, 0.6145, 0.6907, 0.6721, 0.5573, 0.6767, 0.6935,\n",
      "          0.6300, 0.6148],\n",
      "         [0.6426, 0.5971, 0.6908, 0.7606, 0.7464, 0.7973, 0.7544, 0.6917,\n",
      "          0.6004, 0.6148],\n",
      "         [0.7010, 0.5323, 0.5653, 0.6741, 0.7884, 0.6697, 0.5879, 0.6514,\n",
      "          0.6003, 0.6148],\n",
      "         [0.6741, 0.5372, 0.6302, 0.7152, 0.5206, 0.6313, 0.6599, 0.6068,\n",
      "          0.6088, 0.6148],\n",
      "         [0.6828, 0.5944, 0.5827, 0.5957, 0.5153, 0.7455, 0.6735, 0.6996,\n",
      "          0.6003, 0.6148],\n",
      "         [0.6579, 0.5721, 0.6182, 0.7033, 0.6120, 0.7848, 0.7079, 0.6584,\n",
      "          0.6079, 0.6148],\n",
      "         [0.6155, 0.6455, 0.6814, 0.6836, 0.6465, 0.7704, 0.7727, 0.6622,\n",
      "          0.6342, 0.6148],\n",
      "         [0.6245, 0.6249, 0.5736, 0.7432, 0.6864, 0.6073, 0.7624, 0.6816,\n",
      "          0.6533, 0.6148],\n",
      "         [0.6024, 0.4870, 0.6112, 0.4809, 0.7249, 0.6190, 0.6106, 0.6184,\n",
      "          0.5691, 0.6054],\n",
      "         [0.6441, 0.5127, 0.4154, 0.6891, 0.6019, 0.6980, 0.6279, 0.6603,\n",
      "          0.6549, 0.6305],\n",
      "         [0.5345, 0.4232, 0.4589, 0.5464, 0.2454, 0.4449, 0.4546, 0.4450,\n",
      "          0.4375, 0.3885]],\n",
      "\n",
      "        [[0.3541, 0.3215, 0.2335, 0.2279, 0.2279, 0.2279, 0.2279, 0.2422,\n",
      "          0.2563, 0.2807],\n",
      "         [0.3344, 0.4578, 0.3640, 0.3769, 0.3769, 0.3769, 0.3769, 0.3639,\n",
      "          0.4724, 0.4579],\n",
      "         [0.3045, 0.3217, 0.3649, 0.3253, 0.3253, 0.3253, 0.3253, 0.2903,\n",
      "          0.3754, 0.3444],\n",
      "         [0.3060, 0.3991, 0.3794, 0.3459, 0.3459, 0.3459, 0.3459, 0.3257,\n",
      "          0.3898, 0.3852],\n",
      "         [0.3060, 0.3991, 0.3794, 0.3459, 0.3459, 0.3459, 0.3459, 0.3257,\n",
      "          0.3898, 0.3852],\n",
      "         [0.3060, 0.4020, 0.3614, 0.3521, 0.3096, 0.3319, 0.3556, 0.3269,\n",
      "          0.3885, 0.3852],\n",
      "         [0.3060, 0.4107, 0.4139, 0.3504, 0.2922, 0.3243, 0.4060, 0.3482,\n",
      "          0.3832, 0.3852],\n",
      "         [0.3060, 0.4336, 0.3911, 0.3025, 0.2745, 0.2343, 0.2760, 0.3308,\n",
      "          0.3819, 0.3852],\n",
      "         [0.3060, 0.4480, 0.3636, 0.3288, 0.2311, 0.2490, 0.3703, 0.3314,\n",
      "          0.3793, 0.3852],\n",
      "         [0.3060, 0.4301, 0.4010, 0.4552, 0.3701, 0.3168, 0.4779, 0.4239,\n",
      "          0.3620, 0.3852],\n",
      "         [0.2982, 0.3936, 0.3700, 0.3430, 0.4594, 0.4204, 0.5088, 0.3328,\n",
      "          0.4057, 0.3852],\n",
      "         [0.3453, 0.4265, 0.3855, 0.3093, 0.3279, 0.4427, 0.3233, 0.3065,\n",
      "          0.3700, 0.3852],\n",
      "         [0.3574, 0.4029, 0.3092, 0.2394, 0.2536, 0.2027, 0.2456, 0.3083,\n",
      "          0.3996, 0.3852],\n",
      "         [0.2990, 0.4677, 0.4347, 0.3259, 0.2116, 0.3303, 0.4121, 0.3486,\n",
      "          0.3997, 0.3852],\n",
      "         [0.3259, 0.4628, 0.3698, 0.2848, 0.4794, 0.3687, 0.3401, 0.3932,\n",
      "          0.3912, 0.3852],\n",
      "         [0.3172, 0.4056, 0.4173, 0.4043, 0.4847, 0.2545, 0.3265, 0.3004,\n",
      "          0.3997, 0.3852],\n",
      "         [0.3421, 0.4279, 0.3818, 0.2967, 0.3880, 0.2152, 0.2921, 0.3416,\n",
      "          0.3921, 0.3852],\n",
      "         [0.3845, 0.3545, 0.3186, 0.3164, 0.3535, 0.2296, 0.2273, 0.3378,\n",
      "          0.3658, 0.3852],\n",
      "         [0.3755, 0.3751, 0.4264, 0.2568, 0.3136, 0.3927, 0.2376, 0.3184,\n",
      "          0.3467, 0.3852],\n",
      "         [0.3976, 0.5130, 0.3888, 0.5191, 0.2751, 0.3810, 0.3894, 0.3816,\n",
      "          0.4309, 0.3946],\n",
      "         [0.3559, 0.4873, 0.5846, 0.3109, 0.3981, 0.3020, 0.3721, 0.3397,\n",
      "          0.3451, 0.3695],\n",
      "         [0.4655, 0.5768, 0.5411, 0.4536, 0.7546, 0.5551, 0.5454, 0.5550,\n",
      "          0.5625, 0.6115]]])\n"
     ]
    }
   ],
   "source": [
    "class WiderGlobalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.glob = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(320, 16)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 2, 1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, height, width = x.shape\n",
    "        \n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        z = z[:, None, None, None] # Expand dims to match x\n",
    "        z = z.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, z), dim=1)\n",
    "\n",
    "        x = self.loc(x)\n",
    "\n",
    "        x_glob = self.glob(x)\n",
    "        x_glob = x_glob[:, :, None, None] # Expand dims\n",
    "        x_glob = x_glob.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, x_glob), dim=1)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = WiderGlobalModel().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.rand(batch_size)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.4579, 0.3360, 0.3301, 0.3039, 0.3039, 0.3039, 0.3039, 0.3534,\n",
      "          0.4539, 0.4306],\n",
      "         [0.4724, 0.2551, 0.2851, 0.3083, 0.3083, 0.3083, 0.3083, 0.3686,\n",
      "          0.3679, 0.3786],\n",
      "         [0.5247, 0.4068, 0.4367, 0.4651, 0.4651, 0.4651, 0.4651, 0.4939,\n",
      "          0.3566, 0.3731],\n",
      "         [0.5135, 0.3682, 0.3923, 0.3959, 0.3959, 0.3959, 0.3959, 0.4511,\n",
      "          0.3917, 0.4191],\n",
      "         [0.5135, 0.3682, 0.3923, 0.3959, 0.3959, 0.3959, 0.3959, 0.4511,\n",
      "          0.3917, 0.4191],\n",
      "         [0.5135, 0.3682, 0.3923, 0.3959, 0.3959, 0.3959, 0.3959, 0.4511,\n",
      "          0.3917, 0.4191],\n",
      "         [0.5135, 0.3682, 0.3923, 0.3959, 0.3959, 0.3959, 0.3959, 0.4511,\n",
      "          0.3917, 0.4191],\n",
      "         [0.5135, 0.3682, 0.3680, 0.3684, 0.4245, 0.3663, 0.3380, 0.4771,\n",
      "          0.3891, 0.4191],\n",
      "         [0.5157, 0.3545, 0.3313, 0.3100, 0.3870, 0.3262, 0.3408, 0.3974,\n",
      "          0.3681, 0.4191],\n",
      "         [0.5062, 0.3688, 0.4031, 0.2242, 0.3412, 0.3250, 0.4798, 0.5298,\n",
      "          0.4345, 0.4191],\n",
      "         [0.4935, 0.3284, 0.3813, 0.1990, 0.2170, 0.4623, 0.3208, 0.4503,\n",
      "          0.3942, 0.4191],\n",
      "         [0.5181, 0.4155, 0.4525, 0.5287, 0.4260, 0.4550, 0.2737, 0.3496,\n",
      "          0.3371, 0.4191],\n",
      "         [0.5387, 0.4978, 0.3811, 0.2753, 0.1282, 0.2927, 0.3095, 0.4672,\n",
      "          0.3386, 0.4191],\n",
      "         [0.5245, 0.4415, 0.1886, 0.1627, 0.1830, 0.1907, 0.1627, 0.5004,\n",
      "          0.4114, 0.4191],\n",
      "         [0.5027, 0.5961, 0.6050, 0.2136, 0.4197, 0.3634, 0.4236, 0.4054,\n",
      "          0.3564, 0.4191],\n",
      "         [0.4833, 0.3759, 0.3715, 0.1316, 0.2375, 0.4126, 0.3875, 0.3883,\n",
      "          0.3438, 0.4191],\n",
      "         [0.5042, 0.3544, 0.4806, 0.4207, 0.2413, 0.4437, 0.2579, 0.4890,\n",
      "          0.4282, 0.4191],\n",
      "         [0.5439, 0.5064, 0.4240, 0.5019, 0.1719, 0.6320, 0.4334, 0.4611,\n",
      "          0.3904, 0.4191],\n",
      "         [0.5232, 0.4225, 0.1670, 0.1218, 0.3087, 0.4736, 0.3814, 0.4076,\n",
      "          0.3586, 0.4149],\n",
      "         [0.5932, 0.6209, 0.5467, 0.4035, 0.3174, 0.6328, 0.2824, 0.4821,\n",
      "          0.4275, 0.4553],\n",
      "         [0.5285, 0.3706, 0.4395, 0.2240, 0.3030, 0.5582, 0.3360, 0.2812,\n",
      "          0.4415, 0.3963],\n",
      "         [0.6409, 0.5498, 0.5122, 0.4052, 0.6673, 0.7437, 0.4924, 0.4711,\n",
      "          0.4779, 0.4347]],\n",
      "\n",
      "        [[0.5421, 0.6640, 0.6699, 0.6961, 0.6961, 0.6961, 0.6961, 0.6466,\n",
      "          0.5461, 0.5694],\n",
      "         [0.5276, 0.7449, 0.7149, 0.6917, 0.6917, 0.6917, 0.6917, 0.6314,\n",
      "          0.6321, 0.6214],\n",
      "         [0.4753, 0.5932, 0.5633, 0.5349, 0.5349, 0.5349, 0.5349, 0.5061,\n",
      "          0.6434, 0.6269],\n",
      "         [0.4865, 0.6318, 0.6077, 0.6041, 0.6041, 0.6041, 0.6041, 0.5489,\n",
      "          0.6083, 0.5809],\n",
      "         [0.4865, 0.6318, 0.6077, 0.6041, 0.6041, 0.6041, 0.6041, 0.5489,\n",
      "          0.6083, 0.5809],\n",
      "         [0.4865, 0.6318, 0.6077, 0.6041, 0.6041, 0.6041, 0.6041, 0.5489,\n",
      "          0.6083, 0.5809],\n",
      "         [0.4865, 0.6318, 0.6077, 0.6041, 0.6041, 0.6041, 0.6041, 0.5489,\n",
      "          0.6083, 0.5809],\n",
      "         [0.4865, 0.6318, 0.6320, 0.6316, 0.5755, 0.6337, 0.6620, 0.5229,\n",
      "          0.6109, 0.5809],\n",
      "         [0.4843, 0.6455, 0.6687, 0.6900, 0.6130, 0.6738, 0.6592, 0.6026,\n",
      "          0.6319, 0.5809],\n",
      "         [0.4938, 0.6312, 0.5969, 0.7758, 0.6588, 0.6750, 0.5202, 0.4702,\n",
      "          0.5655, 0.5809],\n",
      "         [0.5065, 0.6716, 0.6187, 0.8010, 0.7830, 0.5377, 0.6792, 0.5497,\n",
      "          0.6058, 0.5809],\n",
      "         [0.4819, 0.5845, 0.5475, 0.4713, 0.5740, 0.5450, 0.7263, 0.6504,\n",
      "          0.6629, 0.5809],\n",
      "         [0.4613, 0.5022, 0.6189, 0.7247, 0.8718, 0.7073, 0.6905, 0.5328,\n",
      "          0.6614, 0.5809],\n",
      "         [0.4755, 0.5585, 0.8114, 0.8373, 0.8170, 0.8093, 0.8373, 0.4996,\n",
      "          0.5886, 0.5809],\n",
      "         [0.4973, 0.4039, 0.3950, 0.7864, 0.5803, 0.6366, 0.5764, 0.5946,\n",
      "          0.6436, 0.5809],\n",
      "         [0.5167, 0.6241, 0.6285, 0.8684, 0.7625, 0.5874, 0.6125, 0.6117,\n",
      "          0.6562, 0.5809],\n",
      "         [0.4958, 0.6456, 0.5194, 0.5793, 0.7587, 0.5563, 0.7421, 0.5110,\n",
      "          0.5718, 0.5809],\n",
      "         [0.4561, 0.4936, 0.5760, 0.4981, 0.8281, 0.3680, 0.5666, 0.5389,\n",
      "          0.6096, 0.5809],\n",
      "         [0.4768, 0.5775, 0.8330, 0.8782, 0.6913, 0.5264, 0.6186, 0.5924,\n",
      "          0.6414, 0.5851],\n",
      "         [0.4068, 0.3791, 0.4533, 0.5965, 0.6826, 0.3672, 0.7176, 0.5179,\n",
      "          0.5725, 0.5447],\n",
      "         [0.4715, 0.6294, 0.5605, 0.7760, 0.6970, 0.4418, 0.6640, 0.7188,\n",
      "          0.5585, 0.6037],\n",
      "         [0.3591, 0.4502, 0.4878, 0.5948, 0.3327, 0.2563, 0.5076, 0.5289,\n",
      "          0.5221, 0.5653]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "class WiderAllModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.glob = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(320, 16)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(48, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 2, 1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, height, width = x.shape\n",
    "        \n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        z = z[:, None, None, None] # Expand dims to match x\n",
    "        z = z.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, z), dim=1)\n",
    "\n",
    "        x = self.loc(x)\n",
    "\n",
    "        x_glob = self.glob(x)\n",
    "        x_glob = x_glob[:, :, None, None] # Expand dims\n",
    "        x_glob = x_glob.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, x_glob), dim=1)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = WiderAllModel().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.rand(batch_size)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.4666, 0.6014, 0.4254, 0.6629, 0.6043, 0.6783, 0.5354, 0.4090,\n",
      "          0.4796, 0.4462],\n",
      "         [0.5597, 0.6315, 0.5367, 0.4924, 0.8108, 0.8416, 0.6102, 0.5939,\n",
      "          0.6079, 0.6184],\n",
      "         [0.6034, 0.7628, 0.7845, 0.7994, 0.7042, 0.9159, 0.7694, 0.6277,\n",
      "          0.6216, 0.6239],\n",
      "         [0.6809, 0.8678, 0.8648, 0.8835, 0.5875, 0.3678, 0.8281, 0.8083,\n",
      "          0.6021, 0.4855],\n",
      "         [0.7806, 0.9695, 0.7345, 0.6606, 0.8868, 0.2829, 0.6101, 0.8705,\n",
      "          0.6033, 0.4708],\n",
      "         [0.6210, 0.7642, 0.5345, 0.6443, 0.9824, 0.9222, 0.7318, 0.5394,\n",
      "          0.5356, 0.4825],\n",
      "         [0.7142, 0.5969, 0.8389, 0.3936, 0.7534, 0.5363, 0.6992, 0.6516,\n",
      "          0.5784, 0.6197],\n",
      "         [0.8356, 0.9787, 0.5524, 0.9807, 0.8627, 0.6114, 0.6474, 0.2980,\n",
      "          0.5976, 0.6479],\n",
      "         [0.6194, 0.7868, 0.6864, 0.8630, 0.5132, 0.8959, 0.9864, 0.7140,\n",
      "          0.8057, 0.5543],\n",
      "         [0.4168, 0.7128, 0.8716, 0.7688, 0.8695, 0.8920, 0.6729, 0.3380,\n",
      "          0.5017, 0.6025],\n",
      "         [0.6057, 0.9900, 0.2211, 0.9816, 0.0178, 0.9619, 0.4189, 0.5119,\n",
      "          0.5704, 0.5610],\n",
      "         [0.5747, 0.8389, 0.8447, 0.5919, 0.9716, 0.7698, 0.8698, 0.4128,\n",
      "          0.5003, 0.5825],\n",
      "         [0.6150, 0.9113, 0.9790, 0.7616, 0.3370, 0.7973, 0.6239, 0.8051,\n",
      "          0.5672, 0.5285],\n",
      "         [0.6376, 0.9498, 0.5840, 0.4454, 0.9040, 0.5384, 0.8683, 0.7313,\n",
      "          0.6182, 0.5022],\n",
      "         [0.6067, 0.5877, 0.7364, 0.7214, 0.7151, 0.9353, 0.8693, 0.7911,\n",
      "          0.8279, 0.6496],\n",
      "         [0.5751, 0.4942, 0.4909, 0.9960, 0.1649, 0.9807, 0.7711, 0.4688,\n",
      "          0.8144, 0.4245],\n",
      "         [0.5081, 0.8368, 0.7698, 0.7342, 0.8231, 0.7682, 0.4277, 0.6065,\n",
      "          0.7646, 0.6620],\n",
      "         [0.6725, 0.9860, 0.6634, 0.9686, 0.7617, 0.3799, 0.1562, 0.5748,\n",
      "          0.6684, 0.5438],\n",
      "         [0.7682, 0.8903, 0.9488, 0.5278, 0.6289, 0.8447, 0.5959, 0.7493,\n",
      "          0.6065, 0.4414],\n",
      "         [0.5900, 0.7721, 0.7207, 0.8721, 0.3115, 0.7658, 0.5747, 0.5278,\n",
      "          0.5682, 0.5061],\n",
      "         [0.5796, 0.6706, 0.2643, 0.7849, 0.3789, 0.3226, 0.5094, 0.4561,\n",
      "          0.5167, 0.5426],\n",
      "         [0.6022, 0.7358, 0.5514, 0.5920, 0.6588, 0.5889, 0.7575, 0.4884,\n",
      "          0.6023, 0.6394]],\n",
      "\n",
      "        [[0.5334, 0.3986, 0.5746, 0.3371, 0.3957, 0.3217, 0.4646, 0.5910,\n",
      "          0.5204, 0.5538],\n",
      "         [0.4403, 0.3685, 0.4633, 0.5076, 0.1892, 0.1584, 0.3898, 0.4061,\n",
      "          0.3921, 0.3816],\n",
      "         [0.3966, 0.2372, 0.2155, 0.2006, 0.2958, 0.0841, 0.2306, 0.3723,\n",
      "          0.3784, 0.3761],\n",
      "         [0.3191, 0.1322, 0.1352, 0.1165, 0.4125, 0.6322, 0.1719, 0.1917,\n",
      "          0.3979, 0.5145],\n",
      "         [0.2194, 0.0305, 0.2655, 0.3394, 0.1132, 0.7171, 0.3899, 0.1295,\n",
      "          0.3967, 0.5292],\n",
      "         [0.3790, 0.2358, 0.4655, 0.3557, 0.0176, 0.0778, 0.2682, 0.4606,\n",
      "          0.4644, 0.5175],\n",
      "         [0.2858, 0.4031, 0.1611, 0.6064, 0.2466, 0.4637, 0.3008, 0.3484,\n",
      "          0.4216, 0.3803],\n",
      "         [0.1644, 0.0213, 0.4476, 0.0193, 0.1373, 0.3886, 0.3526, 0.7020,\n",
      "          0.4024, 0.3521],\n",
      "         [0.3806, 0.2132, 0.3136, 0.1370, 0.4868, 0.1041, 0.0136, 0.2860,\n",
      "          0.1943, 0.4457],\n",
      "         [0.5832, 0.2872, 0.1284, 0.2312, 0.1305, 0.1080, 0.3271, 0.6620,\n",
      "          0.4983, 0.3975],\n",
      "         [0.3943, 0.0100, 0.7789, 0.0184, 0.9822, 0.0381, 0.5811, 0.4881,\n",
      "          0.4296, 0.4390],\n",
      "         [0.4253, 0.1611, 0.1553, 0.4081, 0.0284, 0.2302, 0.1302, 0.5872,\n",
      "          0.4997, 0.4175],\n",
      "         [0.3850, 0.0887, 0.0210, 0.2384, 0.6630, 0.2027, 0.3761, 0.1949,\n",
      "          0.4328, 0.4715],\n",
      "         [0.3624, 0.0502, 0.4160, 0.5546, 0.0960, 0.4616, 0.1317, 0.2687,\n",
      "          0.3818, 0.4978],\n",
      "         [0.3933, 0.4123, 0.2636, 0.2786, 0.2849, 0.0647, 0.1307, 0.2089,\n",
      "          0.1721, 0.3504],\n",
      "         [0.4249, 0.5058, 0.5091, 0.0040, 0.8351, 0.0193, 0.2289, 0.5312,\n",
      "          0.1856, 0.5755],\n",
      "         [0.4919, 0.1632, 0.2302, 0.2658, 0.1769, 0.2318, 0.5723, 0.3935,\n",
      "          0.2354, 0.3380],\n",
      "         [0.3275, 0.0140, 0.3366, 0.0314, 0.2383, 0.6201, 0.8438, 0.4252,\n",
      "          0.3316, 0.4562],\n",
      "         [0.2318, 0.1097, 0.0512, 0.4722, 0.3711, 0.1553, 0.4041, 0.2507,\n",
      "          0.3935, 0.5586],\n",
      "         [0.4100, 0.2279, 0.2793, 0.1279, 0.6885, 0.2342, 0.4253, 0.4722,\n",
      "          0.4318, 0.4939],\n",
      "         [0.4204, 0.3294, 0.7357, 0.2151, 0.6211, 0.6774, 0.4906, 0.5439,\n",
      "          0.4833, 0.4574],\n",
      "         [0.3978, 0.2642, 0.4486, 0.4080, 0.3412, 0.4111, 0.2425, 0.5116,\n",
      "          0.3977, 0.3606]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "class SimplifiedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mix = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(32, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(4, 16, 4)),\n",
    "            nn.ConvTranspose2d(4, 8, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 16, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 2, kernel_size=3, padding=1),\n",
    "            nn.LogSoftmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, height, width = x.shape\n",
    "        \n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        z = z[:, None, None, None] # Expand dims to match x\n",
    "        z = z.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, z), dim=1)\n",
    "\n",
    "        x = self.enc(x)\n",
    "        x = self.mix(x)\n",
    "        logits = self.dec(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = SimplifiedModel().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.rand(batch_size)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.5253, 0.5938, 0.6525, 0.8623, 0.7962, 0.9134, 0.8037, 0.7639,\n",
      "          0.6708, 0.5605],\n",
      "         [0.5933, 0.8008, 0.6870, 0.7128, 0.8552, 0.7973, 0.8335, 0.7914,\n",
      "          0.7639, 0.5898],\n",
      "         [0.5776, 0.7346, 0.8057, 0.8515, 0.3866, 0.0689, 0.2169, 0.6137,\n",
      "          0.4699, 0.5636],\n",
      "         [0.5891, 0.7054, 0.4541, 0.3314, 0.9460, 0.7289, 0.9162, 0.5052,\n",
      "          0.8747, 0.4521],\n",
      "         [0.7491, 0.6850, 0.7335, 0.8721, 0.1090, 0.4391, 0.3388, 0.6621,\n",
      "          0.7372, 0.5803],\n",
      "         [0.6698, 0.6273, 0.7377, 0.6579, 0.5504, 0.5643, 0.4946, 0.5593,\n",
      "          0.7364, 0.6567],\n",
      "         [0.4680, 0.5701, 0.8932, 0.8235, 0.9161, 0.3213, 0.6590, 0.8200,\n",
      "          0.4780, 0.5782],\n",
      "         [0.6772, 0.7321, 0.7645, 0.9581, 0.3134, 0.9361, 0.5085, 0.8340,\n",
      "          0.8086, 0.3898],\n",
      "         [0.6662, 0.6978, 0.9796, 0.5574, 0.8998, 0.7225, 0.1592, 0.8208,\n",
      "          0.5472, 0.4778],\n",
      "         [0.6726, 0.6655, 0.6492, 0.9727, 0.7041, 0.7858, 0.7881, 0.9168,\n",
      "          0.1374, 0.7096],\n",
      "         [0.7495, 0.6545, 0.8594, 0.4517, 0.6843, 0.9191, 0.9325, 0.8101,\n",
      "          0.7769, 0.5965],\n",
      "         [0.8005, 0.7420, 0.6686, 0.9201, 0.9915, 0.6170, 0.5522, 0.8591,\n",
      "          0.7319, 0.5467],\n",
      "         [0.7412, 0.9077, 0.9600, 0.7520, 0.9055, 0.8580, 0.7169, 0.0340,\n",
      "          0.8921, 0.4539],\n",
      "         [0.5821, 0.6357, 0.9389, 0.9017, 0.9089, 0.9884, 0.0958, 0.7000,\n",
      "          0.5775, 0.3791],\n",
      "         [0.6686, 0.8216, 0.8738, 0.4377, 0.4340, 0.9962, 0.9948, 0.3892,\n",
      "          0.7926, 0.7151],\n",
      "         [0.5636, 0.6788, 0.7924, 0.7177, 0.9084, 0.7827, 0.7088, 0.8459,\n",
      "          0.4983, 0.4972],\n",
      "         [0.6439, 0.7723, 0.8110, 0.5357, 0.7774, 0.8181, 0.7852, 0.9009,\n",
      "          0.6824, 0.4526],\n",
      "         [0.5289, 0.6140, 0.9556, 0.8047, 0.9036, 0.9209, 0.7838, 0.9682,\n",
      "          0.9018, 0.4412],\n",
      "         [0.7410, 0.7825, 0.8931, 0.7628, 0.9379, 0.7245, 0.8990, 0.8066,\n",
      "          0.8893, 0.6392],\n",
      "         [0.4653, 0.3631, 0.8335, 0.8695, 0.8744, 0.9808, 0.3013, 0.7333,\n",
      "          0.5886, 0.4812],\n",
      "         [0.5982, 0.5415, 0.3121, 0.6536, 0.7318, 0.8250, 0.9236, 0.5756,\n",
      "          0.6329, 0.5343],\n",
      "         [0.5894, 0.6135, 0.6496, 0.5550, 0.5749, 0.8006, 0.7020, 0.5264,\n",
      "          0.5095, 0.4517]],\n",
      "\n",
      "        [[0.4747, 0.4062, 0.3475, 0.1377, 0.2038, 0.0866, 0.1963, 0.2361,\n",
      "          0.3292, 0.4395],\n",
      "         [0.4067, 0.1992, 0.3130, 0.2872, 0.1448, 0.2027, 0.1665, 0.2086,\n",
      "          0.2361, 0.4102],\n",
      "         [0.4224, 0.2654, 0.1943, 0.1485, 0.6134, 0.9311, 0.7831, 0.3863,\n",
      "          0.5301, 0.4364],\n",
      "         [0.4109, 0.2946, 0.5459, 0.6686, 0.0540, 0.2711, 0.0838, 0.4948,\n",
      "          0.1253, 0.5479],\n",
      "         [0.2509, 0.3150, 0.2665, 0.1279, 0.8910, 0.5609, 0.6612, 0.3379,\n",
      "          0.2628, 0.4197],\n",
      "         [0.3302, 0.3727, 0.2623, 0.3421, 0.4496, 0.4357, 0.5054, 0.4407,\n",
      "          0.2636, 0.3433],\n",
      "         [0.5320, 0.4299, 0.1068, 0.1765, 0.0839, 0.6787, 0.3410, 0.1800,\n",
      "          0.5220, 0.4218],\n",
      "         [0.3228, 0.2679, 0.2355, 0.0419, 0.6866, 0.0639, 0.4915, 0.1660,\n",
      "          0.1914, 0.6102],\n",
      "         [0.3338, 0.3022, 0.0204, 0.4426, 0.1002, 0.2775, 0.8408, 0.1792,\n",
      "          0.4528, 0.5222],\n",
      "         [0.3274, 0.3345, 0.3508, 0.0273, 0.2959, 0.2142, 0.2119, 0.0832,\n",
      "          0.8626, 0.2904],\n",
      "         [0.2505, 0.3455, 0.1406, 0.5483, 0.3157, 0.0809, 0.0675, 0.1899,\n",
      "          0.2231, 0.4035],\n",
      "         [0.1995, 0.2580, 0.3314, 0.0799, 0.0085, 0.3830, 0.4478, 0.1409,\n",
      "          0.2681, 0.4533],\n",
      "         [0.2588, 0.0923, 0.0400, 0.2480, 0.0945, 0.1420, 0.2831, 0.9660,\n",
      "          0.1079, 0.5461],\n",
      "         [0.4179, 0.3643, 0.0611, 0.0983, 0.0911, 0.0116, 0.9042, 0.3000,\n",
      "          0.4225, 0.6209],\n",
      "         [0.3314, 0.1784, 0.1262, 0.5623, 0.5660, 0.0038, 0.0052, 0.6108,\n",
      "          0.2074, 0.2849],\n",
      "         [0.4364, 0.3212, 0.2076, 0.2823, 0.0916, 0.2173, 0.2912, 0.1541,\n",
      "          0.5017, 0.5028],\n",
      "         [0.3561, 0.2277, 0.1890, 0.4643, 0.2226, 0.1819, 0.2148, 0.0991,\n",
      "          0.3176, 0.5474],\n",
      "         [0.4711, 0.3860, 0.0444, 0.1953, 0.0964, 0.0791, 0.2162, 0.0318,\n",
      "          0.0982, 0.5588],\n",
      "         [0.2590, 0.2175, 0.1069, 0.2372, 0.0621, 0.2755, 0.1010, 0.1934,\n",
      "          0.1107, 0.3608],\n",
      "         [0.5347, 0.6369, 0.1665, 0.1305, 0.1256, 0.0192, 0.6987, 0.2667,\n",
      "          0.4114, 0.5188],\n",
      "         [0.4018, 0.4585, 0.6879, 0.3464, 0.2682, 0.1750, 0.0764, 0.4244,\n",
      "          0.3671, 0.4657],\n",
      "         [0.4106, 0.3865, 0.3504, 0.4450, 0.4251, 0.1994, 0.2980, 0.4736,\n",
      "          0.4905, 0.5483]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "class SimplifiedModelWithRandomnessLater(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc_x = nn.Sequential(\n",
    "            nn.Conv2d(2, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.enc_z = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mix = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(32, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(4, 16, 4)),\n",
    "            nn.ConvTranspose2d(4, 8, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 16, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 2, kernel_size=3, padding=1),\n",
    "            nn.LogSoftmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        s = self.enc_x(x)\n",
    "\n",
    "        z = z[:, None] # Expand dims to match x\n",
    "        v = self.enc_z(z)\n",
    "        s = torch.cat((s, v), dim=1)\n",
    "\n",
    "        s = self.mix(s)\n",
    "        logits = self.dec(s)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = SimplifiedModelWithRandomnessLater().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.rand(batch_size)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TetrisModel has 12828 parameters.\n",
      "WiderLocalModel has 25020 parameters.\n",
      "WiderGlobalModel has 26674 parameters.\n",
      "WiderAllModel has 48018 parameters.\n",
      "SimplifiedModel has 26830 parameters.\n",
      "SimplifiedModelWithRandomnessLater has 28830 parameters.\n"
     ]
    }
   ],
   "source": [
    "for model_class in [TetrisModel, WiderLocalModel, WiderGlobalModel, WiderAllModel, SimplifiedModel, SimplifiedModelWithRandomnessLater]:\n",
    "    print(f\"{model_class.__name__} has {count_parameters(model_class())} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model TetrisModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Training model WiderLocalModel...\n",
      "Done!\n",
      "Training model WiderGlobalModel...\n",
      "Done!\n",
      "Training model WiderAllModel...\n",
      "Done!\n",
      "Training model SimplifiedModel...\n",
      "Done!\n",
      "Training model SimplifiedModelWithRandomnessLater...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model WiderLocalModel...\n",
      "Done!\n",
      "Training model WiderGlobalModel...\n",
      "Done!\n",
      "Training model WiderAllModel...\n",
      "Done!\n",
      "Training model SimplifiedModel...\n",
      "Done!\n",
      "Training model SimplifiedModelWithRandomnessLater...\n",
      "Done!\n",
      "Results:\n",
      "Class: TetrisModel\n",
      "  Train accuracy mean 0.93141, std 0.01020\n",
      "  Test accuracy mean 0.89773, std 0.01818\n",
      "  Train spawn recall mean 0.72000, std 0.27200\n",
      "  Test spawn recall mean 0.73750, std 0.26250\n",
      "\n",
      "Class: WiderLocalModel\n",
      "  Train accuracy mean 0.94274, std 0.00000\n",
      "  Test accuracy mean 0.91705, std 0.00114\n",
      "  Train spawn recall mean 0.44400, std 0.02000\n",
      "  Test spawn recall mean 0.45000, std 0.02500\n",
      "\n",
      "Class: WiderGlobalModel\n",
      "  Train accuracy mean 0.94246, std 0.00028\n",
      "  Test accuracy mean 0.91591, std 0.00000\n",
      "  Train spawn recall mean 0.39600, std 0.02000\n",
      "  Test spawn recall mean 0.47500, std 0.02500\n",
      "\n",
      "Class: WiderAllModel\n",
      "  Train accuracy mean 0.94246, std 0.00028\n",
      "  Test accuracy mean 0.91705, std 0.00114\n",
      "  Train spawn recall mean 0.40000, std 0.02400\n",
      "  Test spawn recall mean 0.46250, std 0.03750\n",
      "\n",
      "Class: SimplifiedModel\n",
      "  Train accuracy mean 0.00624, std 0.00624\n",
      "  Test accuracy mean 0.00341, std 0.00341\n",
      "  Train spawn recall mean 0.00000, std 0.00000\n",
      "  Test spawn recall mean 0.00000, std 0.00000\n",
      "\n",
      "Class: SimplifiedModelWithRandomnessLater\n",
      "  Train accuracy mean 0.01247, std 0.01020\n",
      "  Test accuracy mean 0.00568, std 0.00341\n",
      "  Train spawn recall mean 0.00000, std 0.00000\n",
      "  Test spawn recall mean 0.00000, std 0.00000\n",
      "\n",
      "Performing t-tests of WiderLocalModel against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=-1.0604308646380671, pvalue=0.4802548153696362)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=1.090304565221948, pvalue=0.4699723249328325)\n",
      "\n",
      "Performing t-tests of WiderGlobalModel against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=-1.0, pvalue=0.49999999999999956)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=0.9954954725939524, pvalue=0.49907469076324706)\n",
      "\n",
      "Performing t-tests of WiderAllModel against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=-1.0604308646380671, pvalue=0.4802548153696362)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=1.0370899457402698, pvalue=0.4830000034373615)\n",
      "\n",
      "Performing t-tests of SimplifiedModel against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=48.345025694831946, pvalue=0.010180548488962713)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=2.8095238095238098, pvalue=0.21769202101168356)\n",
      "\n",
      "Performing t-tests of SimplifiedModelWithRandomnessLater against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=48.22216667146515, pvalue=0.010208301565172872)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=2.8095238095238098, pvalue=0.21769202101168356)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akubi\\AppData\\Local\\Temp\\ipykernel_10248\\476966591.py:33: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  accuracy_ttest = ttest_ind(baseline[\"test_accuracy\"], architecture[\"test_accuracy\"], equal_var=False)\n"
     ]
    }
   ],
   "source": [
    "train_and_compare_architectures([TetrisModel, WiderLocalModel, WiderGlobalModel, WiderAllModel, SimplifiedModel, SimplifiedModelWithRandomnessLater], use_oracle=True, repeats=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the larger local-global based models (`WiderLocalModel`, `WiderGlobalModel` and `WiderAllModel`) have performance comparable to the baseline `TetrisModel`. In terms of loss and board accuracy, they are only marginally better. In terms of spawn recall, they are actually worse, even on the training set. This suggests that the models are not making use of the explanatory randomness at all. Maybe there is a bug in the explanatory randomness computation, or maybe the information needs to be provided to the models in another form, for example by one-hot encoding the block spawn type.\n",
    "\n",
    "The \"simplified\" models (`SimplifiedModel` and `SimplifiedModelWithRandomnessLater`) struggle to learn anywhere near as fast as the local-global based models. The loss decreases very slowly and occasionally has huge spikes. By the end of 100 epochs, the models don't even learn enough to have a board accuracy reliably over 1%. The slow convergence of these models may be due to the aggressive bottleneck that the input `X` goes through to ensure the number of features is small. This is corroborated by the logged predictions, which seem to oscillate between different local structures between training epochs without much attention to detail. We could try to ameliorate this by increasing the number of features in the `mix` module. This can be achieved by increasing the number of filters in the `enc_x` module before it, or by reducing the amount of downscaling that happens in the `enc_x` module.\n",
    "\n",
    "The `SimplifiedModelWithRandomnessLater` model has more frequent loss spikes than `SimplifiedModel`, which might be because of the lack of batch normalization after the second linear layer in the `enc_z` module. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the explanatory randomness calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 673\n",
      "Block spawn type: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1726daa71f0>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test spawn type calculation\n",
    "\n",
    "import random\n",
    "\n",
    "is_block_spawn = False\n",
    "while not is_block_spawn:\n",
    "    idx = random.randrange(len(train_dataset))\n",
    "    x, y = train_dataset[idx]\n",
    "    is_block_spawn = (x[0, :] == 0).all(-1) & (y[0, :] == 1).any(-1)\n",
    "print(f\"Index: {idx}\")\n",
    "print(f\"Block spawn type: {get_spawn_type(y)}\")\n",
    "plt.imshow(x)\n",
    "plt.imshow(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this works correctly. The block spawn types are `0: I, 1: J, 2: T, 3: Z, 4: S, 5: L, 6: O`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABL0AAAHUCAYAAADfglqUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVtUlEQVR4nO3dd3wU1f7/8XcS0hN6SOgQ6TXSQQgovUgHERGlCQIiNlCUi6BSRESuAoIgqCDSFSuC4iUiAlGQqoAUwdBDSShJSM7vD367X5bdJLu0wOT1fDx43MvsmbNnxp0Ps++dOeNljDECAAAAAAAALMQ7qwcAAAAAAAAA3GyEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF7IcnPnzpWXl5cOHDiQ1UMBgCxFPQQAAIAN54Y3jtAL1+3VV1+Vl5eXyz/vv/9+Vg8PAG4b6iEAXHHgwIF06+G1f/gSB8CqODe8c+TI6gHg7jd9+nSFhIQ4LKtdu3YWjQYAsg71EEB2FxYWpk8++cRh2aRJk3T48GFNnjzZqS0AWBnnhlmP0As3rHPnzsqfP39WDwMAshz1EEB2FxwcrB49ejgs++yzz3T69Gmn5VczxujSpUsKDAy81UMEgNuGc8Osx+2NkhISEjR06FCVKFFC/v7+KlCggJo2barff//d3qZRo0aqVKmSfvvtN9WrV0+BgYEqWbKk06WJycnJ+s9//qPq1asrV65cCg4OVoMGDbRmzRqHdtWqVVPHjh0dllWuXFleXl7aunWrfdnChQvl5eWlXbt2Sfq/yyT37t2rxx9/XLlz51auXLnUq1cvXbhw4WbvmhuydetWPf7444qMjFRAQIAiIiLUu3dvnTp1KtN1Y2Nj1bx5c+XPn9++r3v37u3QJi0tTe+8844qVqyogIAAhYeHq3///jp9+vSt2iTA8qiHtwb1ELh7URdvjRIlSqhNmzZauXKlatSoocDAQM2YMcN+e+TcuXOd1vHy8tKrr77qsOzff/9V7969FR4eLn9/f1WsWFEffvjh7dkIIBugBt4anBvePlzpJWnAgAFasmSJBg8erAoVKujUqVP6+eeftWvXLlWrVs3e7vTp02rVqpW6du2qhx9+WIsWLdKTTz4pPz8/+4fs3LlzmjVrlh5++GH169dPCQkJmj17tpo3b66NGzcqKipKktSgQQMtWLDA3nd8fLx27Nghb29vxcTEqEqVKpKkmJgYhYWFqXz58g5j7tq1q0qWLKlx48bp999/16xZs1SgQAFNmDAhw229cOGCWwe8j4+P8uTJ49b+i4+Pd7nuqlWrtG/fPvXq1UsRERHasWOHZs6cqR07dujXX3+Vl5eXy/6OHz+uZs2aKSwsTC+++KJy586tAwcOaNmyZQ7t+vfvr7lz56pXr14aMmSI9u/fr/fee0+bN2/WunXr5Ovr69b4Afwf6qEz6iGQvVEXnXlSFzPy119/6eGHH1b//v3Vr18/lS1b1qP1jx07pjp16sjLy0uDBw9WWFiYvv32W/Xp00fnzp3T0KFDb3iMQHZHDXTGueFdxsDkypXLDBo0KMM2DRs2NJLMpEmT7MuSkpJMVFSUKVCggElOTjbGGHP58mWTlJTksO7p06dNeHi46d27t33Z4sWLjSSzc+dOY4wxK1asMP7+/qZt27bmoYcesrerUqWK6dChg/3vo0aNMpIc+jLGmA4dOph8+fJluq229TP7U7x48evuy7buhQsXnNZZsGCBkWTWrl1rXzZnzhwjyezfv98YY8zy5cuNJLNp06Z03zsmJsZIMvPnz3dY/t1337lcDsA91EPqIQBH1MXrq4tXa926tdM6xYsXN5LMd99957B8//79RpKZM2eOUz+SzKhRo+x/79OnjylYsKA5efKkQ7tu3bqZXLlyuay9ADxDDeTc8G7HlV6ScufOrQ0bNiguLk6FChVKt12OHDnUv39/+9/9/PzUv39/Pfnkk/rtt99Up04d+fj4yMfHR9KVSwrPnDmjtLQ01ahRw+ES0AYNGkiS1q5dq/LlyysmJkY1a9ZU06ZNNW7cOEnSmTNntH37dj3++ONOYxkwYIDD3xs0aKDly5fr3LlzypkzZ7rb0LNnT9WvXz/TfeLJfApLly51eE/bulf3cenSJSUmJqpOnTqSpN9//92+D66VO3duSdJXX32lqlWrukyhFy9erFy5cqlp06Y6efKkfXn16tUVEhKiNWvWqHv37m5vA4ArqIfOqIdA9kZddHaz5t0qWbKkmjdvfl3rGmO0dOlSde3aVcYYh/rXvHlzffbZZ/r9999133333ZSxAtkVNdAZ54Z3F0IvSW+++aYee+wxFS1aVNWrV1erVq3Us2dPRUZGOrQrVKiQgoODHZaVKVNG0pXHM9s+pB999JEmTZqkP//8UykpKfa2JUuWtP//8PBwlS5dWjExMerfv79iYmJ0//33Kzo6Wk899ZT27dunXbt2KS0tzeUHvlixYg5/t11eefr06QwP5MjISKftulHR0dEuJ+eLj4/X6NGj9dlnn+n48eMOr509ezbd/ho2bKhOnTpp9OjRmjx5sho1aqT27dure/fu8vf3lyTt2bNHZ8+eVYECBVz2ce37AXAP9fDGUA8B66Eu3jpXb7OnTpw4oTNnzmjmzJmaOXOmyzbUP+DGUQNvDOeGWY/QS1fu+bWlv99//70mTpyoCRMmaNmyZWrZsqVHfc2bN0+PP/642rdvrxdeeEEFChSQj4+Pxo0bp7///tuhbf369fXDDz/o4sWL+u233/Sf//xHlSpVUu7cuRUTE6Ndu3YpJCRE9957r9P72BLyaxljMhxfYmKiEhMTM90OHx+fG36MdNeuXfXLL7/ohRdeUFRUlEJCQpSWlqYWLVooLS0t3fW8vLy0ZMkS/frrr/ryyy+1cuVK9e7dW5MmTdKvv/5q76dAgQKaP3++yz54BDZwfaiHzqiHQPZGXXR2M+qi5PpqifTmsUlNTXX4u6129ujRQ4899pjLdWzz/gC4ftRAZ5wb3mWy8t7KO9WxY8dM4cKFzX333Wdf1rBhQ5MjRw6TmJjo0Hb69OlGklm/fr0xxph27dqZyMhIk5aW5tCuXr16Tvf+fvjhh0aS+fDDD423t7c5c+aMMcaYNm3amD59+ph69eqZZs2aOaxjuzf4xIkTDsuvvdc3PbfiPuVrx2KMMfHx8UaSGT16tMPy3bt3O83H4M7Y58+fbySZDz74wBhjzMCBA42Pjw9zNQC3GPWQegjAEXXx5s3p1bp1a6e2Z8+eNZLM5MmTHZb//fffDjXz8uXLJjQ01Dz88MMejQXAjaEGcm54t8n2V3qlpqYqMTFRuXLlsi8rUKCAChUqpKSkJIe2ly9f1owZM/Tss89KuvLI1RkzZigsLEzVq1eX9H+psjHG/kvVhg0btH79eqfLLG2XYk6YMEFVqlSxj6FBgwaaPn264uLiNHLkyJu6vbdrroar98PV3nnnnUzXPX36tHLnzu3wS5/tSR62/yZdu3bVtGnT9Nprr2ns2LEO61++fFmJiYn2+50BuId66Br1EMi+qIuu3aw5vVzJmTOn8ufPr7Vr1zo8fXHatGkO7Xx8fNSpUyd9+umn2r59uypVquTw+okTJ7Lf1QzATUYNdI1zw7tLtg+9EhISVKRIEXXu3FlVq1ZVSEiIVq9erU2bNmnSpEkObQsVKqQJEybowIEDKlOmjBYuXKgtW7Zo5syZ9gnk2rRpo2XLlqlDhw5q3bq19u/fr/fff18VKlRwulSyVKlSioiI0F9//aWnnnrKvjw6OlrDhw+XpHQnsLtet2uuhpw5cyo6OlpvvvmmUlJSVLhwYX3//ffav39/put+9NFHmjZtmjp06KB77rlHCQkJ+uCDD5QzZ061atVK0pV7mfv3769x48Zpy5YtatasmXx9fbVnzx4tXrxYU6ZMUefOnW/1ZgKWQj28NaiHwN2Lupg1+vbtq/Hjx6tv376qUaOG1q5dq927dzu1Gz9+vNasWaPatWurX79+qlChguLj4/X7779r9erVio+Pz4LRA9ZBDbw1ODe8zbLwKrM7QlJSknnhhRdM1apVTWhoqAkODjZVq1Y106ZNc2jXsGFDU7FiRRMbG2vq1q1rAgICTPHixc17773n0C4tLc2MHTvWFC9e3Pj7+5t7773XfPXVV+axxx5zeRlkly5djCSzcOFC+7Lk5GQTFBRk/Pz8zMWLFx3a3+glmzdTRpdsGmPM4cOHTYcOHUzu3LlNrly5TJcuXUxcXFyml2z+/vvv5uGHHzbFihUz/v7+pkCBAqZNmzYmNjbW6T1mzpxpqlevbgIDA01oaKipXLmyGTZsmImLi7sVmwxYGvXw+lEPAWuiLt4cntzeaIwxFy5cMH369DG5cuUyoaGhpmvXrub48eNONdOYK7daDRo0yBQtWtT4+vqaiIgI07hxYzNz5sxbtDVA9kENvH6cG945vIzJZDY3SJIaNWqkkydPavv27Vk9FADIUtRDAHBEXQSQnVEDcSfzzuoBAAAAAAAAADcboRcAAAAAAAAsh9ALAAAAAAAAlsOcXgAAAAAAALAcrvQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5d2XolZaWpjfffFMlS5ZUQECAqlSpogULFlxXX/369ZOXl5fatGnj9NozzzyjatWqKW/evAoKClL58uX16quvKjEx0altUlKShg8frkKFCikwMFC1a9fWqlWrnNqlpKRo9OjRioyMlL+/vyIjI/X666/r8uXL192nJCUnJ2vs2LEqV66cAgICFB4ertatW+vw4cMO7fbs2aNu3bqpSJEiCgoKUrly5TRmzBhduHDhusaZmJioUaNGqUWLFsqbN6+8vLw0d+5cl2OUpEWLFqlOnTrKnTu38uXLp4YNG+rrr792aPPnn39q2LBhioqKUmhoqAoWLKjWrVsrNjbWqb9ly5bpoYceUmRkpIKCglS2bFk999xzOnPmjFPbS5cuady4capQoYKCgoJUuHBhdenSRTt27HBod+TIEb344ou6//77FRoaKi8vL/3000/pbpPNmTNnVKBAAXl5eWnJkiVOr//2229q0aKFcubMqdDQUDVr1kxbtmy5oT6Rvd1ILfTkc/7999+rT58+qlSpknx8fFSiRAmX7V599VV5eXml+2fdunX2cc+dO1dt27ZV0aJFFRwcrEqVKun111/XpUuXHPqcO3duhn3Onz8/0/cPCAhwOd7Zs2erfPnyCggIUOnSpfXuu++6vU2u+jx27Jh69eqlAgUKKDAwUNWqVdPixYud2rlbt06dOqWJEycqOjpaYWFhyp07t+rUqaOFCxc69bljxw516dLF3mf+/PkVHR2tL7/80qntxo0bNXDgQFWvXl2+vr7y8vJyuX/upn0P3K56OHbsWNWpU0dhYWH2z+/QoUN14sSJ6x6Tu8fktX7++Wf7cXHy5Emn11evXq37779f+fPnV+7cuVWrVi198sknLvs6duyY+vfvr8KFCysgIEAlSpRQnz59nNr9+++/6tq1q3Lnzq2cOXOqXbt22rdv33WP05PjfPr06erSpYuKFSsmLy8vPf744y7f74cfflDv3r1VpkwZBQUFKTIyUn379tWRI0cc2l24cEFTp05Vs2bNVLBgQYWGhuree+/V9OnTlZqa6tTvG2+8obZt2yo8PFxeXl569dVXM9xum6ZNm8rLy0uDBw92+bq7+16SFi5cqLp16yo4OFi5c+dWvXr19OOPP7o1DmRvN1Ij3T2mbO/z/vvvKyoqSiEhIQoPD1fLli31yy+/OLV197uRu+ehkmfHqSc10iazuutujXT3nPGvv/7SM888o3r16ikgIEBeXl46cOCAy7GVKFHCZT0dMGBAutv/wAMPKFeuXAoNDVX16tUdzjF/+umnDM8D33jjDYf+Vq1apfr16ysoKEh58uRR586dXY41MTFRQ4cOVZEiReTv76/y5ctr+vTpTu3Wrl1r/64QEBCgiIgItWjRwv59whM5PF7jDvDyyy9r/Pjx6tevn2rWrKkvvvhC3bt3l5eXl7p16+Z2P7GxsZo7d266J9CbNm1SgwYN1KtXLwUEBGjz5s0aP368Vq9erbVr18rb+/8yw8cff1xLlizR0KFDVbp0ac2dO1etWrXSmjVrVL9+fXu7Hj16aPHixerdu7dq1KihX3/9VSNHjtQ///yjmTNnOry/u32mpKSodevW+uWXX9SvXz9VqVJFp0+f1oYNG3T27FkVKVJEknTo0CHVqlVLuXLl0uDBg5U3b16tX79eo0aN0m+//aYvvvjC43GePHlSY8aMUbFixVS1atUMw6F3331XQ4YMUevWrTV+/HhdunRJc+fOVZs2bbR06VJ17NhRkjRr1izNnj1bnTp10sCBA3X27FnNmDFDderU0XfffacmTZrY+3ziiSdUqFAh9ejRQ8WKFdO2bdv03nvv6ZtvvtHvv/+uwMBAe9tHHnlEK1asUL9+/VStWjXFxcVp6tSpqlu3rrZt26bixYtLulJcJkyYoNKlS6ty5cpav359utt0tf/85z9O4aHN77//rvr166to0aIaNWqU0tLSNG3aNDVs2FAbN25U2bJlPe4TuJFa6Mnn/NNPP9XChQtVrVo1FSpUKN12HTt2VKlSpZyWjxgxQomJiapZs6akK180evXqpTp16mjAgAEqUKCAvRb98MMP+vHHH+1f+qKjo12egEyePFl//PGHGjdu7PTa9OnTFRISYv+7j4+PU5sZM2ZowIAB6tSpk5599lnFxMRoyJAhunDhgoYPH+5xn+fOnVP9+vV17NgxPf3004qIiNCiRYvUtWtXzZ8/X927d7e3dbdurV+/Xi+//LJatWqlV155RTly5NDSpUvVrVs37dy5U6NHj7b3efDgQSUkJOixxx5ToUKFdOHCBS1dulRt27bVjBkz9MQTT9jbfvPNN5o1a5aqVKmiyMhI7d6922l776Z9D0i3rx7+9ttvioqKUrdu3RQaGqpdu3bpgw8+0Ndff60tW7YoODjY4zG5e0xeLS0tTU899ZSCg4N1/vx5p9dXrFih9u3bq27duvZgadGiRerZs6dOnjypZ555xt720KFDuu+++yRJAwYMUOHChRUXF6eNGzc69JmYmKj7779fZ8+e1YgRI+Tr66vJkyerYcOG2rJli/Lly+fxOG3cOc4nTJighIQE1apVy+WXbZvhw4crPj5eXbp0UenSpbVv3z699957+uqrr7RlyxZFRERIkvbt26ennnpKjRs31rPPPqucOXNq5cqVGjhwoH799Vd99NFHDv2+8sorioiI0L333quVK1em+/5XW7ZsWYafJ3f3vXQlIBwzZow6d+6sxx9/XCkpKdq+fbv+/fdft8aC7O1GaqS7x5QkvfDCC3r77bfVo0cPDRw4UGfOnNGMGTPUsGFDrVu3TrVq1ZLk2Xcjd89DJfePU09qpE1m9czdGunJOeP69ev13//+VxUqVFD58uUzvWAiKipKzz33nMOyMmXKOLWbM2eO+vTpo6ZNm2rs2LHy8fHRX3/9pUOHDtnblC9f3uV54CeffKLvv/9ezZo1sy/76quv1K5dO1WrVk3jx4/XuXPnNGXKFNWvX1+bN29WWFiYJCk1NVXNmzdXbGysBg0apNKlS9vr7unTpzVixAh7n7t375a3t7cGDBigiIgInT59WvPmzVN0dLS+/vprtWjRIsN94cDcZQ4fPmx8fX3NoEGD7MvS0tJMgwYNTJEiRczly5fd6ictLc3UrVvX9O7d2xQvXty0bt3arfXeeustI8msX7/evmzDhg1Gkpk4caJ92cWLF80999xj6tata1+2ceNGI8mMHDnSoc/nnnvOeHl5mT/++MPjPo0xZsKECcbX19ds2LAhw7G/8cYbRpLZvn27w/KePXsaSSY+Pt7jcV66dMkcOXLEGGPMpk2bjCQzZ84cl+9funRpU7NmTZOWlmZfdvbsWRMSEmLatm1rXxYbG2sSEhIc1j158qQJCwsz9913n8PyNWvWOL3PRx99ZCSZDz74wL7s8OHDRpJ5/vnnHdr++OOPRpJ5++237cvOnTtnTp06ZYwxZvHixUaSy/e52rZt20yOHDnMmDFjjCSzePFih9dbtWpl8uTJY06ePGlfFhcXZ0JCQkzHjh2vq09kbzdaCz35nP/7778mOTnZGGNM69atTfHixd0e5z///GO8vLxMv3797MuSkpLMunXrnNqOHj3aSDKrVq3KsM8LFy6Y0NBQ07RpU4flo0aNMpLMiRMnMl0/X758TnX/kUceMcHBwfZa6Emfb775ppFkfvjhB/uy1NRUU7NmTRMREWGSkpLsy92tW/v27TMHDhxwaJeWlmYeeOAB4+/vbxITEzMc0+XLl03VqlVN2bJlHZYfPXrUXLhwwRhjzKBBg4wnpwJ34r4Hbmc9dGXJkiVGklmwYMF1jel6jsnp06ebfPnymaefftrlcdK0aVNTqFAhc+nSJfuylJQUc88995gqVao4tG3ZsqUpWbKkwzmKKxMmTDCSzMaNG+3Ldu3aZXx8fMxLL710XeP05Dg/cOCA/RwyODjYPPbYYy7b/e9//zOpqalOyySZl19+2b7sxIkTTufExhjTq1cvI8ns2bPHYfn+/fvt60kyo0aNynC8Fy9eNCVKlLCfx139WbBxd9+vX7/eeHl5OZyvAu660Rrp7jGVkpJiAgMDTefOnR3a7tu3z0gyQ4YMsS/z5LuRJ+eh7h6nntRIm8zqmbs10pNzxlOnTplz584ZY4yZOHGikWTfxmu5m2ns37/fBAYGOvz38ESpUqVM6dKlHZZVqFDBlCpVymHsW7ZsMd7e3ubZZ5+1L1u0aJGRZGbPnu2wfqdOnUxAQIA5duxYhu99/vx5Ex4ebpo3b+7RmN2+vfHAgQMZXt52u3zxxRdKSUnRwIED7cu8vLz05JNP6vDhw25flfPJJ59o+/btTpflZcZ2OeXVt6EsWbJEPj4+Dr+kBwQEqE+fPlq/fr09MY2JiZEkpzS9W7duMsY4XE7obp9paWmaMmWKOnTooFq1auny5cvpXhl07tw5SVJ4eLjD8oIFC8rb21t+fn4ej9Pf398h3c/IuXPn7Lfq2eTMmVMhISEOV2RVr17d4dc+ScqXL58aNGigXbt2OSxv1KiR0/t06NBBkhzaJiQkSHK97ZIc3j80NFR58+Z1a5tsnn76aXXo0EENGjRw+XpMTIyaNGni8CtowYIF1bBhQ3311Vcub5nNrE9kDavUQk8+54UKFZKvr+91jXPBggUyxuiRRx6xL/Pz81O9evWc2ro6dl358ssvlZCQ4NDn1YwxOnfunIwxLl9fs2aNTp065bDvJGnQoEE6f/680y3X7vQZExOjsLAwPfDAA/Zl3t7e6tq1q44ePar//e9/9uXu1q2SJUvar0C18fLyUvv27ZWUlJTpLUU+Pj4qWrSo022T4eHhDjXPE3fivkfWyY710BVX54aejMnTYzI+Pl6vvPKKxowZo9y5c7tsc+7cOeXJk0f+/v72ZTly5FD+/Pkd3uvPP//Ut99+qxdeeEH58uXTpUuXlJKS4rLPJUuWqGbNmvardiWpXLlyaty4sRYtWnRd47Rx5zgvXry4W5+r6Ohoh7sxbMvy5s3rUGPz58+vihUrOq2f3r9FGd1S5cqbb76ptLQ0Pf/88y5f92Tfv/POO4qIiNDTTz8tY4zL80bceaxSI909plJSUnTx4kWn71sFChSQt7e3Q+3x5LuRJ+eh7h6n7tZIG3fqmbs10pNzxrx58yo0NNStbbJJTk7O8Mra999/X6mpqRozZoykK1eouXuOtXHjRu3du9fhPDA+Pl47d+5Uhw4d7HmCJFWtWlXly5fXZ599Zl+WUc5w6dIlhzvPXAkKClJYWJjLqYwy4nboFRYWpk8++cThz4cffqhcuXLZL1dLT0pKik6ePOnWn7S0tAz72rx5s4KDg1W+fHmH5bZLJTdv3pzptiQkJGj48OEaMWJEpoHN5cuXdfLkScXFxen777/XK6+8otDQUPv72d6zTJkyypkzp8sx2S5DTEpKkiSnAykoKEjSlUvmPe1z586diouLU5UqVfTEE08oODhYwcHBqlKlitasWeOwru2LVp8+fbRlyxYdOnRICxcu1PTp0zVkyBD7JfmejNMTjRo10nfffad3331XBw4c0J9//qlBgwbp7NmzevrppzNd/+jRo8qfP79b7SQ5tL3nnntUpEgRTZo0SV9++aUOHz6sjRs3asCAASpZsqRHt8Vea/Hixfrll1/05ptvptsmKSnJZQENCgpScnKytm/f7nGfyBpWqoW3w/z581W0aFFFR0dn2tbVsZten4GBgfZboq8VGRlpn5+gR48eOnbsmMPrtn1To0YNh+XVq1eXt7e3y32XWZ8ZHeNS5nXT3W3PrO358+d18uRJ/f3335o8ebK+/fZbl7chXq87cd8j62TXemiM0cmTJ3X06FH77bk+Pj4OgfatHNPIkSMVERGh/v37p9umUaNG2rFjh0aOHKm9e/fq77//1muvvabY2FgNGzbM3m716tWSrgRvjRs3VmBgoAIDA9WyZUuHuVjS0tK0detWp2PXtk1///23/QdGT8Zpc6uP88TERCUmJt5wjXXXP//8o/Hjx2vChAnpBpru7nvpypxKNWvW1H//+1+FhYXZ57x97733rnuMuPWsXCNdHVO2Oajnzp2r+fPn659//tHWrVv1+OOPK0+ePA4Xc3j63ehmc7dG2mRWzzypkTd6zpiRH3/8UUFBQQoJCVGJEiU0ZcoUpzarV69WuXLl9M0336hIkSIKDQ1Vvnz5NHLkyEw/S7b5XK8OvdLLDmzbFBcXZ6+rSUlJ8vHxcQjHbO0k19t+7tw5nTx5Un/++adGjBih7du3e3xu6/acXsHBwerRo4fDskGDBikxMTHdydVt1q1bp/vvv9+t99m/f3+GCe2RI0fsE9NdzXbFTlxcXKbvMWbMGAUGBrq8V/dasbGxqlu3rv3vZcuW1YoVKxx+ETxy5Ij9/TMak+3e5HXr1qlkyZL2drbE8+p78t3tc8+ePZKuzLGSN29ezZgxQ9KViVZbtGihTZs2qUqVKpKkFi1a6LXXXtPYsWO1YsUKe58vv/yyXn/9dYdtdHecnvjvf/+rkydPasiQIRoyZIikKycUP/zwg8M+diUmJkbr16/XK6+8kun7TJgwQT4+PurcubN9ma+vr5YuXaru3burbdu29uXVq1fXL7/8kukvkOm5ePGinn/+eT3zzDMqUaJEuhMLli1bVr/++qtSU1Pt81QkJydrw4YNkhz3qbt9ImtYqRbeajt27NDWrVs1bNgwt37RfPPNN5UzZ061bNky3Tbx8fH67rvv1L59e6dfvvLkyaPBgwerbt268vf3V0xMjKZOnaqNGzcqNjbW/iPCkSNH5OPjowIFCjis7+fnp3z58jnsO3f7LFu2rFavXq2DBw86XJ3lbt10VbfS2/5Zs2apQYMGLv+NeO655+z/Dnh7e6tjx4437UvRnbrvkXWyaz08duyYw/FXpEgRffrppypXrtwtH9PWrVs1Y8YMffPNNxnObzdy5Ejt379fb7zxhv0cLygoSEuXLlW7du3s7WznkU888YRq1qyphQsX6p9//tHo0aPVpEkTbd26VUFBQYqPj1dSUlKm56a2c0h3x3m7jvN33nlHycnJeuihhzJsl5ycrHfeeUclS5Z0uFrDU88995zuvffeDH9UdXffnz59WidPntS6dev0448/atSoUSpWrJjmzJmjp556Sr6+vm4Fi7j9rFwj0zum5s2bp4ceeshhuyMjI7Vu3TpFRkbal3ny3ehWcLdGSu7VM09q5I2eM6anSpUqql+/vsqWLatTp05p7ty5Gjp0qOLi4jRhwgR7uz179sjHx0e9evXSsGHDVLVqVS1btsz+0Lpx48a57D81NVULFy5UrVq1HObwDQ8PV+7cuZ0mmD916pR27txp36aIiAiVLVtWqamp+vXXXx3mKM9o27t27Wqfn83Pz0/9+/fXyJEjPdo31z2R/ccff6xp06Zp0qRJmR6QVatWzfTAtsnsyquLFy86XIZoY5uM/uLFixmuv3v3bk2ZMkULFixw2c+1KlSooFWrVun8+fP65ZdftHr1aqdLit0dU6tWrVS8eHE9//zzCgoKUvXq1bVhwwa9/PLLypEjh8PY3e3TNpaEhARt3rxZRYsWlSQ98MADKlWqlN58803NmzfPvn6JEiUUHR2tTp06KV++fPr66681duxYRURE2J8q48k4PWF7SlmRIkXUpk0bJSQkaPLkyerYsaNiYmJcToAtScePH1f37t1VsmRJl8n71T799FPNnj1bw4YNU+nSpR1ey5Mnj6KiotSlSxfVqVNHe/fu1bhx49SlSxetWrXqup4INn78eKWkpDhMuufKwIED9eSTT6pPnz4aNmyY0tLS9Prrr9snYr16n7rbJ+4Md2stvB1c/RqUnrFjx2r16tWaNm1ahiH0kiVLlJyc7LLPa68Y7dSpk2rVqqVHHnlE06ZN04svvijpyr659hcmm4CAAId9526fffv21fvvv6+uXbtq8uTJCg8P16JFi7R8+XL7e6Yno7p1tbS0ND3yyCM6c+aMy6cdStLQoUPVuXNnxcXFadGiRUpNTVVycnK6fXriTt33uHNkl3qYN29erVq1SpcuXdLmzZu1bNmy6z439NSQIUPUsmVLhwmEXfH391eZMmXUuXNndezYUampqZo5c6Z69OihVatWqU6dOpL+7zwyIiJCX3/9tf0WpiJFiujhhx/Wp59+qr59+9rH6+42uTvO23Gcr127VqNHj1bXrl0dbidyZfDgwdq5c6e+/vpr5chxfV+T1qxZo6VLl9q/vKfH3X1va3fq1Cl99tln9pChc+fOqly5sl5//XVCr7uEVWpkRsdUaGioKlasqLp166px48Y6evSoxo8fr/bt2ysmJsZ+ZZgn341uBXdrpORePfOkRt7IOWNGrr6oRZJ69eqlli1b6u2339ZTTz1lf7hdYmKi0tLSNH78ePsDhDp16qT4+HhNmTJFI0aMcHlL5Q8//KBjx445fUf19vZW//79NWHCBL300kvq3bu3zp07p2HDhtnPQW3b1L17d40ZM0a9e/fW1KlTVbp0aX3//feaNm1auts+fvx4Pffcczp06JA++ugjJScn6/Lly57tHI9mAPv/Nm/ebAIDA83DDz98PavfkNatW5vIyEin5efPnzeSzIsvvpjh+i1atDANGzZ0WObJRPbz58833t7eZsuWLfZlFStWNA888IBT2x07dhhJ5v3337cv2759u6lQoYKRZCQZf39/M2XKFFOgQAFTtWpVj/u0Tbh6//33O7W9//77TcmSJe1/X7BggQkMDDSHDh1yaPf444+boKAgh4kE3R3n1TKbyL5FixamTZs2DstOnTpl8ubNa7p27epyncTERFOzZk2TK1cus23bNpdtbNauXWsCAgJM8+bNTUpKisNrZ86cMeHh4eatt95yWP7TTz8ZSWbatGku+8xoQlvbJIAffvihfdmaNWvSnXR+xIgRxtfX175Pa9SoYV5++WUjySxfvvy6+kTWuptr4dU8mbjZ3Yns09LSTPHixU2lSpUybfvZZ58ZLy8v06dPn0zbRkdHm7x589onNHVHRESEady4sf3vgwYNMj4+Pi7bhoWFmW7dunncpzFX9mO+fPnsx3hERISZPn26kWSefvppl/1kVLeuNXDgQCPJfPzxx5mOz6Zp06ZODxC5micT2d/J+x5ZLzvWQ5t169YZSebLL7+84TFldEx+9tlnxtfX1/z111/2ZelNBN+/f39TtWpVh8mnk5OTTenSpU2tWrWc3m/06NEO61++fNnkyJHD9OrVyxjzf5NCjxkzxmlcU6dONZLMn3/+6fE405PZcZ7RRPZX27Vrl8mbN6+JioqyTwadHtvk0q+99lqG7TKaIDslJcVUqlTJ9OzZ02G5XExk7+m+9/X1dZpw3PYAmIMHD2Y4ZmQ9q9TIjI4p2+d/8ODBDst3795tfH19zbBhwxyWu/PdyNW2uHMemtlE9u7WSHfrmSc10pjrO2fMbCJ7V7777jsjyXzyySf2ZcHBwS7rhu2hSv/73/9c9tWzZ0/j4+Njjh496vRaUlKS6dOnj/H29rZvU7NmzcyAAQOMJLN582Z72//973+mWLFi9nY5c+a0v3e7du0y3J6kpCRTsWJF06lTJ7f3gTEeTGRvc/r0aXXq1EllypTRrFmz3FonOTlZR48edetPampqhn0VLFhQR48edZpszZYKZ/QY0x9//FHfffednn76aR04cMD+5/Lly7p48aIOHDhgn+w9PbZ5TK6ekK1gwYIuH53sakwVK1bU9u3btX37dsXExCguLk79+vXTyZMnHR4n6m6ftv+9dsJA6cqkgadPn7b/fdq0abr33nvtKa9N27ZtdeHCBYf7ud0dp7v27dun7777zuHWQunKr6X169d3uhxSuvK56dixo7Zu3aovvvhClSpVSrf/P/74Q23btlWlSpW0ZMkSp1/nli5dqmPHjjm9f8OGDZUzZ06X75+Z//znPypcuLAaNWpk/yzZ7lc+ceKEDhw44HBf9BtvvKFjx44pJiZGW7du1aZNm+yv2/app30i69zNtfB2WLdunQ4ePJjpVV6rVq1Sz5491bp1a73//vsZtv3nn38UExOjLl26eDSxftGiRRUfH2//e8GCBZWamqrjx487tEtOTtapU6fc2nfX9inJfoXVxo0btX79eh08eNB+Kb+ruplZ3bra6NGjNW3aNI0fP16PPvpopuO7ekybNm3S7t273V7HlTt93yNrZfd6WK9ePRUsWNB+deutGtMLL7ygLl26yM/Pz36OYJvM99ChQ/ZblJKTkzV79my1bt3aYfJpX19ftWzZUrGxsfZf39M7j/Tx8VG+fPns55F58+aVv7+/W+em7o4zIzfjOD906JCaNWumXLly6ZtvvslwMui5c+dq+PDhGjBggFtTaaTn448/1l9//aX+/fs7fNeQrtyVceDAAfsDpzzZ9wEBAcqXL5/TrVW2W8WvPt/HnccqNTKzY2rt2rXavn270/et0qVLq3z58k7ft9z5bnQreFIj3a1nntRIyfNzxutluwvs6nqaXu3JqJ5cvHhRy5cvV5MmTVzmDn5+fpo1a5bi4uK0du1a/fXXX1q5cqXOnj0rb29vhzu6oqOjtW/fPm3evFk///yz/v33X/uVdZltu5+fn9q2batly5Z5dEWcR9ftXn1rxerVq+0TjmXml19+uWn3JEdFRWnWrFnatWuXKlSoYF9uu4Q4Kioq3XX/+ecfSXI5Ae+///6rkiVLavLkyRo6dGi6fSQlJSktLU1nz551GNOaNWt07tw5h7kH0huTl5eXw9NivvnmG6WlpalJkyYe91m5cmX5+vq6vP81Li7OYZLEY8eOKU+ePE7tbE+KufYyQXfG6S7bhKSuinJKSorTe6elpalnz5764YcftGjRIjVs2DDdvv/++2+1aNFCBQoU0DfffOP05MeM3t8Yo9TUVM8vkdSVz9PevXsd7k+3sT0d5fTp0w63auXJk8fh/uXVq1erSJEi9nlArqdP3H53ey28HebPny8vLy9179493TYbNmxQhw4dVKNGDS1atCjTW0lcPQkyM8YYHThwQPfee699mW3fxMbGqlWrVvblsbGxSktLy3TfuerTxs/Pz2EeGNtExdfWTXfqls3UqVP16quvaujQofbL0N1lOyG4+t+s63E37HtkDerhFZcuXXI6N7zZYzp06JA+/fRTffrpp06vVatWTVWrVtWWLVt06tQpXb58Od1zrrS0NPtr1atXl+Q8j0pycrJOnjxpP4/09vZW5cqVFRsb69Tnhg0bFBkZaf8C7O4403MzjvNTp06pWbNmSkpK0g8//OBynh2bL774Qn379lXHjh01derU635P6cp5XEpKiu677z6n1z7++GN9/PHHWr58udq3b+/Rvo+KitKmTZuUnJzscIu47Qt3ZpOiI+tYpUa6c0x5+n1Pyvy70a3gSY10t555UiNt3D1nvBG2J31fXSOqV6+uPXv26N9//3X4zplRPVmxYkWGT++2CQ8Pt4diqamp+umnn1S7dm2n81wfHx+Hz50n237x4kUZY5SQkOD+k489uSzsP//5j/H29jYrV6706HKy+Ph4s2rVKrf+XLx4McO+Dh06ZHx9fR0uEU5LSzMNGjQwhQsXdrjsNy4uzuzatct+K8bBgwfN8uXLnf6EhYWZGjVqmOXLl5u9e/caY4w5ffq0y1s43nrrLSPJzJ49277s119/NZLMxIkT7csuXbpkSpUqZWrXrp3h9ly4cMFUq1bNFCxY0OESUU/6bNeunfHx8TG7du2yL9u5c6fx8fExAwcOtC9r06aN8fPzc7g80xhj2rdvb7y9vc2///7r8TivltHtjcePHzfe3t6mUaNGDrfZHDp0yISEhJgWLVo4tLfdxjNjxox0x2SMMUeOHDGRkZGmUKFCGV7quWTJEpeXuH7++edGkhk/frzL9TK6zSEmJsbps/Taa68ZSWbYsGFm+fLlGd4G9NlnnxlJDrdc3mifuD3u9lp4rZt9e2NycrLJly+fadCgQbptdu7cafLly2cqVqxo4uPjM31fY4ypUqWKKVasWLq36h0/ftxpme2S8rffftu+7MKFCyZv3rxOt1v36NHDBAUFmVOnTnncpyu7d+82oaGhTu/jbt0y5kqd8Pb2No888ki6222MMceOHXNalpycbKpVq2YCAwNNQkKCy/Xcvb3xbtv3uH2yUz1MTEw058+fd1puO8cYOXLkdY3pahkdk67OYR966CH7bc8//vijMebK7XG5c+c2ZcqUMUlJSfb1ExISTJEiRUy5cuXsyy5dumQKFChgIiMjHfbzjBkzjCSzaNEi+7Lx48cbSWbTpk32ZX/++afx8fExw4cP93icxlz/cZ7R7Y2JiYmmVq1aJjQ01MTGxqbbhzFXbrMJCAgw999/v7l06VKGbW0yum1q165dLrdfkmnVqpVZvny5iYuLM8Z4tu8nT55sJJmZM2fal128eNFERkaaChUquDVuZA0r1Eh3j6nY2FgjyenY/O2334y3t7cZMGBAhuN09d3oWjfj9kZPaqQn9czdGulKeueMV8vo9sZTp045/buSnJxs7rvvPuPn52eOHDnisE2SzIgRI+zLUlNTTf369U3evHld1sK2bduaoKCgdM8nXbHtjyVLlmTY7vjx46ZYsWKmSpUqDrebujq3PX36tClatKgpWrSo2+Mwxhi3r/Tatm2bXnvtNUVHR+v48eMOk6NLcnoyxdXy5Mlz0xLLIkWKaOjQoZo4caJSUlJUs2ZNff7554qJidH8+fMdLvt96aWX9NFHH9nT72LFiqlYsWJOfQ4dOlTh4eFq3769fdlPP/2kIUOGqHPnzipdurSSk5MVExOjZcuWqUaNGg7bW7t2bXXp0kUvvfSSjh8/rlKlSumjjz7SgQMHNHv2bIf36tq1qwoVKqQKFSro3Llz+vDDD7Vv3z59/fXXDumvJ32OHTtWP/zwgx544AH7UxH/+9//Km/evA4Tzb3wwgv69ttv1aBBAw0ePFj58uXTV199pW+//VZ9+/Z1uOTS3XFK0nvvvaczZ87Y0+Evv/xShw8fliQ99dRT9sfy9u7dW7NmzVLjxo3VsWNHJSQkaNq0abp48aJeeukle3/vvPOOpk2bprp16yooKMjps9ahQwcFBwdLuvJEyn379mnYsGH6+eef9fPPP9vbhYeHq2nTppKkBx98UBUrVtSYMWN08OBB+0T27733ngoWLKg+ffo4vIftKR47duyQJH3yySf2vm2XvV/9q4SN7QqsmjVrOnye1q5dqzFjxqhZs2bKly+ffv31V82ZM0ctWrRwmMTVkz6RNaxQC23c+ZxLV55aY5scc+/evTp79qx93apVq+rBBx90GNvKlSt16tSpdH8NSkhIUPPmzXX69Gm98MIL+vrrrx1ev+eee5ye6Lp9+3Zt3bpVL774YrpPgixevLgeeughVa5cWQEBAfr555/12WefKSoqymGS38DAQL322msaNGiQunTpoubNmysmJkbz5s3TG2+84fB0Xnf7lK48+KRLly4qVqyY9u/fr+nTpytv3rxOt226W7c2btyonj17Kl++fGrcuLHDrVPSlVuqbL/Q9e/fX+fOnVN0dLQKFy6so0ePav78+frzzz81adIkh1/YDh48qE8++USS7L9I2v57Fi9e3On2ybth3yNrZLd6uGfPHjVp0kQPPfSQypUrJ29vb8XGxmrevHkqUaKEw7/nnozJ3WPS1TmA7Yqpli1b2ieI9vHx0fPPP69XXnlFderUUc+ePZWamqrZs2fr8OHDDv+d/P39NXHiRD322GOKjo7Wo48+qn/++UdTpkxRgwYNHO6OGDhwoD744AO1bt1azz//vHx9ffX2228rPDxczz33nL2du+O0bZ+7x/mXX36pP/74Q9KVqzG2bt1q309t27a1P638kUce0caNG9W7d2/t2rVLu3btsvcREhJiH9/BgwfVtm1beXl5qXPnzlq8eLHD+1WpUsXep3TlM3Hw4EH77Ylr1661v/+jjz6q4sWLq1y5culeoVKyZEmHfePJvu/fv79mzZqlQYMGaffu3SpWrJh9PF9++aXL90PWs0qNdPeYql69upo2baqPPvpI586dU7NmzXTkyBG9++67CgwMdLibyt3vRpJn56HuHKee1EhP6pm7NVJy/5zx7Nmz9ocX2W4Pfe+995Q7d27lzp3b/iC6FStW6PXXX1fnzp1VsmRJxcfH69NPP9X27dvtD62zadeunRo3bqxx48bp5MmTqlq1qj7//HP9/PPPmjFjhtNk/PHx8fr222/VqVOndO9MmDdvnpYuXaro6GiFhIRo9erVWrRokfr27atOnTo5tG3YsKHq1q2rUqVK6ejRo5o5c6YSExP11VdfOdxu2rJlSxUpUkS1a9dWgQIF9M8//2jOnDmKi4vTwoULXY4jXe6mY7bJtNP7czulpqaasWPHmuLFixs/Pz9TsWJFM2/ePKd2jz32mFuTvbmayH7v3r2mZ8+eJjIy0gQGBpqAgABTsWJFM2rUKJOYmOjUx8WLF83zzz9vIiIijL+/v6lZs6b57rvvnNpNmDDBlCtXzgQEBJg8efKYtm3bOkzsdj19GnMlQW/SpIkJDg42oaGhpl27dmb37t1O7TZs2GBatmxpIiIijK+vrylTpox54403nCZQ9mScxYsXT/dzcfW+T0lJMe+++66JiooyISEhJiQkxNx///0OCbkx//ffzZ0+M2p37QML4uPjzTPPPGPKlClj/P39Tf78+U23bt3Mvn37nLbpej/r6U06v3fvXtOsWTOTP39+4+/vb8qVK2fGjRvn8OuCp30ia1ipFrq7HXPmzEm3natf2rt162Z8fX0drtq52v79+zN8b1d9vvjii0aS2bp1a7r7o2/fvqZChQomNDTU+Pr6mlKlSpnhw4ene3XqzJkzTdmyZY2fn5+55557zOTJk52uZPKkz27dupmiRYsaPz8/U6hQITNgwACXv1K5W7cy2u+65qraBQsWmCZNmpjw8HCTI0cOkydPHtOkSRPzxRdfOL1/Rp/ha+umMXfHvkfWyG718MSJE+aJJ54w5cqVM8HBwcbPz8+ULl3aDB061OUE7e6OydNj8moZTRA/f/58U6tWLZM7d24TGBhoateune4v7gsWLDBVq1Y1/v7+Jjw83AwePNjlsXbo0CHTuXNnkzNnThMSEmLatGlj9uzZk+EYMxqnJ8d5RueHV9fDjM5Lr75CJLPP77VXiDRs2DDdtpldKS05T2Rv4+6+P3bsmHnsscdM3rx5jb+/v6ldu3a63wtwZ7BKjXT3mDLmyhXdY8aMMRUqVDCBgYEmV65cpk2bNk7fIz35buTJeagnx6knNfJqGdVdd2uku+eMGZ0zX73vY2NjzYMPPmgKFy5s/Pz8TEhIiKlfv77DFaNXS0hIME8//bSJiIgwfn5+pnLlyi4/D8YY8/777xtJZsWKFenukw0bNpjo6GiTJ08eExAQYKpWrWref/99l3cIPPPMMyYyMtL4+/ubsLAw0717d/P33387tXvvvfdM/fr1Tf78+U2OHDlMWFiYefDBB83atWvTHUd6vIy5ZkY7AAAAAAAA4C7n8dMbAQAAAAAAgDsdoRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXrfBL7/8oldffVVnzpzJ6qHcVmPHjlWdOnUUFhamgIAAlS5dWkOHDtWJEyeyemgAskB2rYXSlW2vX7++goKCFBERoSFDhigxMTGrhwUgi1APqYcAsmctvHDhgqZOnapmzZqpYMGCCg0N1b333qvp06crNTU1q4dnSYRet8Evv/yi0aNHZ6uDWZJ+++03RUVF6eWXX9bUqVPVrl07zZkzR/Xq1dP58+ezengAbrPsWgu3bNmixo0b68KFC3r77bfVt29fzZw5U126dMnqoQHIItRD6iGA7FkL9+3bp6eeekrGGD377LN66623VLJkSQ0cOFC9e/fO6uFZUo6sHgCsa+nSpU7L6tatq86dO+vLL79Ut27dsmBUAHB7jRgxQnny5NFPP/2knDlzSpJKlCihfv366fvvv1ezZs2yeIQAcHtQDwFkdxEREdq2bZsqVqxoX9a/f3/17t1bc+bM0ciRI1WqVKksHKH1cKXXLfbqq6/qhRdekCSVLFlSXl5e8vLy0oEDB9SwYUNVrVrV5Xply5ZV8+bNJUkHDhyQl5eX3nrrLU2ePFnFixdXYGCgGjZsqO3btzut++eff6pz587KmzevAgICVKNGDa1YseLWbaQHSpQoIUnZKs0HkH1r4blz57Rq1Sr16NHD/gVPknr27KmQkBAtWrToto4HQNajHlIPAWTfWpg/f36HwMumQ4cOkqRdu3bd1vFkB1zpdYt17NhRu3fv1oIFCzR58mTlz59fkhQWFqZHH31U/fr10/bt21WpUiX7Ops2bdLu3bv1yiuvOPT18ccfKyEhQYMGDdKlS5c0ZcoUPfDAA9q2bZvCw8MlSTt27NB9992nwoUL68UXX1RwcLAWLVqk9u3ba+nSpfaDKT2nT592617ioKAgBQUFZdrOGKNTp07p8uXL2rNnj1588UX5+PioUaNGma4LwDqyay3ctm2bLl++rBo1ajgs9/PzU1RUlDZv3pzpewCwFuoh9RBA9q2F6Tl69Kgk2fcDbiKDW27ixIlGktm/f7/D8jNnzpiAgAAzfPhwh+VDhgwxwcHBJjEx0RhjzP79+40kExgYaA4fPmxvt2HDBiPJPPPMM/ZljRs3NpUrVzaXLl2yL0tLSzP16tUzpUuXznSsxYsXN5Iy/TNq1Ci3tv3IkSMO6xUpUsQsXLjQrXUBWEt2rIWLFy82kszatWudXuvSpYuJiIjIdCwArId66Ih6CGRP2bEWupKUlGQqVKhgSpYsaVJSUjxeHxnjSq8slCtXLrVr104LFizQuHHj5OXlpdTUVC1cuFDt27dXcHCwQ/v27durcOHC9r/XqlVLtWvX1jfffKO3335b8fHx+vHHHzVmzBglJCQoISHB3rZ58+YaNWqU/v33X4c+rjV//nxdvHgx07FHRka6tY158+bVqlWrdOnSJW3evFnLli3jCT0AHFi5Ftr68Pf3d3otICDArfcAkH1QDwHA2rXQlcGDB2vnzp36+uuvlSMHEc3Nxh7NYj179tTChQsVExOj6OhorV69WseOHdOjjz7q1LZ06dJOy8qUKWOfA2Hv3r0yxmjkyJEaOXKky/c7fvx4hgfzfffdd51b4pqfn5+aNGkiSWrTpo0aN26s++67TwUKFFCbNm1u6nsBuHtZtRYGBgZKkpKSkpxeu3Tpkv11ALChHgKAdWvhtSZOnKgPPvhAr732mlq1anVL3iO7I/TKYs2bN1d4eLjmzZun6OhozZs3TxEREfagyBNpaWmSpOeff94+ud+1MnsSxIkTJ9y6VzkkJEQhISEej7FevXoqWLCg5s+fT+gFwM6qtbBgwYKSpCNHjji9duTIERUqVCjT9wCQvVAPAcC6tfBqc+fO1fDhwzVgwACnecpw8xB63QZeXl7pvubj46Pu3btr7ty5mjBhgj7//HP169dPPj4+Tm337NnjtGz37t32JyLaLqX09fW9rmIgSTVr1tTBgwczbTdq1Ci9+uqr1/Uely5d0tmzZ69rXQB3r+xYCytVqqQcOXIoNjZWXbt2tS9PTk7Wli1bHJYByD6oh9RDANmzFtp88cUX6tu3rzp27KipU6de15jgHkKv28B2z/GZM2dcvv7oo49q8uTJ6t+/vxITE9WjRw+X7T7//HOHe403btyoDRs2aOjQoZKkAgUKqFGjRpoxY4aeeuop+y9qNidOnFBYWFiGY71Z9yqfP39eXl5eTk+uWLp0qU6fPu305B4A1pcda2GuXLnUpEkTzZs3TyNHjlRoaKgk6ZNPPlFiYqK6dOmS6XsAsB7qIfUQQPashZK0du1adevWTdHR0Zo/f768vb0zXQfXz8sYY7J6EFa3adMm1apVS61atVK3bt3k6+urBx980GECvsqVK2v79u0qX768du7c6bD+gQMHVLJkSVWuXFkJCQl68sknlZSUpHfeeUdeXl7atm2b/cDduXOn6tevL29vb/Xr10+RkZE6duyY1q9fr8OHD+uPP/64Ldu8ZcsWNWnSRA899JDKlSsnb29vxcbGat68eSpSpIhiY2OVL1++2zIWAHeG7FgLJen3339XvXr1VKFCBT3xxBM6fPiwJk2apOjoaK1cufK2jQPAnYN6SD0EkD1r4cGDB1W1alUlJyfrrbfeUs6cOR1er1KliqpUqXJbxpJtZOWjI7OT1157zRQuXNh4e3u7fCzrm2++aSSZsWPHOq1rexTrxIkTzaRJk0zRokWNv7+/adCggfnjjz+c2v/999+mZ8+eJiIiwvj6+prChQubNm3amCVLltyqzXNy4sQJ88QTT5hy5cqZ4OBg4+fnZ0qXLm2GDh1qTpw4cdvGAeDOkt1qoU1MTIypV6+eCQgIMGFhYWbQoEHm3Llzt30cAO4c1EPqIYDsVwvXrFljJKX7Z9SoUbdtLNkFV3rdIaZMmaJnnnlGBw4cULFixRxesyXYEydO1PPPP59FIwSAW49aCABXUA8BgFqIG8fNo3cAY4xmz56thg0bOh3IAJBdUAsB4ArqIQBQC3FzMJF9Fjp//rxWrFihNWvWaNu2bfriiy+yekgAcNtRCwHgCuohAFALcXMRemWhEydOqHv37sqdO7dGjBihtm3bZvWQAOC2oxYCwBXUQwCgFuLmYk4vAAAAAAAAWA5zegEAAAAAAMBy7rjbG9PS0hQXF6fQ0FB5eXll9XAAtxljlJCQoEKFCsnbmzwZN4ZaiLsVtRA3E7UQdytqIW426iHuVlldD++40CsuLk5FixbN6mEA1+3QoUMqUqRIVg8DdzlqIe521ELcDNRC3O2ohbhZqIe422VVPbzjQq/Q0FBJUn21Ug75ZvFoAPddVop+1jf2zzBwI6iFuFtRC3EzWbEWLt+9LauHcEfqUKZyVg/hpqIW4mazYj1E9pDV9fCOC71sl2rmkK9yeHEw4y7y/x8JweXGuBmohbhrUQtxE1mxFuYM5VY3V6zy39eOWoibzIr1ENlEFtdD/tUFAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFgOoRcAAAAAAAAsJ0dWDwAAAADILpoXisrqIQAAkG1wpRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFgOoRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFgOoRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMBycmT1AAAA2cPKuC1ZPYSbrnmhqKweAgAAAIB0cKUXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFgOoRcAAAAAAAAsJ0dWDwAAkD00LxR1U/pZGbflhvu4WWMBAAAAcOfiSi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsJwcWT0AAAA80bxQVFYPAQAAAMBdgCu9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFgOoRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFgOoRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFgOoRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFgOoRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFgOoRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFhOjqweAAAAAAAAyH5Wxm3J6iHckZoXisrqIVgGV3oBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMBycmT1AAAAAAAAQPbTvFBUVg8BFseVXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOTmyegC4vVbGbbkp/TQvFHVT+gEAAIDnbtY53c3AeSEA4E7FlV4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFgOoRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALCcHFk9gOxgZdyWrB4CAAAAAABAtsKVXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOTmyegDZQfNCUVk9BAAAANwBVsZtyeohAACQbXClFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMByCL0AAAAAAABgOYReAAAAAAAAsBxCLwAAAAAAAFgOoRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALCdHVg8AAAAAwO3XvFBUVg8BAIBbiiu9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWA6hFwAAAAAAACyH0AsAAAAAAACWQ+gFAAAAAAAAyyH0AgAAAAAAgOUQegEAAAAAAMBycmT1AAAAAADcfivjttyUfpoXirop/QAAcLNxpRcAAAAAAAAsh9ALAAAAAAAAlkPoBQAAAAAAAMsh9AIAAAAAAIDlEHoBAAAAAADAcgi9AAAAAAAAYDmEXgAAAAAAALAcQi8AAAAAAABYDqEXAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwHEIvAAAAAAAAWE6OrB7AtYwxkqTLSpFMFg8G8MBlpUj6v88wcCOohbhbUQtxM1mxFp5LSMvqIdx0l01KVg/hjkMtxM1mxXqI7CGr6+EdF3olJCRIkn7WN1k8EuD6JCQkKFeuXFk9DNzlqIW421ELcTNYsRbmKZPVI7gV9mX1AO5Y1ELcLFash8hesqoeepk77OeHtLQ0xcXFKTQ0VF5eXlk9HMBtxhglJCSoUKFC8vbmzmHcGGoh7lbUQtxM1ELcraiFuNmoh7hbZXU9vONCLwAAAAAAAOBG8bMDAAAAAAAALIfQCwAAAAAAAJZD6AUAAAAAAADLIfQCAAAAAACA5RB6AQAAAAAAwHIIvQAAAAAAAGA5hF4AAAAAAACwnP8HiValbkOiCFAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1600x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test explanatory randomness calculation\n",
    "\n",
    "import math\n",
    "\n",
    "def get_num_rows_cols(batch_size):\n",
    "    return int(math.ceil(batch_size / 4.0)), 4\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    spawns = (X[:, 0, :] == 0).all(-1) & (y[:, 0, :] == 1).any(-1)\n",
    "    if spawns.any():\n",
    "        break\n",
    "nrows, ncols = get_num_rows_cols(X.shape[0])\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 5*nrows))\n",
    "z = get_explanatory_randomness(X, y)\n",
    "spawn_types = (7.0 * z).type(torch.int)\n",
    "for i in range(X.shape[0]):\n",
    "    ax = axs.flat[i]\n",
    "    ax.set_title(f\"spawn = {spawns[i]}\\nz = {z[i]}\\ntype = {spawn_types[i]}\")\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    ax.imshow(y[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the explanatory randomness is being calculated correctly. So why do the models not learn from it?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot randomness\n",
    "\n",
    "Next, let's try one-hot encoding the spawn type instead of having it as one linear variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SPAWN_TYPES = 7\n",
    "\n",
    "def get_explanatory_onehot(X, y):\n",
    "    \"\"\"Given a batch of training examples X, returns a batch of one-hot features that explain the block spawn type.\n",
    "    \n",
    "    If no block spawns, then the one-hot vector is chosen randomly.\n",
    "    \"\"\"\n",
    "    batch_size, height, width = X.shape\n",
    "\n",
    "    # Allocate output tensor\n",
    "    z = torch.zeros(batch_size, NUM_SPAWN_TYPES)\n",
    "\n",
    "    # Identify block spawns\n",
    "    spawns = (X[:, 0, :] == 0).all(-1) & (y[:, 0, :] == 1).any(-1)\n",
    "\n",
    "    # For each example, modify the random number if it's a block spawn\n",
    "    for i in range(batch_size):\n",
    "        if spawns[i]:\n",
    "            spawn_type = get_spawn_type(y[i])\n",
    "            z[i, spawn_type] = 1.0\n",
    "        else:\n",
    "            rnd = random.randrange(NUM_SPAWN_TYPES)\n",
    "            z[i, rnd] = 1.0\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.3932, 0.3435, 0.3612, 0.3847, 0.3696, 0.4218, 0.3828, 0.4220,\n",
      "          0.3854, 0.4132],\n",
      "         [0.4558, 0.5244, 0.4909, 0.4389, 0.4186, 0.4938, 0.5501, 0.5425,\n",
      "          0.5190, 0.4808],\n",
      "         [0.4401, 0.5018, 0.3354, 0.2004, 0.4962, 0.3854, 0.4769, 0.5359,\n",
      "          0.4592, 0.3907],\n",
      "         [0.4474, 0.6109, 0.5556, 0.4035, 0.4873, 0.4217, 0.4567, 0.4169,\n",
      "          0.4032, 0.3919],\n",
      "         [0.4677, 0.4358, 0.4450, 0.5095, 0.5359, 0.4177, 0.5360, 0.5076,\n",
      "          0.4966, 0.4094],\n",
      "         [0.4489, 0.3997, 0.3187, 0.4879, 0.4423, 0.4778, 0.3600, 0.4071,\n",
      "          0.4600, 0.4148],\n",
      "         [0.4732, 0.4742, 0.2651, 0.1985, 0.2512, 0.3288, 0.5738, 0.4656,\n",
      "          0.5014, 0.5264],\n",
      "         [0.4528, 0.6387, 0.5023, 0.3191, 0.2226, 0.4356, 0.4381, 0.4569,\n",
      "          0.5057, 0.4376],\n",
      "         [0.4441, 0.5279, 0.5279, 0.2957, 0.5815, 0.4816, 0.4517, 0.5330,\n",
      "          0.4429, 0.4428],\n",
      "         [0.4732, 0.4569, 0.6666, 0.6140, 0.4376, 0.3867, 0.6008, 0.4676,\n",
      "          0.4176, 0.4395],\n",
      "         [0.4280, 0.4571, 0.4538, 0.4880, 0.2751, 0.5386, 0.5849, 0.4279,\n",
      "          0.4287, 0.4037],\n",
      "         [0.4295, 0.4846, 0.5520, 0.7465, 0.2269, 0.3227, 0.3666, 0.4524,\n",
      "          0.4814, 0.3957],\n",
      "         [0.4631, 0.3665, 0.4140, 0.4959, 0.2677, 0.4043, 0.3733, 0.4911,\n",
      "          0.4310, 0.3919],\n",
      "         [0.4676, 0.4540, 0.2969, 0.3121, 0.4243, 0.4054, 0.5144, 0.5650,\n",
      "          0.4258, 0.3919],\n",
      "         [0.4218, 0.6187, 0.3794, 0.2876, 0.4677, 0.4751, 0.4280, 0.3813,\n",
      "          0.4324, 0.3919],\n",
      "         [0.4789, 0.4703, 0.6290, 0.3836, 0.1557, 0.3186, 0.5661, 0.5486,\n",
      "          0.4136, 0.3919],\n",
      "         [0.4404, 0.4355, 0.4240, 0.4800, 0.3562, 0.4614, 0.3438, 0.4161,\n",
      "          0.4029, 0.3919],\n",
      "         [0.4266, 0.5027, 0.4766, 0.6950, 0.1438, 0.2031, 0.5235, 0.5487,\n",
      "          0.4354, 0.3919],\n",
      "         [0.4593, 0.4812, 0.5288, 0.3696, 0.3355, 0.3077, 0.3089, 0.4872,\n",
      "          0.4636, 0.4094],\n",
      "         [0.4575, 0.4539, 0.4637, 0.6178, 0.3269, 0.2053, 0.5160, 0.4572,\n",
      "          0.3948, 0.4264],\n",
      "         [0.4410, 0.4240, 0.3032, 0.4792, 0.4200, 0.3478, 0.4594, 0.4563,\n",
      "          0.3941, 0.5552],\n",
      "         [0.4855, 0.5324, 0.5194, 0.5259, 0.6442, 0.4800, 0.4576, 0.4992,\n",
      "          0.4468, 0.5247]],\n",
      "\n",
      "        [[0.6068, 0.6565, 0.6388, 0.6153, 0.6304, 0.5782, 0.6172, 0.5780,\n",
      "          0.6146, 0.5868],\n",
      "         [0.5442, 0.4756, 0.5091, 0.5611, 0.5814, 0.5062, 0.4499, 0.4575,\n",
      "          0.4810, 0.5192],\n",
      "         [0.5599, 0.4982, 0.6646, 0.7996, 0.5038, 0.6146, 0.5231, 0.4641,\n",
      "          0.5408, 0.6093],\n",
      "         [0.5526, 0.3891, 0.4444, 0.5965, 0.5127, 0.5783, 0.5433, 0.5831,\n",
      "          0.5968, 0.6081],\n",
      "         [0.5323, 0.5642, 0.5550, 0.4905, 0.4641, 0.5823, 0.4640, 0.4924,\n",
      "          0.5034, 0.5906],\n",
      "         [0.5511, 0.6003, 0.6813, 0.5121, 0.5577, 0.5222, 0.6400, 0.5929,\n",
      "          0.5400, 0.5852],\n",
      "         [0.5268, 0.5258, 0.7349, 0.8015, 0.7488, 0.6712, 0.4262, 0.5344,\n",
      "          0.4986, 0.4736],\n",
      "         [0.5472, 0.3613, 0.4977, 0.6809, 0.7774, 0.5644, 0.5619, 0.5431,\n",
      "          0.4943, 0.5624],\n",
      "         [0.5559, 0.4721, 0.4721, 0.7043, 0.4185, 0.5184, 0.5483, 0.4670,\n",
      "          0.5571, 0.5572],\n",
      "         [0.5268, 0.5431, 0.3334, 0.3860, 0.5624, 0.6133, 0.3992, 0.5324,\n",
      "          0.5824, 0.5605],\n",
      "         [0.5720, 0.5429, 0.5462, 0.5120, 0.7249, 0.4614, 0.4151, 0.5721,\n",
      "          0.5713, 0.5963],\n",
      "         [0.5705, 0.5154, 0.4480, 0.2535, 0.7731, 0.6773, 0.6334, 0.5476,\n",
      "          0.5186, 0.6043],\n",
      "         [0.5369, 0.6335, 0.5860, 0.5041, 0.7323, 0.5957, 0.6267, 0.5089,\n",
      "          0.5690, 0.6081],\n",
      "         [0.5324, 0.5460, 0.7031, 0.6879, 0.5757, 0.5946, 0.4856, 0.4350,\n",
      "          0.5742, 0.6081],\n",
      "         [0.5782, 0.3813, 0.6206, 0.7124, 0.5323, 0.5249, 0.5720, 0.6187,\n",
      "          0.5676, 0.6081],\n",
      "         [0.5211, 0.5297, 0.3710, 0.6164, 0.8443, 0.6814, 0.4339, 0.4514,\n",
      "          0.5864, 0.6081],\n",
      "         [0.5596, 0.5645, 0.5760, 0.5200, 0.6438, 0.5386, 0.6562, 0.5839,\n",
      "          0.5971, 0.6081],\n",
      "         [0.5734, 0.4973, 0.5234, 0.3050, 0.8562, 0.7969, 0.4765, 0.4513,\n",
      "          0.5646, 0.6081],\n",
      "         [0.5407, 0.5188, 0.4712, 0.6304, 0.6645, 0.6923, 0.6911, 0.5128,\n",
      "          0.5364, 0.5906],\n",
      "         [0.5425, 0.5461, 0.5363, 0.3822, 0.6731, 0.7947, 0.4840, 0.5428,\n",
      "          0.6052, 0.5736],\n",
      "         [0.5590, 0.5760, 0.6968, 0.5208, 0.5800, 0.6522, 0.5406, 0.5437,\n",
      "          0.6059, 0.4448],\n",
      "         [0.5145, 0.4676, 0.4806, 0.4741, 0.3558, 0.5200, 0.5424, 0.5008,\n",
      "          0.5532, 0.4753]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "class ModelWithRandomness_OneHot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Sequential(\n",
    "            nn.Conv2d(9, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.glob = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(160, 10)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(26, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 2, 1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, height, width = x.shape\n",
    "        \n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        z = z[:, :, None, None] # Expand dims to match x\n",
    "        z = z.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, z), dim=1)\n",
    "\n",
    "        x = self.loc(x)\n",
    "\n",
    "        x_glob = self.glob(x)\n",
    "        x_glob = x_glob[:, :, None, None] # Expand dims\n",
    "        x_glob = x_glob.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, x_glob), dim=1)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = ModelWithRandomness_OneHot().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.zeros(batch_size, NUM_SPAWN_TYPES)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model TetrisModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Training model ModelWithRandomness_OneHot...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness_OneHot...\n",
      "Done!\n",
      "Results:\n",
      "Class: TetrisModel\n",
      "  Train accuracy mean 0.94218, std 0.00000\n",
      "  Test accuracy mean 0.91477, std 0.00114\n",
      "  Train spawn recall mean 0.40000, std 0.00800\n",
      "  Test spawn recall mean 0.36250, std 0.01250\n",
      "\n",
      "Class: ModelWithRandomness_OneHot\n",
      "  Train accuracy mean 0.94246, std 0.00028\n",
      "  Test accuracy mean 0.91591, std 0.00000\n",
      "  Train spawn recall mean 0.46000, std 0.09200\n",
      "  Test spawn recall mean 0.41250, std 0.06250\n",
      "\n",
      "Performing t-tests of ModelWithRandomness_OneHot against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=-1.0, pvalue=0.49999999999999956)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=-0.784464540552736, pvalue=0.5685008075657317)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akubi\\AppData\\Local\\Temp\\ipykernel_10248\\1981840718.py:33: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  accuracy_ttest = ttest_ind(baseline[\"test_accuracy\"], architecture[\"test_accuracy\"], equal_var=False)\n"
     ]
    }
   ],
   "source": [
    "train_and_compare_architectures([TetrisModel, ModelWithRandomness_OneHot], use_oracle=True, onehot=True, repeats=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelWithRandomness_OneHot` has only slightly better loss than the baseline `TetrisModel`, but the spawn recall is about the same. The loss and spawn accuracy jump around noticeably less with `ModelWithRandomness_OneHot`. With both models, the training loss stops noticeably decreasing after 30 epochs, suggesting that the models are underfitting.\n",
    "\n",
    "Another issue might be the lack of signal in the `z` input, because only about 5% of training examples are block spawns, and `z` takes a random value for examples that aren't block spawns. We could get around this by giving `z` an 8th possible value to indicate no block spawn. However, this should only be used as a temporary diagnostic, because it may make the model unable to identify block spawns from looking at the cell values themselves."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some of the larger models with this one-hot randomness encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.7041, 0.6368, 0.6456, 0.6176, 0.6176, 0.6176, 0.6176, 0.6087,\n",
      "          0.6118, 0.4044],\n",
      "         [0.5370, 0.4906, 0.4850, 0.4078, 0.4078, 0.4078, 0.4078, 0.4217,\n",
      "          0.5446, 0.3830],\n",
      "         [0.5856, 0.4343, 0.3766, 0.3797, 0.3797, 0.3797, 0.3797, 0.3968,\n",
      "          0.5320, 0.3439],\n",
      "         [0.6147, 0.5776, 0.5136, 0.5186, 0.5186, 0.5186, 0.5186, 0.4792,\n",
      "          0.6219, 0.3412],\n",
      "         [0.6147, 0.5776, 0.5136, 0.5186, 0.5186, 0.5186, 0.5186, 0.4792,\n",
      "          0.6219, 0.3412],\n",
      "         [0.5775, 0.5781, 0.4701, 0.5005, 0.5595, 0.5637, 0.5709, 0.4821,\n",
      "          0.5810, 0.3412],\n",
      "         [0.6007, 0.5416, 0.5553, 0.5450, 0.7071, 0.7416, 0.6192, 0.4517,\n",
      "          0.5976, 0.3412],\n",
      "         [0.6032, 0.6337, 0.6012, 0.5690, 0.4633, 0.6723, 0.5188, 0.5057,\n",
      "          0.5647, 0.3412],\n",
      "         [0.5928, 0.6268, 0.7001, 0.3730, 0.6871, 0.5171, 0.5636, 0.3993,\n",
      "          0.5860, 0.3412],\n",
      "         [0.6402, 0.6194, 0.5368, 0.4836, 0.6303, 0.6599, 0.3740, 0.4586,\n",
      "          0.6096, 0.3412],\n",
      "         [0.5557, 0.5869, 0.4246, 0.3498, 0.4022, 0.3372, 0.4186, 0.4410,\n",
      "          0.5476, 0.3412],\n",
      "         [0.5981, 0.5722, 0.4563, 0.3305, 0.5186, 0.2938, 0.5184, 0.4218,\n",
      "          0.6487, 0.3412],\n",
      "         [0.6147, 0.5704, 0.5062, 0.4714, 0.3783, 0.5183, 0.4935, 0.4846,\n",
      "          0.6219, 0.3412],\n",
      "         [0.6147, 0.5776, 0.5136, 0.5186, 0.5186, 0.5186, 0.5186, 0.4792,\n",
      "          0.6219, 0.3412],\n",
      "         [0.6147, 0.5776, 0.5136, 0.5186, 0.5186, 0.5186, 0.5186, 0.4792,\n",
      "          0.6219, 0.3412],\n",
      "         [0.6147, 0.5776, 0.5136, 0.5186, 0.5186, 0.5186, 0.5186, 0.4792,\n",
      "          0.6219, 0.3412],\n",
      "         [0.6147, 0.5776, 0.5136, 0.5186, 0.5186, 0.5186, 0.5186, 0.4792,\n",
      "          0.6219, 0.3412],\n",
      "         [0.6147, 0.5776, 0.5136, 0.5186, 0.5186, 0.5186, 0.5186, 0.4792,\n",
      "          0.6219, 0.3412],\n",
      "         [0.6147, 0.5776, 0.5136, 0.5186, 0.5186, 0.5186, 0.5186, 0.4792,\n",
      "          0.6219, 0.3412],\n",
      "         [0.6021, 0.5834, 0.5267, 0.5387, 0.5387, 0.5387, 0.5387, 0.4678,\n",
      "          0.5707, 0.3662],\n",
      "         [0.4997, 0.6135, 0.5560, 0.5892, 0.5892, 0.5892, 0.5892, 0.5731,\n",
      "          0.6781, 0.3947],\n",
      "         [0.5584, 0.5751, 0.5107, 0.5456, 0.5456, 0.5456, 0.5456, 0.4995,\n",
      "          0.5863, 0.3575]],\n",
      "\n",
      "        [[0.2959, 0.3632, 0.3544, 0.3824, 0.3824, 0.3824, 0.3824, 0.3913,\n",
      "          0.3882, 0.5956],\n",
      "         [0.4630, 0.5094, 0.5150, 0.5922, 0.5922, 0.5922, 0.5922, 0.5783,\n",
      "          0.4554, 0.6170],\n",
      "         [0.4144, 0.5657, 0.6234, 0.6203, 0.6203, 0.6203, 0.6203, 0.6032,\n",
      "          0.4680, 0.6561],\n",
      "         [0.3853, 0.4224, 0.4864, 0.4814, 0.4814, 0.4814, 0.4814, 0.5208,\n",
      "          0.3781, 0.6588],\n",
      "         [0.3853, 0.4224, 0.4864, 0.4814, 0.4814, 0.4814, 0.4814, 0.5208,\n",
      "          0.3781, 0.6588],\n",
      "         [0.4225, 0.4219, 0.5299, 0.4995, 0.4405, 0.4363, 0.4291, 0.5179,\n",
      "          0.4190, 0.6588],\n",
      "         [0.3993, 0.4584, 0.4447, 0.4550, 0.2929, 0.2584, 0.3808, 0.5483,\n",
      "          0.4024, 0.6588],\n",
      "         [0.3968, 0.3663, 0.3988, 0.4310, 0.5367, 0.3277, 0.4812, 0.4943,\n",
      "          0.4353, 0.6588],\n",
      "         [0.4072, 0.3732, 0.2999, 0.6270, 0.3129, 0.4829, 0.4364, 0.6007,\n",
      "          0.4140, 0.6588],\n",
      "         [0.3598, 0.3806, 0.4632, 0.5164, 0.3697, 0.3401, 0.6260, 0.5414,\n",
      "          0.3904, 0.6588],\n",
      "         [0.4443, 0.4131, 0.5754, 0.6502, 0.5978, 0.6628, 0.5814, 0.5590,\n",
      "          0.4524, 0.6588],\n",
      "         [0.4019, 0.4278, 0.5437, 0.6695, 0.4814, 0.7062, 0.4816, 0.5782,\n",
      "          0.3513, 0.6588],\n",
      "         [0.3853, 0.4296, 0.4938, 0.5286, 0.6217, 0.4817, 0.5065, 0.5154,\n",
      "          0.3781, 0.6588],\n",
      "         [0.3853, 0.4224, 0.4864, 0.4814, 0.4814, 0.4814, 0.4814, 0.5208,\n",
      "          0.3781, 0.6588],\n",
      "         [0.3853, 0.4224, 0.4864, 0.4814, 0.4814, 0.4814, 0.4814, 0.5208,\n",
      "          0.3781, 0.6588],\n",
      "         [0.3853, 0.4224, 0.4864, 0.4814, 0.4814, 0.4814, 0.4814, 0.5208,\n",
      "          0.3781, 0.6588],\n",
      "         [0.3853, 0.4224, 0.4864, 0.4814, 0.4814, 0.4814, 0.4814, 0.5208,\n",
      "          0.3781, 0.6588],\n",
      "         [0.3853, 0.4224, 0.4864, 0.4814, 0.4814, 0.4814, 0.4814, 0.5208,\n",
      "          0.3781, 0.6588],\n",
      "         [0.3853, 0.4224, 0.4864, 0.4814, 0.4814, 0.4814, 0.4814, 0.5208,\n",
      "          0.3781, 0.6588],\n",
      "         [0.3979, 0.4166, 0.4733, 0.4613, 0.4613, 0.4613, 0.4613, 0.5322,\n",
      "          0.4293, 0.6338],\n",
      "         [0.5003, 0.3865, 0.4440, 0.4108, 0.4108, 0.4108, 0.4108, 0.4269,\n",
      "          0.3219, 0.6053],\n",
      "         [0.4416, 0.4249, 0.4893, 0.4544, 0.4544, 0.4544, 0.4544, 0.5005,\n",
      "          0.4137, 0.6425]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "class WiderAllModel_OneHot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Sequential(\n",
    "            nn.Conv2d(9, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.glob = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(320, 16)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(48, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 2, 1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, height, width = x.shape\n",
    "        \n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        z = z[:, :, None, None] # Expand dims to match x\n",
    "        z = z.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, z), dim=1)\n",
    "\n",
    "        x = self.loc(x)\n",
    "\n",
    "        x_glob = self.glob(x)\n",
    "        x_glob = x_glob[:, :, None, None] # Expand dims\n",
    "        x_glob = x_glob.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, x_glob), dim=1)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = WiderAllModel_OneHot().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.zeros(batch_size, NUM_SPAWN_TYPES)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.6383, 0.6217, 0.5532, 0.5853, 0.5901, 0.4467, 0.5360, 0.4842,\n",
      "          0.5731, 0.5788],\n",
      "         [0.6704, 0.6691, 0.8023, 0.7609, 0.6961, 0.8268, 0.5007, 0.6243,\n",
      "          0.7608, 0.6634],\n",
      "         [0.5180, 0.6061, 0.4892, 0.4812, 0.5161, 0.5965, 0.6281, 0.4911,\n",
      "          0.6472, 0.6401],\n",
      "         [0.5715, 0.5848, 0.6077, 0.5067, 0.6304, 0.6537, 0.4452, 0.4882,\n",
      "          0.7108, 0.6066],\n",
      "         [0.5808, 0.6644, 0.5058, 0.4132, 0.4597, 0.3718, 0.4033, 0.5567,\n",
      "          0.4617, 0.5660],\n",
      "         [0.6049, 0.6744, 0.4538, 0.4485, 0.4955, 0.3929, 0.3759, 0.5561,\n",
      "          0.4283, 0.5213],\n",
      "         [0.5508, 0.6499, 0.5719, 0.3992, 0.6233, 0.5076, 0.4429, 0.5474,\n",
      "          0.5121, 0.6151],\n",
      "         [0.6212, 0.5179, 0.5018, 0.5198, 0.5116, 0.3673, 0.3837, 0.5311,\n",
      "          0.4673, 0.5161],\n",
      "         [0.5653, 0.5924, 0.5694, 0.7745, 0.6344, 0.5090, 0.3559, 0.5447,\n",
      "          0.5607, 0.6225],\n",
      "         [0.4484, 0.6064, 0.2146, 0.2989, 0.5090, 0.5360, 0.3664, 0.4997,\n",
      "          0.4988, 0.6034],\n",
      "         [0.5801, 0.5819, 0.3912, 0.3585, 0.5377, 0.6233, 0.2652, 0.4607,\n",
      "          0.5227, 0.5854],\n",
      "         [0.5171, 0.6884, 0.6119, 0.6178, 0.6110, 0.4503, 0.6095, 0.5506,\n",
      "          0.5306, 0.5956],\n",
      "         [0.5320, 0.5955, 0.2091, 0.5687, 0.6837, 0.3043, 0.3860, 0.5828,\n",
      "          0.4607, 0.5855],\n",
      "         [0.4856, 0.5279, 0.2771, 0.5658, 0.6413, 0.7509, 0.3065, 0.6303,\n",
      "          0.5723, 0.5857],\n",
      "         [0.5451, 0.5337, 0.4154, 0.6657, 0.4502, 0.5240, 0.5221, 0.5999,\n",
      "          0.5254, 0.6358],\n",
      "         [0.6126, 0.6105, 0.5698, 0.3640, 0.5912, 0.5297, 0.3830, 0.4855,\n",
      "          0.5480, 0.5819],\n",
      "         [0.5990, 0.3941, 0.5840, 0.6165, 0.6834, 0.6468, 0.4159, 0.5147,\n",
      "          0.5621, 0.6041],\n",
      "         [0.5515, 0.5332, 0.4465, 0.4068, 0.4970, 0.4695, 0.6211, 0.5249,\n",
      "          0.4627, 0.6449],\n",
      "         [0.5813, 0.5447, 0.4932, 0.4269, 0.6105, 0.6554, 0.3378, 0.4549,\n",
      "          0.6405, 0.5865],\n",
      "         [0.5254, 0.5411, 0.5613, 0.4712, 0.5202, 0.5262, 0.3743, 0.5594,\n",
      "          0.6255, 0.5625],\n",
      "         [0.6128, 0.7276, 0.5427, 0.5528, 0.7297, 0.5618, 0.7191, 0.5594,\n",
      "          0.6153, 0.6766],\n",
      "         [0.6310, 0.6086, 0.5921, 0.5825, 0.5418, 0.5551, 0.4531, 0.5368,\n",
      "          0.6131, 0.6354]],\n",
      "\n",
      "        [[0.3617, 0.3783, 0.4468, 0.4147, 0.4099, 0.5533, 0.4640, 0.5158,\n",
      "          0.4269, 0.4212],\n",
      "         [0.3296, 0.3309, 0.1977, 0.2391, 0.3039, 0.1732, 0.4993, 0.3757,\n",
      "          0.2392, 0.3366],\n",
      "         [0.4820, 0.3939, 0.5108, 0.5188, 0.4839, 0.4035, 0.3719, 0.5089,\n",
      "          0.3528, 0.3599],\n",
      "         [0.4285, 0.4152, 0.3923, 0.4933, 0.3696, 0.3463, 0.5548, 0.5118,\n",
      "          0.2892, 0.3934],\n",
      "         [0.4192, 0.3356, 0.4942, 0.5868, 0.5403, 0.6282, 0.5967, 0.4433,\n",
      "          0.5383, 0.4340],\n",
      "         [0.3951, 0.3256, 0.5462, 0.5515, 0.5045, 0.6071, 0.6241, 0.4439,\n",
      "          0.5717, 0.4787],\n",
      "         [0.4492, 0.3501, 0.4281, 0.6008, 0.3767, 0.4924, 0.5571, 0.4526,\n",
      "          0.4879, 0.3849],\n",
      "         [0.3788, 0.4821, 0.4982, 0.4802, 0.4884, 0.6327, 0.6163, 0.4689,\n",
      "          0.5327, 0.4839],\n",
      "         [0.4347, 0.4076, 0.4306, 0.2255, 0.3656, 0.4910, 0.6441, 0.4553,\n",
      "          0.4393, 0.3775],\n",
      "         [0.5516, 0.3936, 0.7854, 0.7011, 0.4910, 0.4640, 0.6336, 0.5003,\n",
      "          0.5012, 0.3966],\n",
      "         [0.4199, 0.4181, 0.6088, 0.6415, 0.4623, 0.3767, 0.7348, 0.5393,\n",
      "          0.4773, 0.4146],\n",
      "         [0.4829, 0.3116, 0.3881, 0.3822, 0.3890, 0.5497, 0.3905, 0.4494,\n",
      "          0.4694, 0.4044],\n",
      "         [0.4680, 0.4045, 0.7909, 0.4313, 0.3163, 0.6957, 0.6140, 0.4172,\n",
      "          0.5393, 0.4145],\n",
      "         [0.5144, 0.4721, 0.7229, 0.4342, 0.3587, 0.2491, 0.6935, 0.3697,\n",
      "          0.4277, 0.4143],\n",
      "         [0.4549, 0.4663, 0.5846, 0.3343, 0.5498, 0.4760, 0.4779, 0.4001,\n",
      "          0.4746, 0.3642],\n",
      "         [0.3874, 0.3895, 0.4302, 0.6360, 0.4088, 0.4703, 0.6170, 0.5145,\n",
      "          0.4520, 0.4181],\n",
      "         [0.4010, 0.6059, 0.4160, 0.3835, 0.3166, 0.3532, 0.5841, 0.4853,\n",
      "          0.4379, 0.3959],\n",
      "         [0.4485, 0.4668, 0.5535, 0.5932, 0.5030, 0.5305, 0.3789, 0.4751,\n",
      "          0.5373, 0.3551],\n",
      "         [0.4187, 0.4553, 0.5068, 0.5731, 0.3895, 0.3446, 0.6622, 0.5451,\n",
      "          0.3595, 0.4135],\n",
      "         [0.4746, 0.4589, 0.4387, 0.5288, 0.4798, 0.4738, 0.6257, 0.4406,\n",
      "          0.3745, 0.4375],\n",
      "         [0.3872, 0.2724, 0.4573, 0.4472, 0.2703, 0.4382, 0.2809, 0.4406,\n",
      "          0.3847, 0.3234],\n",
      "         [0.3690, 0.3914, 0.4079, 0.4175, 0.4582, 0.4449, 0.5469, 0.4632,\n",
      "          0.3869, 0.3646]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "class DeeperModel_OneHot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Sequential(\n",
    "            nn.Conv2d(9, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.glob = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(160, 10)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(26, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 2, 1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, height, width = x.shape\n",
    "        \n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        z = z[:, :, None, None] # Expand dims to match x\n",
    "        z = z.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, z), dim=1)\n",
    "\n",
    "        x = self.loc(x)\n",
    "\n",
    "        x_glob = self.glob(x)\n",
    "        x_glob = x_glob[:, :, None, None] # Expand dims\n",
    "        x_glob = x_glob.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, x_glob), dim=1)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = DeeperModel_OneHot().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.zeros(batch_size, NUM_SPAWN_TYPES)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.5099, 0.5970, 0.5678, 0.4890, 0.4141, 0.4899, 0.3683, 0.5560,\n",
      "          0.5524, 0.5298],\n",
      "         [0.5452, 0.5228, 0.4277, 0.5074, 0.3321, 0.3664, 0.4523, 0.3016,\n",
      "          0.3432, 0.4730],\n",
      "         [0.5872, 0.2259, 0.1985, 0.1170, 0.5305, 0.3131, 0.3058, 0.6278,\n",
      "          0.5278, 0.4634],\n",
      "         [0.6608, 0.3175, 0.3766, 0.5078, 0.2011, 0.3514, 0.1880, 0.5170,\n",
      "          0.6726, 0.6007],\n",
      "         [0.5089, 0.3984, 0.3166, 0.0510, 0.0784, 0.1811, 0.2280, 0.0798,\n",
      "          0.4516, 0.3908],\n",
      "         [0.6169, 0.2925, 0.2584, 0.2813, 0.4567, 0.6077, 0.2675, 0.5034,\n",
      "          0.1987, 0.3386],\n",
      "         [0.6427, 0.6923, 0.1635, 0.0047, 0.0073, 0.0838, 0.0121, 0.1150,\n",
      "          0.1595, 0.2052],\n",
      "         [0.6772, 0.0891, 0.2308, 0.2758, 0.0637, 0.0145, 0.3420, 0.4725,\n",
      "          0.2993, 0.4929],\n",
      "         [0.3786, 0.7535, 0.0926, 0.0947, 0.9626, 0.0166, 0.0806, 0.0810,\n",
      "          0.2330, 0.4625],\n",
      "         [0.5491, 0.7762, 0.0452, 0.2452, 0.0285, 0.0435, 0.0717, 0.6328,\n",
      "          0.3817, 0.4617],\n",
      "         [0.5004, 0.7027, 0.0911, 0.3061, 0.0094, 0.0379, 0.4357, 0.2716,\n",
      "          0.2204, 0.3526],\n",
      "         [0.5866, 0.1787, 0.0666, 0.0966, 0.4026, 0.1457, 0.0086, 0.6579,\n",
      "          0.1117, 0.4404],\n",
      "         [0.6904, 0.2575, 0.0443, 0.0611, 0.3266, 0.2493, 0.4269, 0.1279,\n",
      "          0.4907, 0.3077],\n",
      "         [0.7232, 0.6324, 0.6308, 0.0017, 0.0384, 0.0386, 0.1665, 0.1533,\n",
      "          0.0251, 0.4761],\n",
      "         [0.7997, 0.3597, 0.1125, 0.1409, 0.0474, 0.0142, 0.2062, 0.1145,\n",
      "          0.5990, 0.4455],\n",
      "         [0.5553, 0.7215, 0.1208, 0.0228, 0.0421, 0.0025, 0.0025, 0.0479,\n",
      "          0.1479, 0.2529],\n",
      "         [0.7779, 0.1427, 0.1806, 0.1923, 0.3215, 0.9182, 0.2272, 0.1401,\n",
      "          0.4040, 0.2690],\n",
      "         [0.6091, 0.3507, 0.0684, 0.4083, 0.0563, 0.0051, 0.0346, 0.2141,\n",
      "          0.2137, 0.2063],\n",
      "         [0.5954, 0.3864, 0.5634, 0.3524, 0.0961, 0.0693, 0.6129, 0.4591,\n",
      "          0.3968, 0.2429],\n",
      "         [0.4204, 0.7414, 0.0321, 0.1723, 0.6207, 0.2635, 0.7774, 0.3534,\n",
      "          0.5310, 0.3845],\n",
      "         [0.5328, 0.7216, 0.4267, 0.3297, 0.2662, 0.6113, 0.5422, 0.1815,\n",
      "          0.1416, 0.5114],\n",
      "         [0.3943, 0.3559, 0.3720, 0.5517, 0.4189, 0.3501, 0.2163, 0.4306,\n",
      "          0.2480, 0.3385]],\n",
      "\n",
      "        [[0.4901, 0.4030, 0.4322, 0.5110, 0.5859, 0.5101, 0.6317, 0.4440,\n",
      "          0.4476, 0.4702],\n",
      "         [0.4548, 0.4772, 0.5723, 0.4926, 0.6679, 0.6336, 0.5477, 0.6984,\n",
      "          0.6568, 0.5270],\n",
      "         [0.4128, 0.7741, 0.8015, 0.8830, 0.4695, 0.6869, 0.6942, 0.3722,\n",
      "          0.4722, 0.5366],\n",
      "         [0.3392, 0.6825, 0.6234, 0.4922, 0.7989, 0.6486, 0.8120, 0.4830,\n",
      "          0.3274, 0.3993],\n",
      "         [0.4911, 0.6016, 0.6834, 0.9490, 0.9216, 0.8189, 0.7720, 0.9202,\n",
      "          0.5484, 0.6092],\n",
      "         [0.3831, 0.7075, 0.7416, 0.7187, 0.5433, 0.3923, 0.7325, 0.4966,\n",
      "          0.8013, 0.6614],\n",
      "         [0.3573, 0.3077, 0.8365, 0.9953, 0.9927, 0.9162, 0.9879, 0.8850,\n",
      "          0.8405, 0.7948],\n",
      "         [0.3228, 0.9109, 0.7692, 0.7242, 0.9363, 0.9855, 0.6580, 0.5275,\n",
      "          0.7007, 0.5071],\n",
      "         [0.6214, 0.2465, 0.9074, 0.9053, 0.0374, 0.9834, 0.9194, 0.9190,\n",
      "          0.7670, 0.5375],\n",
      "         [0.4509, 0.2238, 0.9548, 0.7548, 0.9715, 0.9565, 0.9283, 0.3672,\n",
      "          0.6183, 0.5383],\n",
      "         [0.4996, 0.2973, 0.9089, 0.6939, 0.9906, 0.9621, 0.5643, 0.7284,\n",
      "          0.7796, 0.6474],\n",
      "         [0.4134, 0.8213, 0.9334, 0.9034, 0.5974, 0.8543, 0.9914, 0.3421,\n",
      "          0.8883, 0.5596],\n",
      "         [0.3096, 0.7425, 0.9557, 0.9389, 0.6734, 0.7507, 0.5731, 0.8721,\n",
      "          0.5093, 0.6923],\n",
      "         [0.2768, 0.3676, 0.3692, 0.9983, 0.9616, 0.9614, 0.8335, 0.8467,\n",
      "          0.9749, 0.5239],\n",
      "         [0.2003, 0.6403, 0.8875, 0.8591, 0.9526, 0.9858, 0.7938, 0.8855,\n",
      "          0.4010, 0.5545],\n",
      "         [0.4447, 0.2785, 0.8792, 0.9772, 0.9579, 0.9975, 0.9975, 0.9521,\n",
      "          0.8521, 0.7471],\n",
      "         [0.2221, 0.8573, 0.8194, 0.8077, 0.6785, 0.0818, 0.7728, 0.8599,\n",
      "          0.5960, 0.7310],\n",
      "         [0.3909, 0.6493, 0.9316, 0.5917, 0.9437, 0.9949, 0.9654, 0.7859,\n",
      "          0.7863, 0.7937],\n",
      "         [0.4046, 0.6136, 0.4366, 0.6476, 0.9039, 0.9307, 0.3871, 0.5409,\n",
      "          0.6032, 0.7571],\n",
      "         [0.5796, 0.2586, 0.9679, 0.8277, 0.3793, 0.7365, 0.2226, 0.6466,\n",
      "          0.4690, 0.6155],\n",
      "         [0.4672, 0.2784, 0.5733, 0.6703, 0.7338, 0.3887, 0.4578, 0.8185,\n",
      "          0.8584, 0.4886],\n",
      "         [0.6057, 0.6441, 0.6280, 0.4483, 0.5811, 0.6499, 0.7837, 0.5694,\n",
      "          0.7520, 0.6615]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "class SimplifiedModelWithRandomnessLater_OneHot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc_x = nn.Sequential(\n",
    "            nn.Conv2d(2, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.enc_z = nn.Sequential(\n",
    "            nn.Linear(7, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mix = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(32, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(4, 16, 4)),\n",
    "            nn.ConvTranspose2d(4, 8, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 16, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 2, kernel_size=3, padding=1),\n",
    "            nn.LogSoftmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        s = self.enc_x(x)\n",
    "\n",
    "        v = self.enc_z(z)\n",
    "        s = torch.cat((s, v), dim=1)\n",
    "\n",
    "        s = self.mix(s)\n",
    "        logits = self.dec(s)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = SimplifiedModelWithRandomnessLater_OneHot().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.zeros(batch_size, NUM_SPAWN_TYPES)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model TetrisModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Training model ModelWithRandomness_OneHot...\n",
      "Done!\n",
      "Training model WiderAllModel_OneHot...\n",
      "Done!\n",
      "Training model DeeperModel_OneHot...\n",
      "Done!\n",
      "Training model SimplifiedModelWithRandomnessLater_OneHot...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model ModelWithRandomness_OneHot...\n",
      "Done!\n",
      "Training model WiderAllModel_OneHot...\n",
      "Done!\n",
      "Training model DeeperModel_OneHot...\n",
      "Done!\n",
      "Training model SimplifiedModelWithRandomnessLater_OneHot...\n",
      "Done!\n",
      "Results:\n",
      "Class: TetrisModel\n",
      "  Train accuracy mean 0.94161, std 0.00057\n",
      "  Test accuracy mean 0.91477, std 0.00114\n",
      "  Train spawn recall mean 0.41200, std 0.00400\n",
      "  Test spawn recall mean 0.46250, std 0.01250\n",
      "\n",
      "Class: ModelWithRandomness_OneHot\n",
      "  Train accuracy mean 0.94246, std 0.00028\n",
      "  Test accuracy mean 0.91818, std 0.00000\n",
      "  Train spawn recall mean 0.51200, std 0.14400\n",
      "  Test spawn recall mean 0.42500, std 0.05000\n",
      "\n",
      "Class: WiderAllModel_OneHot\n",
      "  Train accuracy mean 0.94274, std 0.00000\n",
      "  Test accuracy mean 0.91818, std 0.00000\n",
      "  Train spawn recall mean 0.37600, std 0.00800\n",
      "  Test spawn recall mean 0.36250, std 0.01250\n",
      "\n",
      "Class: DeeperModel_OneHot\n",
      "  Train accuracy mean 1.00000, std 0.00000\n",
      "  Test accuracy mean 0.98864, std 0.00000\n",
      "  Train spawn recall mean 1.00000, std 0.00000\n",
      "  Test spawn recall mean 0.98750, std 0.01250\n",
      "\n",
      "Class: SimplifiedModelWithRandomnessLater_OneHot\n",
      "  Train accuracy mean 0.01616, std 0.00255\n",
      "  Test accuracy mean 0.00682, std 0.00455\n",
      "  Train spawn recall mean 0.15200, std 0.15200\n",
      "  Test spawn recall mean 0.16250, std 0.16250\n",
      "\n",
      "Performing t-tests of ModelWithRandomness_OneHot against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=-3.0000000000000977, pvalue=0.20483276469912723)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=0.7276068751089997, pvalue=0.5882950283399668)\n",
      "\n",
      "Performing t-tests of WiderAllModel_OneHot against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=-3.0000000000000977, pvalue=0.20483276469912723)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=5.656854249492383, pvalue=0.029857499854668085)\n",
      "\n",
      "Performing t-tests of DeeperModel_OneHot against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=-65.00000000000196, pvalue=0.009793377739801233)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=-29.698484809835005, pvalue=0.0011318622755624337)\n",
      "\n",
      "Performing t-tests of SimplifiedModelWithRandomnessLater_OneHot against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=193.7859644040304, pvalue=0.0017554301265107976)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=1.8407159732336893, pvalue=0.3145997736964844)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akubi\\AppData\\Local\\Temp\\ipykernel_10248\\1981840718.py:33: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  accuracy_ttest = ttest_ind(baseline[\"test_accuracy\"], architecture[\"test_accuracy\"], equal_var=False)\n"
     ]
    }
   ],
   "source": [
    "train_and_compare_architectures([TetrisModel, ModelWithRandomness_OneHot, WiderAllModel_OneHot, DeeperModel_OneHot, SimplifiedModelWithRandomnessLater_OneHot], use_oracle=True, onehot=True, repeats=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TetrisModel has 12828 parameters.\n",
      "ModelWithRandomness_OneHot has 13836 parameters.\n",
      "WiderAllModel_OneHot has 49746 parameters.\n",
      "DeeperModel_OneHot has 18540 parameters.\n",
      "SimplifiedModelWithRandomnessLater_OneHot has 29086 parameters.\n"
     ]
    }
   ],
   "source": [
    "for model_class in [TetrisModel, ModelWithRandomness_OneHot, WiderAllModel_OneHot, DeeperModel_OneHot, SimplifiedModelWithRandomnessLater_OneHot]:\n",
    "    print(f\"{model_class.__name__} has {count_parameters(model_class())} parameters.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the previously-used models struggle just as before, but the deeper local-global model `DeeperModel_OneHot` blows away the competition! Its training and test losses are 2 orders of magnitude lower than those of the other models, and it gets 100% training accuracy, near 100% test accuracy, and similarly for spawn recall. This is very strong evidence that adding these 2 extra layers to the `TetrisModel` will be important when we train it as a GAN.\n",
    "\n",
    "The next question is: does this deeper model do as well with a random number that's not one-hot encoded? This will inform how many random numbers we will input into the GAN generator.\n",
    "\n",
    "Another question is: can we add these two new layers to the non-GAN `TetrisModel` and expect an improvement, e.g. in terms of spawn recall?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper TetrisModel, no randomness\n",
    "\n",
    "Let's assess a version of the `TetrisModel` which ignores the randomness, but has the extra depth of `DeeperModel_OneHot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.4237, 0.4022, 0.4364, 0.4192, 0.4289, 0.4666, 0.4443, 0.3802,\n",
      "          0.4119, 0.3665],\n",
      "         [0.2916, 0.5069, 0.3737, 0.4242, 0.4145, 0.4170, 0.5037, 0.4395,\n",
      "          0.3179, 0.4672],\n",
      "         [0.3174, 0.4744, 0.4473, 0.4233, 0.4543, 0.4415, 0.4754, 0.3895,\n",
      "          0.3847, 0.3834],\n",
      "         [0.3998, 0.4539, 0.4179, 0.4123, 0.4162, 0.4257, 0.4893, 0.3397,\n",
      "          0.4037, 0.3696],\n",
      "         [0.3754, 0.4322, 0.4070, 0.4284, 0.4444, 0.4458, 0.4859, 0.3862,\n",
      "          0.4343, 0.3940],\n",
      "         [0.3834, 0.4277, 0.3908, 0.4150, 0.4354, 0.4279, 0.5050, 0.4113,\n",
      "          0.4240, 0.3794],\n",
      "         [0.3707, 0.4461, 0.4076, 0.3981, 0.4356, 0.4075, 0.4706, 0.4320,\n",
      "          0.4517, 0.3885],\n",
      "         [0.4342, 0.4037, 0.3652, 0.4308, 0.4617, 0.6366, 0.5386, 0.5142,\n",
      "          0.4144, 0.3850],\n",
      "         [0.4682, 0.3849, 0.2860, 0.5260, 0.4830, 0.6006, 0.5559, 0.4994,\n",
      "          0.4427, 0.3992],\n",
      "         [0.4091, 0.3580, 0.3857, 0.5053, 0.6290, 0.5725, 0.8205, 0.4094,\n",
      "          0.4261, 0.3619],\n",
      "         [0.3311, 0.2728, 0.4859, 0.6353, 0.4873, 0.4730, 0.7795, 0.5037,\n",
      "          0.5154, 0.3335],\n",
      "         [0.3928, 0.4222, 0.2111, 0.2588, 0.6823, 0.4430, 0.5799, 0.4695,\n",
      "          0.4723, 0.3581],\n",
      "         [0.5085, 0.6352, 0.3825, 0.6312, 0.5645, 0.6929, 0.7008, 0.5560,\n",
      "          0.4455, 0.4561],\n",
      "         [0.3763, 0.2709, 0.2985, 0.4888, 0.4101, 0.6357, 0.3182, 0.3467,\n",
      "          0.4830, 0.4447],\n",
      "         [0.4591, 0.4337, 0.4674, 0.3079, 0.5662, 0.7204, 0.6666, 0.4851,\n",
      "          0.5207, 0.4158],\n",
      "         [0.4036, 0.2687, 0.1041, 0.3768, 0.5283, 0.6796, 0.6508, 0.4583,\n",
      "          0.4918, 0.3692],\n",
      "         [0.4166, 0.2866, 0.2725, 0.6462, 0.6026, 0.7451, 0.7543, 0.4547,\n",
      "          0.3834, 0.3571],\n",
      "         [0.4163, 0.3875, 0.3727, 0.7444, 0.7970, 0.5214, 0.6750, 0.3623,\n",
      "          0.4149, 0.4638],\n",
      "         [0.5174, 0.3647, 0.4189, 0.6261, 0.6581, 0.5253, 0.6331, 0.4195,\n",
      "          0.3922, 0.3958],\n",
      "         [0.4179, 0.4153, 0.4188, 0.6938, 0.6009, 0.8418, 0.6292, 0.4806,\n",
      "          0.5407, 0.4079],\n",
      "         [0.3069, 0.4818, 0.4031, 0.7412, 0.4920, 0.6882, 0.4646, 0.4781,\n",
      "          0.5131, 0.4086],\n",
      "         [0.5410, 0.5849, 0.4871, 0.5779, 0.4980, 0.4739, 0.3733, 0.4884,\n",
      "          0.4020, 0.4114]],\n",
      "\n",
      "        [[0.5763, 0.5978, 0.5636, 0.5808, 0.5711, 0.5334, 0.5557, 0.6198,\n",
      "          0.5881, 0.6335],\n",
      "         [0.7084, 0.4931, 0.6263, 0.5758, 0.5855, 0.5830, 0.4963, 0.5605,\n",
      "          0.6821, 0.5328],\n",
      "         [0.6826, 0.5256, 0.5527, 0.5767, 0.5457, 0.5585, 0.5246, 0.6105,\n",
      "          0.6153, 0.6166],\n",
      "         [0.6002, 0.5461, 0.5821, 0.5877, 0.5838, 0.5743, 0.5107, 0.6603,\n",
      "          0.5963, 0.6304],\n",
      "         [0.6246, 0.5678, 0.5930, 0.5716, 0.5556, 0.5542, 0.5141, 0.6138,\n",
      "          0.5657, 0.6060],\n",
      "         [0.6166, 0.5723, 0.6092, 0.5850, 0.5646, 0.5721, 0.4950, 0.5887,\n",
      "          0.5760, 0.6206],\n",
      "         [0.6293, 0.5539, 0.5924, 0.6019, 0.5644, 0.5925, 0.5294, 0.5680,\n",
      "          0.5483, 0.6115],\n",
      "         [0.5658, 0.5963, 0.6348, 0.5692, 0.5383, 0.3634, 0.4614, 0.4858,\n",
      "          0.5856, 0.6150],\n",
      "         [0.5318, 0.6151, 0.7140, 0.4740, 0.5170, 0.3994, 0.4441, 0.5006,\n",
      "          0.5573, 0.6008],\n",
      "         [0.5909, 0.6420, 0.6143, 0.4947, 0.3710, 0.4275, 0.1795, 0.5906,\n",
      "          0.5739, 0.6381],\n",
      "         [0.6689, 0.7272, 0.5141, 0.3647, 0.5127, 0.5270, 0.2205, 0.4963,\n",
      "          0.4846, 0.6665],\n",
      "         [0.6072, 0.5778, 0.7889, 0.7412, 0.3177, 0.5570, 0.4201, 0.5305,\n",
      "          0.5277, 0.6419],\n",
      "         [0.4915, 0.3648, 0.6175, 0.3688, 0.4355, 0.3071, 0.2992, 0.4440,\n",
      "          0.5545, 0.5439],\n",
      "         [0.6237, 0.7291, 0.7015, 0.5112, 0.5899, 0.3643, 0.6818, 0.6533,\n",
      "          0.5170, 0.5553],\n",
      "         [0.5409, 0.5663, 0.5326, 0.6921, 0.4338, 0.2796, 0.3334, 0.5149,\n",
      "          0.4793, 0.5842],\n",
      "         [0.5964, 0.7313, 0.8959, 0.6232, 0.4717, 0.3204, 0.3492, 0.5417,\n",
      "          0.5082, 0.6308],\n",
      "         [0.5834, 0.7134, 0.7275, 0.3538, 0.3974, 0.2549, 0.2457, 0.5453,\n",
      "          0.6166, 0.6429],\n",
      "         [0.5837, 0.6125, 0.6273, 0.2556, 0.2030, 0.4786, 0.3250, 0.6377,\n",
      "          0.5851, 0.5362],\n",
      "         [0.4826, 0.6353, 0.5811, 0.3739, 0.3419, 0.4747, 0.3669, 0.5805,\n",
      "          0.6078, 0.6042],\n",
      "         [0.5821, 0.5847, 0.5812, 0.3062, 0.3991, 0.1582, 0.3708, 0.5194,\n",
      "          0.4593, 0.5921],\n",
      "         [0.6931, 0.5182, 0.5969, 0.2588, 0.5080, 0.3118, 0.5354, 0.5219,\n",
      "          0.4869, 0.5914],\n",
      "         [0.4590, 0.4151, 0.5129, 0.4221, 0.5020, 0.5261, 0.6267, 0.5116,\n",
      "          0.5980, 0.5886]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "class DeeperModelIgnoringRandomness(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Sequential(\n",
    "            nn.Conv2d(2, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.glob = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(160, 10)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(26, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 2, 1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, height, width = x.shape\n",
    "        \n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        x = self.loc(x)\n",
    "\n",
    "        x_glob = self.glob(x)\n",
    "        x_glob = x_glob[:, :, None, None] # Expand dims\n",
    "        x_glob = x_glob.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, x_glob), dim=1)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = DeeperModelIgnoringRandomness().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.zeros(batch_size)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TetrisModel has 12828 parameters.\n",
      "DeeperModelIgnoringRandomness has 17532 parameters.\n"
     ]
    }
   ],
   "source": [
    "for model_class in [TetrisModel, DeeperModelIgnoringRandomness]:\n",
    "    print(f\"{model_class.__name__} has {count_parameters(model_class())} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model TetrisModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Training model DeeperModelIgnoringRandomness...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model DeeperModelIgnoringRandomness...\n",
      "Done!\n",
      "Results:\n",
      "Class: TetrisModel\n",
      "  Train accuracy mean 0.94218, std 0.00000\n",
      "  Test accuracy mean 0.91477, std 0.00114\n",
      "  Train spawn recall mean 0.45600, std 0.05600\n",
      "  Test spawn recall mean 0.46250, std 0.01250\n",
      "\n",
      "Class: DeeperModelIgnoringRandomness\n",
      "  Train accuracy mean 0.98413, std 0.00057\n",
      "  Test accuracy mean 0.91023, std 0.00114\n",
      "  Train spawn recall mean 1.00000, std 0.00000\n",
      "  Test spawn recall mean 0.98750, std 0.01250\n",
      "\n",
      "Performing t-tests of DeeperModelIgnoringRandomness against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=2.828427124746259, pvalue=0.10557280900007976)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=-29.698484809835005, pvalue=0.0011318622755624337)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_compare_architectures([TetrisModel, DeeperModelIgnoringRandomness], use_oracle=False, onehot=False, repeats=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the deeper model does a lot better than the original `TetrisModel`. We should keep these two extra layers. The better performance can be seen most clearly in the spawn recall, and also in the loss. The deeper model's spawn recall is over 90%, whereas the original model's oscillates around 50%. In the loss curve, we see that the deeper model starts to overfit after about 30 epochs. The test loss goes up as the training loss goes down further. At 30 epochs, the training loss is about 0.001 while the test loss is about 0.002. We can also see the overfitting in the logged predictions, where we see the model reliably guessing the block spawn types even though it should have no way of knowing them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce number of random variables\n",
    "\n",
    "Let's try to reduce the number of random inputs to 1 again and see how the deeper model fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted states: tensor([[[0.5856, 0.5879, 0.5762, 0.5294, 0.5642, 0.5416, 0.5314, 0.5617,\n",
      "          0.4799, 0.5047],\n",
      "         [0.4765, 0.4992, 0.6162, 0.4900, 0.5294, 0.5435, 0.5404, 0.5399,\n",
      "          0.6441, 0.5806],\n",
      "         [0.4718, 0.3986, 0.6102, 0.5862, 0.6039, 0.5918, 0.6217, 0.5070,\n",
      "          0.4988, 0.6487],\n",
      "         [0.4745, 0.4843, 0.6755, 0.5761, 0.5807, 0.5716, 0.5727, 0.4637,\n",
      "          0.4311, 0.6698],\n",
      "         [0.5123, 0.4880, 0.6644, 0.5326, 0.5633, 0.5850, 0.5550, 0.4271,\n",
      "          0.3956, 0.6618],\n",
      "         [0.4939, 0.4860, 0.5841, 0.5622, 0.5720, 0.6898, 0.5865, 0.4967,\n",
      "          0.4153, 0.6511],\n",
      "         [0.4760, 0.4538, 0.5493, 0.5649, 0.5948, 0.6658, 0.5324, 0.4724,\n",
      "          0.5057, 0.6178],\n",
      "         [0.5246, 0.3447, 0.4420, 0.6951, 0.5846, 0.7162, 0.5803, 0.6168,\n",
      "          0.5074, 0.6837],\n",
      "         [0.4856, 0.5669, 0.7183, 0.7893, 0.6165, 0.5577, 0.5556, 0.5935,\n",
      "          0.5332, 0.6817],\n",
      "         [0.5556, 0.5238, 0.4559, 0.7858, 0.8140, 0.5552, 0.6837, 0.6367,\n",
      "          0.3796, 0.6431],\n",
      "         [0.5219, 0.5183, 0.5378, 0.4771, 0.6148, 0.5242, 0.7258, 0.6458,\n",
      "          0.4326, 0.6466],\n",
      "         [0.5227, 0.5769, 0.5575, 0.5197, 0.4896, 0.4541, 0.6511, 0.5483,\n",
      "          0.3493, 0.6934],\n",
      "         [0.4954, 0.5157, 0.5989, 0.3935, 0.5502, 0.4709, 0.6457, 0.5676,\n",
      "          0.4257, 0.6504],\n",
      "         [0.5098, 0.4809, 0.6729, 0.6108, 0.5382, 0.6393, 0.5434, 0.4785,\n",
      "          0.3628, 0.6598],\n",
      "         [0.4976, 0.5026, 0.6931, 0.5776, 0.5813, 0.5749, 0.5907, 0.5030,\n",
      "          0.3507, 0.6503],\n",
      "         [0.5006, 0.5077, 0.6216, 0.6133, 0.5831, 0.6190, 0.6018, 0.4789,\n",
      "          0.3349, 0.6476],\n",
      "         [0.4719, 0.4360, 0.5347, 0.7251, 0.5836, 0.5850, 0.6012, 0.4606,\n",
      "          0.3645, 0.6529],\n",
      "         [0.4687, 0.3617, 0.5114, 0.6045, 0.6005, 0.4724, 0.5716, 0.4784,\n",
      "          0.4488, 0.7008],\n",
      "         [0.5009, 0.5774, 0.6704, 0.6535, 0.5588, 0.5854, 0.6787, 0.4928,\n",
      "          0.5896, 0.7000],\n",
      "         [0.4770, 0.5297, 0.6849, 0.6383, 0.8090, 0.7460, 0.5924, 0.5334,\n",
      "          0.5033, 0.6263],\n",
      "         [0.3587, 0.5840, 0.6263, 0.6208, 0.5961, 0.6372, 0.7664, 0.6158,\n",
      "          0.5572, 0.5688],\n",
      "         [0.4373, 0.6059, 0.5331, 0.6163, 0.6360, 0.5814, 0.6555, 0.6638,\n",
      "          0.5102, 0.5748]],\n",
      "\n",
      "        [[0.4144, 0.4121, 0.4238, 0.4706, 0.4358, 0.4584, 0.4686, 0.4383,\n",
      "          0.5201, 0.4953],\n",
      "         [0.5235, 0.5008, 0.3838, 0.5100, 0.4706, 0.4565, 0.4596, 0.4601,\n",
      "          0.3559, 0.4194],\n",
      "         [0.5282, 0.6014, 0.3898, 0.4138, 0.3961, 0.4082, 0.3783, 0.4930,\n",
      "          0.5012, 0.3513],\n",
      "         [0.5255, 0.5157, 0.3245, 0.4239, 0.4193, 0.4284, 0.4273, 0.5363,\n",
      "          0.5689, 0.3302],\n",
      "         [0.4877, 0.5120, 0.3356, 0.4674, 0.4367, 0.4150, 0.4450, 0.5729,\n",
      "          0.6044, 0.3382],\n",
      "         [0.5061, 0.5140, 0.4159, 0.4378, 0.4280, 0.3102, 0.4135, 0.5033,\n",
      "          0.5847, 0.3489],\n",
      "         [0.5240, 0.5462, 0.4507, 0.4351, 0.4052, 0.3342, 0.4676, 0.5276,\n",
      "          0.4943, 0.3822],\n",
      "         [0.4754, 0.6553, 0.5580, 0.3049, 0.4154, 0.2838, 0.4197, 0.3832,\n",
      "          0.4926, 0.3163],\n",
      "         [0.5144, 0.4331, 0.2817, 0.2107, 0.3835, 0.4423, 0.4444, 0.4065,\n",
      "          0.4668, 0.3183],\n",
      "         [0.4444, 0.4762, 0.5441, 0.2142, 0.1860, 0.4448, 0.3163, 0.3633,\n",
      "          0.6204, 0.3569],\n",
      "         [0.4781, 0.4817, 0.4622, 0.5229, 0.3852, 0.4758, 0.2742, 0.3542,\n",
      "          0.5674, 0.3534],\n",
      "         [0.4773, 0.4231, 0.4425, 0.4803, 0.5104, 0.5459, 0.3489, 0.4517,\n",
      "          0.6507, 0.3066],\n",
      "         [0.5046, 0.4843, 0.4011, 0.6065, 0.4498, 0.5291, 0.3543, 0.4324,\n",
      "          0.5743, 0.3496],\n",
      "         [0.4902, 0.5191, 0.3271, 0.3892, 0.4618, 0.3607, 0.4566, 0.5215,\n",
      "          0.6372, 0.3402],\n",
      "         [0.5024, 0.4974, 0.3069, 0.4224, 0.4187, 0.4251, 0.4093, 0.4970,\n",
      "          0.6493, 0.3497],\n",
      "         [0.4994, 0.4923, 0.3784, 0.3867, 0.4169, 0.3810, 0.3982, 0.5211,\n",
      "          0.6651, 0.3524],\n",
      "         [0.5281, 0.5640, 0.4653, 0.2749, 0.4164, 0.4150, 0.3988, 0.5394,\n",
      "          0.6355, 0.3471],\n",
      "         [0.5313, 0.6383, 0.4886, 0.3955, 0.3995, 0.5276, 0.4284, 0.5216,\n",
      "          0.5512, 0.2992],\n",
      "         [0.4991, 0.4226, 0.3296, 0.3465, 0.4412, 0.4146, 0.3213, 0.5072,\n",
      "          0.4104, 0.3000],\n",
      "         [0.5230, 0.4703, 0.3151, 0.3617, 0.1910, 0.2540, 0.4076, 0.4666,\n",
      "          0.4967, 0.3737],\n",
      "         [0.6413, 0.4160, 0.3737, 0.3792, 0.4039, 0.3628, 0.2336, 0.3842,\n",
      "          0.4428, 0.4312],\n",
      "         [0.5627, 0.3941, 0.4669, 0.3837, 0.3640, 0.4186, 0.3445, 0.3362,\n",
      "          0.4898, 0.4252]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "class DeeperModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.glob = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(160, 10)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(26, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 2, 1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, height, width = x.shape\n",
    "        \n",
    "        x = F.one_hot(x.long(), 2) # One-hot encode the two cell classes\n",
    "        x = x.type(torch.float) # Convert to floating-point\n",
    "        x = x.permute((0, 3, 1, 2)) # Move channels/classes to dimension 1\n",
    "\n",
    "        z = z[:, None, None, None] # Expand dims to match x\n",
    "        z = z.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, z), dim=1)\n",
    "\n",
    "        x = self.loc(x)\n",
    "\n",
    "        x_glob = self.glob(x)\n",
    "        x_glob = x_glob[:, :, None, None] # Expand dims\n",
    "        x_glob = x_glob.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, x_glob), dim=1)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = DeeperModel().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = torch.zeros(batch_size)\n",
    "    logits = model(X, z)[0]\n",
    "    preds = torch.exp(logits)\n",
    "    print(f\"Predicted states: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model TetrisModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\game_emulation\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Training model DeeperModelIgnoringRandomness...\n",
      "Done!\n",
      "Training model DeeperModel...\n",
      "Done!\n",
      "Training model TetrisModel...\n",
      "Done!\n",
      "Training model DeeperModelIgnoringRandomness...\n",
      "Done!\n",
      "Training model DeeperModel...\n",
      "Done!\n",
      "Results:\n",
      "Class: TetrisModel\n",
      "  Train accuracy mean 0.94133, std 0.00028\n",
      "  Test accuracy mean 0.91591, std 0.00000\n",
      "  Train spawn recall mean 0.43600, std 0.02000\n",
      "  Test spawn recall mean 0.35000, std 0.02500\n",
      "\n",
      "Class: DeeperModelIgnoringRandomness\n",
      "  Train accuracy mean 0.98384, std 0.00198\n",
      "  Test accuracy mean 0.91136, std 0.00227\n",
      "  Train spawn recall mean 0.98800, std 0.01200\n",
      "  Test spawn recall mean 0.97500, std 0.02500\n",
      "\n",
      "Class: DeeperModel\n",
      "  Train accuracy mean 1.00000, std 0.00000\n",
      "  Test accuracy mean 0.95568, std 0.00114\n",
      "  Train spawn recall mean 1.00000, std 0.00000\n",
      "  Test spawn recall mean 0.98750, std 0.01250\n",
      "\n",
      "Performing t-tests of DeeperModelIgnoringRandomness against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=1.9999999999999512, pvalue=0.2951672353008727)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=-17.677669529663685, pvalue=0.0031847214638750024)\n",
      "\n",
      "Performing t-tests of DeeperModel against TetrisModel\n",
      "Accuracy t-test results:\n",
      "Ttest_indResult(statistic=-35.00000000000107, pvalue=0.01818418935129631)\n",
      "Spawn recall t-test results:\n",
      "Ttest_indResult(statistic=-22.807893370497858, pvalue=0.007477976961125441)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akubi\\AppData\\Local\\Temp\\ipykernel_10248\\1981840718.py:33: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  accuracy_ttest = ttest_ind(baseline[\"test_accuracy\"], architecture[\"test_accuracy\"], equal_var=False)\n"
     ]
    }
   ],
   "source": [
    "train_and_compare_architectures([TetrisModel, DeeperModelIgnoringRandomness, DeeperModel], use_oracle=True, onehot=False, repeats=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the oracle randomness encoded linearly rather than one-hot encoded, the deeper model takes a little longer to learn, but still does much better than when it ignores the randomness. In terms of loss, we see some overfitting, so linear randomness may need more data to achieve an equivalent loss value. The same can be seen in the board accuracy. In terms of spawn recall, the deeper model with linear oracle randomness achieves 100% on the training and test sets, just like the deeper model with one-hot oracle randomness."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Surprisingly, there is still room for improvement, even before employing GAN techniques. We can just take the existing `TetrisModel` and add two more 3x3 `Conv2D` layers to the `head` module, which greatly improves the spawn recall. This leads to overfitting after 30 epochs, so it may be good to finish the training early, say at 50 epochs, rather than training for 100 epochs as we've done most often.\n",
    "\n",
    "Once we start using a GAN, we can sample a small number of random variables, broadcast these to the grid dimensions, and concatenate them with the grid as extra channels. To decide the block spawn type, we have some indication that 7 random variables (the number of block spawn types) work better than 1 random variable, likely because it avoids the need for the network to model 6 different thresholds for the variable. However, the \"7 random variables\" we used were actually a one-hot encoded random variable taking 7 values, so the performance may differ when we use 7 independent, continuous uniform random variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
