{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Tetris emulator\n",
    "\n",
    "In this notebook, we train a model to emulate Tetris and provide a backend for our model-based game engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from models import TetrisModel, TetrisDiscriminator\n",
    "import metrics\n",
    "from recording import RecordingDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordingDataset(Dataset):\n",
    "    def __init__(self, path: str):\n",
    "        self._db = RecordingDatabase(path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._db)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        boards = self._db[idx]\n",
    "        x = self._transform(boards[-2]) # Ignore all boards except the last two\n",
    "        y = self._transform(boards[-1])\n",
    "        return x, y\n",
    "    \n",
    "    def _transform(self, board):\n",
    "        board = torch.tensor(board, dtype=torch.long)\n",
    "        board = F.one_hot(board, 2) # One-hot encode the cell types\n",
    "        board = board.type(torch.float) # Convert to floating-point\n",
    "        board = board.permute((2, 0, 1)) # Move channels/classes to dimension 0\n",
    "        return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 22, 10]) torch.float32\n",
      "torch.Size([4, 2, 22, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "train_dataset = RecordingDataset(os.path.join(\"data\", \"tetris_emulator\", \"train\"))\n",
    "test_dataset = RecordingDataset(os.path.join(\"data\", \"tetris_emulator\", \"test\"))\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "print(x.shape, x.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of generator parameters: 17996\n",
      "Number of discriminator parameters: 7057\n",
      "Predicted label for real data: 0.5882883071899414\n",
      "Predicted label for fake data: 0.5279089212417603\n"
     ]
    }
   ],
   "source": [
    "gen = TetrisModel().to(device)\n",
    "disc = TetrisDiscriminator().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    y_gen = gen(X)\n",
    "    pred_on_real = F.sigmoid(disc(X, y)[0])\n",
    "    pred_on_fake = F.sigmoid(disc(X, y_gen)[0])\n",
    "    print(f\"Number of generator parameters: {count_parameters(gen)}\")\n",
    "    print(f\"Number of discriminator parameters: {count_parameters(disc)}\")\n",
    "    print(f\"Predicted label for real data: {pred_on_real}\")\n",
    "    print(f\"Predicted label for fake data: {pred_on_fake}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def find_interesting_examples(dataset, num=3):\n",
    "    num_spawns = num\n",
    "    \n",
    "    def inner():\n",
    "        num_spawns_left = num_spawns\n",
    "\n",
    "        for x, y in dataset:\n",
    "            # Check for block spawn\n",
    "            if (x.argmax(0)[0] == 0).all() & (y.argmax(0)[0] == 1).any():\n",
    "                if num_spawns_left > 0:\n",
    "                    num_spawns_left -= 1\n",
    "                    yield x, y\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "    return list(itertools.islice(inner(), num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prediction(x, pred, y):\n",
    "    \"\"\"Renders an example and prediction into a single-image array.\n",
    "    \n",
    "    Inputs:\n",
    "        x: Tensor of shape (height, width), the model input.\n",
    "        pred: Tensor of shape (height, width), the model prediction.\n",
    "        y: Tensor of shape (height, width), the target.\n",
    "    \"\"\"\n",
    "    assert len(x.shape) == 2, f\"Expected tensors of shape (width, height) but got {x.shape}\"\n",
    "    assert x.shape == pred.shape, f\"Shapes do not match: {x.shape} != {pred.shape}\"\n",
    "    assert x.shape == y.shape, f\"Shapes do not match: {x.shape} != {y.shape}\"\n",
    "    height, width = x.shape\n",
    "    with torch.no_grad():\n",
    "        separator = torch.ones(height, 1, dtype=x.dtype)\n",
    "        return torch.cat((x, separator, pred, separator, y), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "\n",
    "def train_loop(dataloader, gen, disc, loss_fn, optimizer_gen, optimizer_disc):\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        ##################################################################\n",
    "        # (1) Update discriminator: minimize -log(D(x)) - log(1 - D(G(z)))\n",
    "        ##################################################################\n",
    "        disc.zero_grad()\n",
    "\n",
    "        ## Train with all-real batch\n",
    "        # Format batch\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        batch_size = X.size(0)\n",
    "        real_labels = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through discriminator\n",
    "        output = torch.flatten(disc(X, y))\n",
    "        # Calculate loss on all-real batch\n",
    "        err_disc_real = loss_fn(output, real_labels)\n",
    "        # Calculate gradients for discriminator in backward pass\n",
    "        err_disc_real.backward()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate fake image batch with generator\n",
    "        y_fake = gen(X)\n",
    "        fake_labels = torch.full((batch_size,), fake_label, dtype=torch.float, device=device)\n",
    "        # Classify all fake batch with discriminator\n",
    "        output = torch.flatten(disc(X, y_fake.detach()))\n",
    "        # Calculate discriminator's loss on the all-fake batch\n",
    "        err_disc_fake = loss_fn(output, fake_labels)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        err_disc_fake.backward()\n",
    "\n",
    "        ## Update discriminator weights\n",
    "        # Compute error of discriminator as sum over the fake and the real batches\n",
    "        err_disc = err_disc_real + err_disc_fake\n",
    "        # Update discriminator\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        ##############################################\n",
    "        # (2) Update generator: minimize -log(D(G(z)))\n",
    "        ##############################################\n",
    "        gen.zero_grad()\n",
    "        # Since we just updated the discriminator, perform another forward pass of the all-fake batch through it\n",
    "        output = torch.flatten(disc(X, y_fake))\n",
    "        # Calculate the generator's loss based on this output\n",
    "        # We use real labels because the generator wants to fool the discriminator\n",
    "        err_gen = loss_fn(output, real_labels)\n",
    "        # Calculate gradients for generator\n",
    "        err_gen.backward()\n",
    "        # Update generator\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if batch % 30 == 0:\n",
    "            current = batch * dataloader.batch_size + batch_size\n",
    "            print(f\"[{current}/{size}] D loss: {err_disc.item():.4f}, G loss: {err_gen.item():.4f}\")\n",
    "\n",
    "\n",
    "def test_loop(split_name, dataloader, gen, disc, loss_fn, tb_writer, epoch, examples):\n",
    "    gen.eval()\n",
    "    disc.eval()\n",
    "\n",
    "    loss_disc = 0.0\n",
    "    loss_gen = 0.0\n",
    "    disc_accuracy = 0.0\n",
    "    cell_accuracy = metrics.CellAccuracy()\n",
    "    board_accuracy = metrics.BoardAccuracy()\n",
    "    board_plausibility = metrics.BoardPlausibility()\n",
    "    spawn_recall = metrics.SpawnRecall()\n",
    "    spawn_precision = metrics.SpawnPrecision()\n",
    "    spawn_validity = metrics.SpawnValidity()\n",
    "    scores_real = np.zeros(len(dataloader.dataset))\n",
    "    scores_fake = np.zeros(len(dataloader.dataset))\n",
    "    spawn_diversity = metrics.SpawnDiversity()\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    with torch.no_grad():        \n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            batch_size = X.size(0)\n",
    "            real_labels = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "            fake_labels = torch.full((batch_size,), fake_label, dtype=torch.float, device=device)\n",
    "\n",
    "            output_real = disc(X, y)\n",
    "            loss_disc += loss_fn(output_real, real_labels).item()\n",
    "\n",
    "            y_fake = gen(X)\n",
    "            output_fake = disc(X, y_fake)\n",
    "            \n",
    "            loss_disc += loss_fn(output_fake, fake_labels).item()\n",
    "            loss_gen += loss_fn(output_fake, real_labels).item()\n",
    "\n",
    "            pred_real = (output_real > 0.0)\n",
    "            pred_fake = (output_fake > 0.0)\n",
    "            disc_accuracy += pred_real.type(torch.float).mean().item()\n",
    "            disc_accuracy += (~pred_fake).type(torch.float).mean().item()\n",
    "\n",
    "            classes_X = torch.argmax(X, dim=1)\n",
    "            classes_y = torch.argmax(y, dim=1)\n",
    "            classes_y_fake = torch.argmax(y_fake, dim=1)\n",
    "            cell_accuracy.update_state(classes_y_fake, classes_y)\n",
    "            board_accuracy.update_state(classes_y_fake, classes_y)\n",
    "            board_plausibility.update_state(classes_X, classes_y_fake, classes_y)\n",
    "\n",
    "            spawn_recall.update_state(classes_X, classes_y_fake, classes_y)\n",
    "            spawn_precision.update_state(classes_X, classes_y_fake, classes_y)\n",
    "            spawn_validity.update_state(classes_X, classes_y_fake)\n",
    "\n",
    "            start_index = dataloader.batch_size * batch\n",
    "            end_index = start_index + batch_size\n",
    "            scores_real[start_index:end_index] = torch.sigmoid(output_real).numpy()\n",
    "            scores_fake[start_index:end_index] = torch.sigmoid(output_fake).numpy()\n",
    "\n",
    "            spawn_diversity.update_state(classes_X, classes_y_fake)\n",
    "\n",
    "    loss_disc /= num_batches\n",
    "    loss_gen /= num_batches\n",
    "    disc_accuracy /= (2.0 * num_batches)\n",
    "\n",
    "    print(f\"{split_name} error: \\n D loss: {loss_disc:>8f}, G loss: {loss_gen:>8f}, D accuracy: {(100*disc_accuracy):>0.1f}%, cell accuracy: {(cell_accuracy.result()):>0.1%}, board accuracy: {(board_accuracy.result()):>0.1%} \\n\")\n",
    "\n",
    "    tb_writer.add_scalar(f\"Discriminator loss/{split_name}\", loss_disc, epoch)\n",
    "    tb_writer.add_scalar(f\"Loss/{split_name}\", loss_gen, epoch)\n",
    "    tb_writer.add_scalar(f\"Discriminator accuracy/{split_name}\", disc_accuracy, epoch)\n",
    "    tb_writer.add_scalar(f\"Cell accuracy/{split_name}\", cell_accuracy.result(), epoch)\n",
    "    tb_writer.add_scalar(f\"Board accuracy/{split_name}\", board_accuracy.result(), epoch)\n",
    "    tb_writer.add_scalar(f\"Board plausibility/{split_name}\", board_plausibility.result(), epoch)\n",
    "    tb_writer.add_scalar(f\"Spawn recall/{split_name}\", spawn_recall.result(), epoch)\n",
    "    tb_writer.add_scalar(f\"Spawn precision/{split_name}\", spawn_precision.result(), epoch)\n",
    "    tb_writer.add_scalar(f\"Spawn validity/{split_name}\", spawn_validity.result(), epoch)\n",
    "    tb_writer.add_scalar(f\"Spawn diversity/{split_name}\", spawn_diversity.result(), epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(examples):\n",
    "            X, y = X.unsqueeze(0), y.unsqueeze(0)\n",
    "            y_fake = gen(X)\n",
    "            X, y, y_fake = X.squeeze(0), y.squeeze(0), y_fake.squeeze(0)\n",
    "            X, y, y_fake = X.argmax(0), y.argmax(0), y_fake.argmax(0)\n",
    "            img = render_prediction(X, y_fake, y)\n",
    "            tb_writer.add_image(f\"Predictions/{split_name}/{i}\", img, epoch, dataformats=\"HW\")\n",
    "    \n",
    "    tb_writer.add_histogram(f\"Discriminator scores/{split_name}/real\", scores_real, epoch)\n",
    "    tb_writer.add_histogram(f\"Discriminator scores/{split_name}/fake\", scores_fake, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3876, G loss: 0.6675\n",
      "[124/1778] D loss: 1.3504, G loss: 0.6491\n",
      "[244/1778] D loss: 1.3430, G loss: 0.6344\n",
      "[364/1778] D loss: 1.3217, G loss: 0.6277\n",
      "[484/1778] D loss: 1.2160, G loss: 0.7047\n",
      "[604/1778] D loss: 1.1163, G loss: 0.7825\n",
      "[724/1778] D loss: 1.0084, G loss: 0.8382\n",
      "[844/1778] D loss: 0.8709, G loss: 1.0203\n",
      "[964/1778] D loss: 0.7319, G loss: 1.1359\n",
      "[1084/1778] D loss: 0.5979, G loss: 1.2188\n",
      "[1204/1778] D loss: 0.4833, G loss: 1.5985\n",
      "[1324/1778] D loss: 0.3306, G loss: 1.9530\n",
      "[1444/1778] D loss: 0.2820, G loss: 2.3131\n",
      "[1564/1778] D loss: 0.2216, G loss: 2.4783\n",
      "[1684/1778] D loss: 0.2664, G loss: 2.4259\n",
      "train error: \n",
      " D loss: 0.313720, G loss: 2.593960, D accuracy: 96.0%, cell accuracy: 18581.6%%, board accuracy: 0.0% \n",
      "\n",
      "test error: \n",
      " D loss: 0.309808, G loss: 2.616915, D accuracy: 95.8%, cell accuracy: 18524.1%%, board accuracy: 0.0% \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "[4/1778] D loss: 0.1862, G loss: 3.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\tetris-emulator\\metrics.py:199: RuntimeWarning: invalid value encountered in divide\n",
      "  probs = self.predicted_spawn_type_counts / num_predicted_spawns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124/1778] D loss: 0.2015, G loss: 3.0198\n",
      "[244/1778] D loss: 0.1306, G loss: 3.0649\n",
      "[364/1778] D loss: 0.1401, G loss: 3.4345\n",
      "[484/1778] D loss: 0.1446, G loss: 3.6977\n",
      "[604/1778] D loss: 0.0780, G loss: 3.9261\n",
      "[724/1778] D loss: 0.0968, G loss: 3.6195\n",
      "[844/1778] D loss: 0.1617, G loss: 3.5085\n",
      "[964/1778] D loss: 0.1257, G loss: 4.1769\n",
      "[1084/1778] D loss: 0.1291, G loss: 3.5585\n",
      "[1204/1778] D loss: 0.1143, G loss: 3.6501\n",
      "[1324/1778] D loss: 0.2275, G loss: 4.1440\n",
      "[1444/1778] D loss: 0.3022, G loss: 4.2715\n",
      "[1564/1778] D loss: 0.0852, G loss: 3.8730\n",
      "[1684/1778] D loss: 0.3967, G loss: 2.7460\n",
      "train error: \n",
      " D loss: 0.222812, G loss: 4.112778, D accuracy: 99.0%, cell accuracy: 21138.2%%, board accuracy: 0.0% \n",
      "\n",
      "test error: \n",
      " D loss: 0.227431, G loss: 4.062106, D accuracy: 99.3%, cell accuracy: 21123.9%%, board accuracy: 0.0% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "[4/1778] D loss: 0.1777, G loss: 3.6535\n",
      "[124/1778] D loss: 0.1586, G loss: 3.6505\n",
      "[244/1778] D loss: 0.1812, G loss: 3.6917\n",
      "[364/1778] D loss: 0.2006, G loss: 3.3826\n",
      "[484/1778] D loss: 0.1670, G loss: 3.4504\n",
      "[604/1778] D loss: 0.1288, G loss: 3.7896\n",
      "[724/1778] D loss: 0.2032, G loss: 2.9901\n",
      "[844/1778] D loss: 0.2026, G loss: 2.6619\n",
      "[964/1778] D loss: 0.1850, G loss: 3.0488\n",
      "[1084/1778] D loss: 0.3975, G loss: 2.4119\n",
      "[1204/1778] D loss: 0.2365, G loss: 2.1619\n",
      "[1324/1778] D loss: 0.1095, G loss: 2.8937\n",
      "[1444/1778] D loss: 0.2710, G loss: 2.2716\n",
      "[1564/1778] D loss: 0.3335, G loss: 2.7933\n",
      "[1684/1778] D loss: 0.2808, G loss: 3.3274\n",
      "train error: \n",
      " D loss: 0.345645, G loss: 2.119508, D accuracy: 96.3%, cell accuracy: 21276.5%%, board accuracy: 0.0% \n",
      "\n",
      "test error: \n",
      " D loss: 0.347767, G loss: 2.107470, D accuracy: 96.4%, cell accuracy: 21272.1%%, board accuracy: 0.0% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "[4/1778] D loss: 0.1553, G loss: 2.5931\n",
      "[124/1778] D loss: 0.4444, G loss: 1.6911\n",
      "[244/1778] D loss: 0.1226, G loss: 3.1348\n",
      "[364/1778] D loss: 0.2415, G loss: 2.2080\n",
      "[484/1778] D loss: 0.4992, G loss: 1.6085\n",
      "[604/1778] D loss: 0.5898, G loss: 1.2119\n",
      "[724/1778] D loss: 0.4649, G loss: 1.9655\n",
      "[844/1778] D loss: 0.3396, G loss: 3.3671\n",
      "[964/1778] D loss: 0.5376, G loss: 1.2576\n",
      "[1084/1778] D loss: 0.6064, G loss: 2.1001\n",
      "[1204/1778] D loss: 0.6165, G loss: 2.0450\n",
      "[1324/1778] D loss: 0.7572, G loss: 1.9101\n",
      "[1444/1778] D loss: 0.5802, G loss: 1.3930\n",
      "[1564/1778] D loss: 0.7822, G loss: 1.9605\n",
      "[1684/1778] D loss: 0.8700, G loss: 1.3580\n",
      "train error: \n",
      " D loss: 0.771590, G loss: 1.790072, D accuracy: 86.0%, cell accuracy: 21417.5%%, board accuracy: 8.4% \n",
      "\n",
      "test error: \n",
      " D loss: 0.789870, G loss: 1.889484, D accuracy: 85.6%, cell accuracy: 21373.6%%, board accuracy: 6.3% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "[4/1778] D loss: 0.8262, G loss: 1.0921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\tetris-emulator\\metrics.py:226: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return self.num_true_positives / self.num_spawns_pred\n",
      "c:\\Projects\\tetris-emulator\\metrics.py:277: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return self.num_valid_spawns_pred / self.num_spawns_pred\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124/1778] D loss: 1.0537, G loss: 1.9737\n",
      "[244/1778] D loss: 0.9726, G loss: 1.6956\n",
      "[364/1778] D loss: 0.8364, G loss: 0.9437\n",
      "[484/1778] D loss: 0.4975, G loss: 2.0551\n",
      "[604/1778] D loss: 0.8167, G loss: 1.5756\n",
      "[724/1778] D loss: 0.6838, G loss: 1.6669\n",
      "[844/1778] D loss: 0.4317, G loss: 2.8702\n",
      "[964/1778] D loss: 0.7180, G loss: 1.6015\n",
      "[1084/1778] D loss: 0.6843, G loss: 1.2982\n",
      "[1204/1778] D loss: 0.7450, G loss: 1.5983\n",
      "[1324/1778] D loss: 0.6481, G loss: 2.0884\n",
      "[1444/1778] D loss: 0.8900, G loss: 1.3227\n",
      "[1564/1778] D loss: 0.9190, G loss: 0.8142\n",
      "[1684/1778] D loss: 0.9667, G loss: 0.9274\n",
      "train error: \n",
      " D loss: 0.858238, G loss: 1.503825, D accuracy: 81.8%, cell accuracy: 21567.1%%, board accuracy: 9.4% \n",
      "\n",
      "test error: \n",
      " D loss: 0.875702, G loss: 1.489287, D accuracy: 81.2%, cell accuracy: 21542.1%%, board accuracy: 8.1% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0083, G loss: 1.2165\n",
      "[124/1778] D loss: 0.7448, G loss: 0.8386\n",
      "[244/1778] D loss: 0.9138, G loss: 1.5865\n",
      "[364/1778] D loss: 1.1603, G loss: 0.7910\n",
      "[484/1778] D loss: 0.8171, G loss: 2.0116\n",
      "[604/1778] D loss: 1.0214, G loss: 1.2616\n",
      "[724/1778] D loss: 0.8319, G loss: 1.6631\n",
      "[844/1778] D loss: 1.0106, G loss: 0.7796\n",
      "[964/1778] D loss: 1.2199, G loss: 1.1727\n",
      "[1084/1778] D loss: 0.6809, G loss: 2.0009\n",
      "[1204/1778] D loss: 1.0505, G loss: 1.3856\n",
      "[1324/1778] D loss: 0.9661, G loss: 1.0237\n",
      "[1444/1778] D loss: 0.8332, G loss: 1.0522\n",
      "[1564/1778] D loss: 1.2600, G loss: 0.6956\n",
      "[1684/1778] D loss: 0.9313, G loss: 1.6547\n",
      "train error: \n",
      " D loss: 1.078660, G loss: 0.941424, D accuracy: 74.9%, cell accuracy: 21733.2%%, board accuracy: 14.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.085665, G loss: 0.932839, D accuracy: 75.7%, cell accuracy: 21708.8%%, board accuracy: 12.2% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "[4/1778] D loss: 0.9165, G loss: 1.2190\n",
      "[124/1778] D loss: 1.0106, G loss: 2.9139\n",
      "[244/1778] D loss: 1.0748, G loss: 1.8560\n",
      "[364/1778] D loss: 1.1176, G loss: 1.5001\n",
      "[484/1778] D loss: 0.9798, G loss: 1.1946\n",
      "[604/1778] D loss: 1.1165, G loss: 1.4906\n",
      "[724/1778] D loss: 0.9906, G loss: 1.6758\n",
      "[844/1778] D loss: 1.0701, G loss: 0.7457\n",
      "[964/1778] D loss: 1.0813, G loss: 1.7894\n",
      "[1084/1778] D loss: 1.0086, G loss: 1.2783\n",
      "[1204/1778] D loss: 1.7852, G loss: 1.7015\n",
      "[1324/1778] D loss: 0.9668, G loss: 1.0179\n",
      "[1444/1778] D loss: 1.0497, G loss: 0.8083\n",
      "[1564/1778] D loss: 1.0094, G loss: 0.8472\n",
      "[1684/1778] D loss: 1.0717, G loss: 0.7229\n",
      "train error: \n",
      " D loss: 1.188849, G loss: 1.237161, D accuracy: 67.5%, cell accuracy: 21814.5%%, board accuracy: 22.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.205609, G loss: 1.247084, D accuracy: 67.7%, cell accuracy: 21799.3%%, board accuracy: 22.1% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1168, G loss: 1.6298\n",
      "[124/1778] D loss: 1.1773, G loss: 1.4893\n",
      "[244/1778] D loss: 1.0121, G loss: 0.8880\n",
      "[364/1778] D loss: 1.2455, G loss: 1.0042\n",
      "[484/1778] D loss: 1.1020, G loss: 0.7991\n",
      "[604/1778] D loss: 0.8887, G loss: 1.3912\n",
      "[724/1778] D loss: 0.9583, G loss: 1.1409\n",
      "[844/1778] D loss: 0.9080, G loss: 1.0266\n",
      "[964/1778] D loss: 1.1673, G loss: 1.0213\n",
      "[1084/1778] D loss: 1.4527, G loss: 1.2674\n",
      "[1204/1778] D loss: 1.0718, G loss: 0.7179\n",
      "[1324/1778] D loss: 1.3205, G loss: 0.6487\n",
      "[1444/1778] D loss: 0.9309, G loss: 1.0605\n",
      "[1564/1778] D loss: 1.2569, G loss: 0.8529\n",
      "[1684/1778] D loss: 1.2576, G loss: 0.7988\n",
      "train error: \n",
      " D loss: 1.267566, G loss: 0.777000, D accuracy: 63.0%, cell accuracy: 21854.9%%, board accuracy: 32.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.276210, G loss: 0.790133, D accuracy: 64.9%, cell accuracy: 21838.3%%, board accuracy: 30.0% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4511, G loss: 0.4434\n",
      "[124/1778] D loss: 1.0468, G loss: 1.4232\n",
      "[244/1778] D loss: 1.2411, G loss: 1.1029\n",
      "[364/1778] D loss: 1.2918, G loss: 1.1767\n",
      "[484/1778] D loss: 1.2966, G loss: 0.8391\n",
      "[604/1778] D loss: 1.2854, G loss: 1.0971\n",
      "[724/1778] D loss: 1.2506, G loss: 0.8620\n",
      "[844/1778] D loss: 1.1874, G loss: 0.9573\n",
      "[964/1778] D loss: 1.2158, G loss: 1.0876\n",
      "[1084/1778] D loss: 1.3861, G loss: 0.6488\n",
      "[1204/1778] D loss: 1.1718, G loss: 0.6808\n",
      "[1324/1778] D loss: 1.6069, G loss: 1.2758\n",
      "[1444/1778] D loss: 1.1922, G loss: 1.2661\n",
      "[1564/1778] D loss: 1.3375, G loss: 0.6516\n",
      "[1684/1778] D loss: 1.2926, G loss: 1.0262\n",
      "train error: \n",
      " D loss: 1.253684, G loss: 0.778175, D accuracy: 64.9%, cell accuracy: 21863.9%%, board accuracy: 36.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.258827, G loss: 0.789447, D accuracy: 64.5%, cell accuracy: 21845.9%%, board accuracy: 34.2% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3245, G loss: 0.5973\n",
      "[124/1778] D loss: 1.0178, G loss: 0.9036\n",
      "[244/1778] D loss: 1.3507, G loss: 0.5118\n",
      "[364/1778] D loss: 1.0730, G loss: 0.8539\n",
      "[484/1778] D loss: 1.3688, G loss: 1.2640\n",
      "[604/1778] D loss: 1.2748, G loss: 0.6578\n",
      "[724/1778] D loss: 1.2132, G loss: 0.8036\n",
      "[844/1778] D loss: 1.2304, G loss: 0.6555\n",
      "[964/1778] D loss: 1.1503, G loss: 0.5935\n",
      "[1084/1778] D loss: 1.2738, G loss: 0.7575\n",
      "[1204/1778] D loss: 1.1401, G loss: 1.2001\n",
      "[1324/1778] D loss: 1.2208, G loss: 0.5527\n",
      "[1444/1778] D loss: 0.8076, G loss: 1.0571\n",
      "[1564/1778] D loss: 1.3418, G loss: 0.8529\n",
      "[1684/1778] D loss: 1.1346, G loss: 0.9193\n",
      "train error: \n",
      " D loss: 1.233115, G loss: 0.971548, D accuracy: 65.7%, cell accuracy: 21864.2%%, board accuracy: 36.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.255768, G loss: 0.974961, D accuracy: 65.4%, cell accuracy: 21847.1%%, board accuracy: 33.1% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.6941, G loss: 0.9138\n",
      "[124/1778] D loss: 1.5665, G loss: 0.4488\n",
      "[244/1778] D loss: 1.0411, G loss: 0.7660\n",
      "[364/1778] D loss: 1.2581, G loss: 0.6683\n",
      "[484/1778] D loss: 1.5111, G loss: 0.5676\n",
      "[604/1778] D loss: 0.9703, G loss: 1.0162\n",
      "[724/1778] D loss: 1.3664, G loss: 1.3261\n",
      "[844/1778] D loss: 1.3139, G loss: 0.7470\n",
      "[964/1778] D loss: 1.4253, G loss: 0.7978\n",
      "[1084/1778] D loss: 1.2612, G loss: 0.8491\n",
      "[1204/1778] D loss: 1.3555, G loss: 0.8433\n",
      "[1324/1778] D loss: 1.1938, G loss: 1.3860\n",
      "[1444/1778] D loss: 1.3354, G loss: 0.7260\n",
      "[1564/1778] D loss: 1.3772, G loss: 0.6786\n",
      "[1684/1778] D loss: 1.4264, G loss: 0.9925\n",
      "train error: \n",
      " D loss: 1.385183, G loss: 0.655499, D accuracy: 57.3%, cell accuracy: 21880.8%%, board accuracy: 48.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.397452, G loss: 0.671663, D accuracy: 58.2%, cell accuracy: 21863.7%%, board accuracy: 45.0% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3849, G loss: 0.9975\n",
      "[124/1778] D loss: 1.4643, G loss: 0.6999\n",
      "[244/1778] D loss: 1.2796, G loss: 0.9246\n",
      "[364/1778] D loss: 1.2023, G loss: 0.9964\n",
      "[484/1778] D loss: 1.3028, G loss: 1.0193\n",
      "[604/1778] D loss: 1.2242, G loss: 0.7792\n",
      "[724/1778] D loss: 1.2312, G loss: 0.7925\n",
      "[844/1778] D loss: 1.4050, G loss: 0.5422\n",
      "[964/1778] D loss: 1.4503, G loss: 0.7186\n",
      "[1084/1778] D loss: 1.3371, G loss: 0.7390\n",
      "[1204/1778] D loss: 1.1948, G loss: 1.0389\n",
      "[1324/1778] D loss: 1.1580, G loss: 1.1302\n",
      "[1444/1778] D loss: 1.1677, G loss: 0.6857\n",
      "[1564/1778] D loss: 1.3997, G loss: 0.6602\n",
      "[1684/1778] D loss: 1.3064, G loss: 0.7430\n",
      "train error: \n",
      " D loss: 1.408450, G loss: 0.600781, D accuracy: 56.2%, cell accuracy: 21872.9%%, board accuracy: 45.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.416725, G loss: 0.616908, D accuracy: 55.7%, cell accuracy: 21861.5%%, board accuracy: 44.6% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.5114, G loss: 0.5519\n",
      "[124/1778] D loss: 1.4102, G loss: 0.4112\n",
      "[244/1778] D loss: 1.2705, G loss: 0.8329\n",
      "[364/1778] D loss: 1.4183, G loss: 0.6536\n",
      "[484/1778] D loss: 1.2226, G loss: 0.8066\n",
      "[604/1778] D loss: 1.2637, G loss: 0.8813\n",
      "[724/1778] D loss: 1.2435, G loss: 1.2557\n",
      "[844/1778] D loss: 1.3225, G loss: 0.6872\n",
      "[964/1778] D loss: 1.3464, G loss: 0.6789\n",
      "[1084/1778] D loss: 1.5095, G loss: 0.6983\n",
      "[1204/1778] D loss: 1.4226, G loss: 0.9496\n",
      "[1324/1778] D loss: 1.3059, G loss: 1.0599\n",
      "[1444/1778] D loss: 1.1646, G loss: 1.0111\n",
      "[1564/1778] D loss: 1.4223, G loss: 0.8164\n",
      "[1684/1778] D loss: 1.4331, G loss: 0.6155\n",
      "train error: \n",
      " D loss: 1.366016, G loss: 0.729430, D accuracy: 57.1%, cell accuracy: 21876.6%%, board accuracy: 45.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.375141, G loss: 0.748925, D accuracy: 57.4%, cell accuracy: 21859.5%%, board accuracy: 41.7% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3668, G loss: 0.7969\n",
      "[124/1778] D loss: 1.3939, G loss: 0.5908\n",
      "[244/1778] D loss: 1.3490, G loss: 0.9940\n",
      "[364/1778] D loss: 1.3842, G loss: 0.8689\n",
      "[484/1778] D loss: 1.4359, G loss: 1.0364\n",
      "[604/1778] D loss: 1.6529, G loss: 0.8698\n",
      "[724/1778] D loss: 1.5216, G loss: 0.7074\n",
      "[844/1778] D loss: 1.3948, G loss: 0.7639\n",
      "[964/1778] D loss: 1.4546, G loss: 0.6281\n",
      "[1084/1778] D loss: 1.2964, G loss: 0.8799\n",
      "[1204/1778] D loss: 1.4185, G loss: 0.8136\n",
      "[1324/1778] D loss: 1.3808, G loss: 0.6155\n",
      "[1444/1778] D loss: 1.5239, G loss: 0.6793\n",
      "[1564/1778] D loss: 1.1798, G loss: 1.0816\n",
      "[1684/1778] D loss: 1.3668, G loss: 0.8655\n",
      "train error: \n",
      " D loss: 1.347065, G loss: 0.813794, D accuracy: 58.2%, cell accuracy: 21879.4%%, board accuracy: 47.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.356063, G loss: 0.835696, D accuracy: 58.1%, cell accuracy: 21863.3%%, board accuracy: 42.1% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.5207, G loss: 0.8500\n",
      "[124/1778] D loss: 1.5468, G loss: 1.1387\n",
      "[244/1778] D loss: 1.4603, G loss: 0.8607\n",
      "[364/1778] D loss: 1.3937, G loss: 0.7446\n",
      "[484/1778] D loss: 1.4603, G loss: 0.3971\n",
      "[604/1778] D loss: 1.4224, G loss: 0.6254\n",
      "[724/1778] D loss: 1.3442, G loss: 0.7954\n",
      "[844/1778] D loss: 1.3084, G loss: 0.9460\n",
      "[964/1778] D loss: 1.3922, G loss: 0.6547\n",
      "[1084/1778] D loss: 1.2820, G loss: 0.7282\n",
      "[1204/1778] D loss: 1.3424, G loss: 0.9107\n",
      "[1324/1778] D loss: 1.2003, G loss: 1.2272\n",
      "[1444/1778] D loss: 1.3514, G loss: 0.6663\n",
      "[1564/1778] D loss: 1.4100, G loss: 0.7494\n",
      "[1684/1778] D loss: 1.5858, G loss: 0.6652\n",
      "train error: \n",
      " D loss: 1.380417, G loss: 0.724246, D accuracy: 55.2%, cell accuracy: 21886.8%%, board accuracy: 51.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.380305, G loss: 0.754518, D accuracy: 55.2%, cell accuracy: 21873.9%%, board accuracy: 47.7% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3501, G loss: 0.6129\n",
      "[124/1778] D loss: 1.4019, G loss: 0.7482\n",
      "[244/1778] D loss: 1.5125, G loss: 0.5313\n",
      "[364/1778] D loss: 1.4119, G loss: 0.5905\n",
      "[484/1778] D loss: 1.4855, G loss: 0.4446\n",
      "[604/1778] D loss: 1.3053, G loss: 0.7261\n",
      "[724/1778] D loss: 1.3819, G loss: 0.6898\n",
      "[844/1778] D loss: 1.4846, G loss: 0.7631\n",
      "[964/1778] D loss: 1.2663, G loss: 1.0149\n",
      "[1084/1778] D loss: 1.1108, G loss: 0.9233\n",
      "[1204/1778] D loss: 1.1277, G loss: 0.8403\n",
      "[1324/1778] D loss: 1.2983, G loss: 0.8799\n",
      "[1444/1778] D loss: 1.2856, G loss: 0.9454\n",
      "[1564/1778] D loss: 1.6550, G loss: 0.4935\n",
      "[1684/1778] D loss: 1.2951, G loss: 0.9655\n",
      "train error: \n",
      " D loss: 1.371972, G loss: 0.737780, D accuracy: 55.4%, cell accuracy: 21884.1%%, board accuracy: 49.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.370501, G loss: 0.759571, D accuracy: 57.1%, cell accuracy: 21869.6%%, board accuracy: 45.9% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3609, G loss: 0.7073\n",
      "[124/1778] D loss: 1.3957, G loss: 0.6576\n",
      "[244/1778] D loss: 1.2086, G loss: 1.1330\n",
      "[364/1778] D loss: 1.5219, G loss: 0.5848\n",
      "[484/1778] D loss: 1.3317, G loss: 1.0412\n",
      "[604/1778] D loss: 1.3658, G loss: 0.6624\n",
      "[724/1778] D loss: 1.4091, G loss: 0.5986\n",
      "[844/1778] D loss: 1.4842, G loss: 0.8804\n",
      "[964/1778] D loss: 1.3915, G loss: 0.7097\n",
      "[1084/1778] D loss: 1.4927, G loss: 0.5213\n",
      "[1204/1778] D loss: 1.4481, G loss: 0.8763\n",
      "[1324/1778] D loss: 1.3205, G loss: 1.0145\n",
      "[1444/1778] D loss: 1.3488, G loss: 0.6085\n",
      "[1564/1778] D loss: 1.4051, G loss: 0.6571\n",
      "[1684/1778] D loss: 1.4113, G loss: 0.6403\n",
      "train error: \n",
      " D loss: 1.367486, G loss: 0.727600, D accuracy: 55.3%, cell accuracy: 21884.9%%, board accuracy: 50.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.356746, G loss: 0.750486, D accuracy: 57.3%, cell accuracy: 21869.4%%, board accuracy: 46.6% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2717, G loss: 0.6364\n",
      "[124/1778] D loss: 1.2555, G loss: 0.6547\n",
      "[244/1778] D loss: 1.5488, G loss: 1.0994\n",
      "[364/1778] D loss: 1.1602, G loss: 1.1514\n",
      "[484/1778] D loss: 1.3192, G loss: 0.5660\n",
      "[604/1778] D loss: 1.1808, G loss: 0.9216\n",
      "[724/1778] D loss: 1.4015, G loss: 0.8831\n",
      "[844/1778] D loss: 1.3428, G loss: 0.7418\n",
      "[964/1778] D loss: 1.3650, G loss: 0.9144\n",
      "[1084/1778] D loss: 1.3984, G loss: 0.7291\n",
      "[1204/1778] D loss: 1.1359, G loss: 0.8888\n",
      "[1324/1778] D loss: 1.4122, G loss: 0.6116\n",
      "[1444/1778] D loss: 1.4020, G loss: 0.7284\n",
      "[1564/1778] D loss: 1.3754, G loss: 0.5480\n",
      "[1684/1778] D loss: 1.2933, G loss: 0.6766\n",
      "train error: \n",
      " D loss: 1.404065, G loss: 1.031589, D accuracy: 53.4%, cell accuracy: 21888.9%%, board accuracy: 51.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.403483, G loss: 1.050068, D accuracy: 53.5%, cell accuracy: 21875.2%%, board accuracy: 46.2% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4356, G loss: 0.8638\n",
      "[124/1778] D loss: 1.3828, G loss: 0.7403\n",
      "[244/1778] D loss: 1.1542, G loss: 1.0604\n",
      "[364/1778] D loss: 1.4533, G loss: 0.7051\n",
      "[484/1778] D loss: 1.2958, G loss: 0.7879\n",
      "[604/1778] D loss: 1.2804, G loss: 0.7226\n",
      "[724/1778] D loss: 1.3875, G loss: 0.7552\n",
      "[844/1778] D loss: 1.2614, G loss: 0.8107\n",
      "[964/1778] D loss: 1.4255, G loss: 1.1041\n",
      "[1084/1778] D loss: 1.3296, G loss: 0.8034\n",
      "[1204/1778] D loss: 1.3234, G loss: 0.5858\n",
      "[1324/1778] D loss: 1.3957, G loss: 0.7059\n",
      "[1444/1778] D loss: 1.4319, G loss: 0.5215\n",
      "[1564/1778] D loss: 1.3161, G loss: 0.8459\n",
      "[1684/1778] D loss: 1.3048, G loss: 0.8856\n",
      "train error: \n",
      " D loss: 1.354334, G loss: 0.823166, D accuracy: 56.0%, cell accuracy: 21891.5%%, board accuracy: 52.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.350577, G loss: 0.834281, D accuracy: 56.2%, cell accuracy: 21878.8%%, board accuracy: 48.0% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3829, G loss: 0.9599\n",
      "[124/1778] D loss: 1.5636, G loss: 0.6029\n",
      "[244/1778] D loss: 1.4517, G loss: 0.8126\n",
      "[364/1778] D loss: 1.5214, G loss: 0.6152\n",
      "[484/1778] D loss: 1.4135, G loss: 0.8405\n",
      "[604/1778] D loss: 1.2253, G loss: 0.8472\n",
      "[724/1778] D loss: 1.3504, G loss: 0.5773\n",
      "[844/1778] D loss: 1.2992, G loss: 0.7124\n",
      "[964/1778] D loss: 1.4070, G loss: 0.7936\n",
      "[1084/1778] D loss: 1.5587, G loss: 0.4878\n",
      "[1204/1778] D loss: 1.1719, G loss: 0.7756\n",
      "[1324/1778] D loss: 1.4575, G loss: 0.6731\n",
      "[1444/1778] D loss: 1.2201, G loss: 1.1369\n",
      "[1564/1778] D loss: 1.1205, G loss: 1.0014\n",
      "[1684/1778] D loss: 1.3829, G loss: 0.6318\n",
      "train error: \n",
      " D loss: 1.351055, G loss: 0.781347, D accuracy: 55.6%, cell accuracy: 21911.1%%, board accuracy: 63.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.347520, G loss: 0.792696, D accuracy: 55.9%, cell accuracy: 21897.1%%, board accuracy: 59.9% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3127, G loss: 0.9658\n",
      "[124/1778] D loss: 1.5374, G loss: 0.5732\n",
      "[244/1778] D loss: 1.3471, G loss: 0.8797\n",
      "[364/1778] D loss: 1.2873, G loss: 0.5894\n",
      "[484/1778] D loss: 1.4325, G loss: 0.7081\n",
      "[604/1778] D loss: 1.4345, G loss: 0.7619\n",
      "[724/1778] D loss: 1.1759, G loss: 0.7281\n",
      "[844/1778] D loss: 1.3492, G loss: 1.0590\n",
      "[964/1778] D loss: 1.6233, G loss: 0.6019\n",
      "[1084/1778] D loss: 1.1779, G loss: 0.7863\n",
      "[1204/1778] D loss: 1.2307, G loss: 0.9734\n",
      "[1324/1778] D loss: 1.4146, G loss: 0.5259\n",
      "[1444/1778] D loss: 1.2397, G loss: 0.7931\n",
      "[1564/1778] D loss: 1.4104, G loss: 0.8696\n",
      "[1684/1778] D loss: 1.4473, G loss: 0.8526\n",
      "train error: \n",
      " D loss: 1.351597, G loss: 0.706410, D accuracy: 55.6%, cell accuracy: 21925.5%%, board accuracy: 69.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.344493, G loss: 0.721832, D accuracy: 56.6%, cell accuracy: 21912.4%%, board accuracy: 66.9% \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3930, G loss: 0.7286\n",
      "[124/1778] D loss: 1.4225, G loss: 0.7255\n",
      "[244/1778] D loss: 1.3696, G loss: 0.6985\n",
      "[364/1778] D loss: 1.4905, G loss: 0.7312\n",
      "[484/1778] D loss: 1.4930, G loss: 0.6185\n",
      "[604/1778] D loss: 1.4432, G loss: 0.9624\n",
      "[724/1778] D loss: 1.4288, G loss: 0.7190\n",
      "[844/1778] D loss: 1.3964, G loss: 0.7095\n",
      "[964/1778] D loss: 1.1756, G loss: 0.9616\n",
      "[1084/1778] D loss: 1.3761, G loss: 0.7400\n",
      "[1204/1778] D loss: 1.3259, G loss: 0.9262\n",
      "[1324/1778] D loss: 1.4518, G loss: 0.9535\n",
      "[1444/1778] D loss: 1.3643, G loss: 0.5653\n",
      "[1564/1778] D loss: 1.2272, G loss: 0.8974\n",
      "[1684/1778] D loss: 1.4008, G loss: 0.6349\n",
      "train error: \n",
      " D loss: 1.363624, G loss: 0.626956, D accuracy: 55.3%, cell accuracy: 21926.2%%, board accuracy: 68.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.356277, G loss: 0.638803, D accuracy: 56.2%, cell accuracy: 21916.0%%, board accuracy: 66.4% \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2611, G loss: 0.5964\n",
      "[124/1778] D loss: 1.3942, G loss: 0.6481\n",
      "[244/1778] D loss: 1.4304, G loss: 0.6050\n",
      "[364/1778] D loss: 1.4344, G loss: 0.5971\n",
      "[484/1778] D loss: 1.4473, G loss: 0.9487\n",
      "[604/1778] D loss: 1.3939, G loss: 0.5851\n",
      "[724/1778] D loss: 1.4046, G loss: 0.5730\n",
      "[844/1778] D loss: 1.4901, G loss: 0.5730\n",
      "[964/1778] D loss: 1.2763, G loss: 0.6686\n",
      "[1084/1778] D loss: 1.4225, G loss: 0.8633\n",
      "[1204/1778] D loss: 1.4609, G loss: 0.8705\n",
      "[1324/1778] D loss: 1.4208, G loss: 0.9333\n",
      "[1444/1778] D loss: 1.3382, G loss: 0.6335\n",
      "[1564/1778] D loss: 1.1551, G loss: 0.8700\n",
      "[1684/1778] D loss: 1.3977, G loss: 0.6655\n",
      "train error: \n",
      " D loss: 1.353172, G loss: 0.685697, D accuracy: 55.4%, cell accuracy: 21925.2%%, board accuracy: 68.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.344683, G loss: 0.696009, D accuracy: 55.5%, cell accuracy: 21918.2%%, board accuracy: 66.4% \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4093, G loss: 0.5212\n",
      "[124/1778] D loss: 1.3091, G loss: 0.7728\n",
      "[244/1778] D loss: 1.4508, G loss: 0.5198\n",
      "[364/1778] D loss: 1.4167, G loss: 0.6237\n",
      "[484/1778] D loss: 1.3741, G loss: 0.7767\n",
      "[604/1778] D loss: 1.3647, G loss: 0.6331\n",
      "[724/1778] D loss: 1.3826, G loss: 0.8282\n",
      "[844/1778] D loss: 1.5320, G loss: 1.0569\n",
      "[964/1778] D loss: 1.3662, G loss: 0.5894\n",
      "[1084/1778] D loss: 1.3830, G loss: 0.6257\n",
      "[1204/1778] D loss: 1.3617, G loss: 0.7468\n",
      "[1324/1778] D loss: 1.2413, G loss: 0.8246\n",
      "[1444/1778] D loss: 1.4022, G loss: 0.8005\n",
      "[1564/1778] D loss: 1.4098, G loss: 0.8349\n",
      "[1684/1778] D loss: 1.2794, G loss: 0.8512\n",
      "train error: \n",
      " D loss: 1.355578, G loss: 0.732651, D accuracy: 54.4%, cell accuracy: 21923.3%%, board accuracy: 66.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.349339, G loss: 0.740469, D accuracy: 56.1%, cell accuracy: 21914.6%%, board accuracy: 65.5% \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3533, G loss: 0.7978\n",
      "[124/1778] D loss: 1.2344, G loss: 0.7684\n",
      "[244/1778] D loss: 1.3521, G loss: 0.8601\n",
      "[364/1778] D loss: 1.4409, G loss: 0.7211\n",
      "[484/1778] D loss: 1.2084, G loss: 0.8214\n",
      "[604/1778] D loss: 1.4655, G loss: 0.6395\n",
      "[724/1778] D loss: 1.2556, G loss: 0.7829\n",
      "[844/1778] D loss: 1.4395, G loss: 0.6949\n",
      "[964/1778] D loss: 1.3463, G loss: 0.6409\n",
      "[1084/1778] D loss: 1.3995, G loss: 0.6587\n",
      "[1204/1778] D loss: 1.5487, G loss: 0.6260\n",
      "[1324/1778] D loss: 1.3885, G loss: 0.7843\n",
      "[1444/1778] D loss: 1.3905, G loss: 0.6500\n",
      "[1564/1778] D loss: 1.3372, G loss: 0.8287\n",
      "[1684/1778] D loss: 1.2619, G loss: 0.8114\n",
      "train error: \n",
      " D loss: 1.362609, G loss: 0.735078, D accuracy: 53.4%, cell accuracy: 21901.5%%, board accuracy: 55.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.362592, G loss: 0.737639, D accuracy: 54.1%, cell accuracy: 21891.9%%, board accuracy: 55.4% \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3627, G loss: 0.7517\n",
      "[124/1778] D loss: 1.2697, G loss: 0.7466\n",
      "[244/1778] D loss: 1.2211, G loss: 0.7929\n",
      "[364/1778] D loss: 1.4008, G loss: 0.8422\n",
      "[484/1778] D loss: 1.1577, G loss: 0.9693\n",
      "[604/1778] D loss: 1.3048, G loss: 0.8765\n",
      "[724/1778] D loss: 1.3557, G loss: 0.8563\n",
      "[844/1778] D loss: 1.2105, G loss: 0.8803\n",
      "[964/1778] D loss: 1.2755, G loss: 0.7036\n",
      "[1084/1778] D loss: 1.3973, G loss: 0.7127\n",
      "[1204/1778] D loss: 1.3931, G loss: 0.5828\n",
      "[1324/1778] D loss: 1.4519, G loss: 0.7187\n",
      "[1444/1778] D loss: 1.3811, G loss: 0.6673\n",
      "[1564/1778] D loss: 1.1931, G loss: 0.8485\n",
      "[1684/1778] D loss: 1.3993, G loss: 0.7255\n",
      "train error: \n",
      " D loss: 1.364673, G loss: 0.658762, D accuracy: 54.0%, cell accuracy: 21906.1%%, board accuracy: 58.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.361677, G loss: 0.668005, D accuracy: 55.0%, cell accuracy: 21900.0%%, board accuracy: 59.0% \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1913, G loss: 0.7698\n",
      "[124/1778] D loss: 1.4389, G loss: 0.5915\n",
      "[244/1778] D loss: 1.3210, G loss: 0.9441\n",
      "[364/1778] D loss: 1.2421, G loss: 0.8731\n",
      "[484/1778] D loss: 1.3580, G loss: 0.7754\n",
      "[604/1778] D loss: 1.0463, G loss: 1.0735\n",
      "[724/1778] D loss: 1.4350, G loss: 0.6735\n",
      "[844/1778] D loss: 1.3748, G loss: 0.7602\n",
      "[964/1778] D loss: 1.4501, G loss: 0.6010\n",
      "[1084/1778] D loss: 1.4047, G loss: 0.7131\n",
      "[1204/1778] D loss: 1.2407, G loss: 0.8291\n",
      "[1324/1778] D loss: 1.4538, G loss: 0.9712\n",
      "[1444/1778] D loss: 1.3982, G loss: 0.7674\n",
      "[1564/1778] D loss: 1.4117, G loss: 0.8929\n",
      "[1684/1778] D loss: 1.2033, G loss: 0.8127\n",
      "train error: \n",
      " D loss: 1.358989, G loss: 0.699123, D accuracy: 53.5%, cell accuracy: 21925.0%%, board accuracy: 69.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.348634, G loss: 0.711712, D accuracy: 55.4%, cell accuracy: 21916.4%%, board accuracy: 66.9% \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3875, G loss: 0.6938\n",
      "[124/1778] D loss: 1.4350, G loss: 0.5914\n",
      "[244/1778] D loss: 1.4091, G loss: 0.5842\n",
      "[364/1778] D loss: 1.4553, G loss: 0.8534\n",
      "[484/1778] D loss: 1.3864, G loss: 0.7819\n",
      "[604/1778] D loss: 1.5174, G loss: 0.5186\n",
      "[724/1778] D loss: 1.4084, G loss: 0.9113\n",
      "[844/1778] D loss: 1.3837, G loss: 0.6257\n",
      "[964/1778] D loss: 1.3534, G loss: 0.6893\n",
      "[1084/1778] D loss: 1.4165, G loss: 0.6446\n",
      "[1204/1778] D loss: 1.3951, G loss: 0.8594\n",
      "[1324/1778] D loss: 1.2250, G loss: 0.7519\n",
      "[1444/1778] D loss: 1.3745, G loss: 0.9208\n",
      "[1564/1778] D loss: 1.2442, G loss: 0.7321\n",
      "[1684/1778] D loss: 1.4386, G loss: 0.6019\n",
      "train error: \n",
      " D loss: 1.361575, G loss: 0.659882, D accuracy: 54.1%, cell accuracy: 21927.2%%, board accuracy: 70.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.350935, G loss: 0.667659, D accuracy: 54.2%, cell accuracy: 21914.4%%, board accuracy: 67.3% \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2871, G loss: 0.5954\n",
      "[124/1778] D loss: 1.4058, G loss: 0.7438\n",
      "[244/1778] D loss: 1.4299, G loss: 0.6855\n",
      "[364/1778] D loss: 1.4182, G loss: 0.6622\n",
      "[484/1778] D loss: 1.3991, G loss: 0.7925\n",
      "[604/1778] D loss: 1.4086, G loss: 0.5332\n",
      "[724/1778] D loss: 1.3189, G loss: 0.7551\n",
      "[844/1778] D loss: 1.4265, G loss: 0.5711\n",
      "[964/1778] D loss: 1.5590, G loss: 0.5389\n",
      "[1084/1778] D loss: 1.4214, G loss: 0.6913\n",
      "[1204/1778] D loss: 1.0401, G loss: 0.8585\n",
      "[1324/1778] D loss: 1.2448, G loss: 0.7087\n",
      "[1444/1778] D loss: 1.2018, G loss: 0.6282\n",
      "[1564/1778] D loss: 1.4099, G loss: 0.6036\n",
      "[1684/1778] D loss: 1.2328, G loss: 0.7362\n",
      "train error: \n",
      " D loss: 1.356261, G loss: 0.663777, D accuracy: 53.4%, cell accuracy: 21924.2%%, board accuracy: 69.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.346973, G loss: 0.672860, D accuracy: 54.8%, cell accuracy: 21913.7%%, board accuracy: 66.9% \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4169, G loss: 0.6049\n",
      "[124/1778] D loss: 1.3067, G loss: 0.6162\n",
      "[244/1778] D loss: 1.9919, G loss: 0.4286\n",
      "[364/1778] D loss: 1.7473, G loss: 0.6596\n",
      "[484/1778] D loss: 1.4045, G loss: 0.8036\n",
      "[604/1778] D loss: 1.2383, G loss: 0.9762\n",
      "[724/1778] D loss: 1.4191, G loss: 1.0300\n",
      "[844/1778] D loss: 1.5330, G loss: 0.6857\n",
      "[964/1778] D loss: 1.3842, G loss: 0.7006\n",
      "[1084/1778] D loss: 1.4079, G loss: 0.7736\n",
      "[1204/1778] D loss: 1.3840, G loss: 0.7886\n",
      "[1324/1778] D loss: 1.4578, G loss: 0.6110\n",
      "[1444/1778] D loss: 1.3943, G loss: 0.8183\n",
      "[1564/1778] D loss: 1.4164, G loss: 0.7751\n",
      "[1684/1778] D loss: 1.4030, G loss: 0.7568\n",
      "train error: \n",
      " D loss: 1.402112, G loss: 0.636917, D accuracy: 50.1%, cell accuracy: 21932.8%%, board accuracy: 72.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.400796, G loss: 0.642004, D accuracy: 50.5%, cell accuracy: 21920.9%%, board accuracy: 70.7% \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3856, G loss: 0.6698\n",
      "[124/1778] D loss: 1.3899, G loss: 0.6748\n",
      "[244/1778] D loss: 1.3798, G loss: 0.6910\n",
      "[364/1778] D loss: 1.3976, G loss: 0.6810\n",
      "[484/1778] D loss: 1.4088, G loss: 0.6451\n",
      "[604/1778] D loss: 1.4002, G loss: 0.7100\n",
      "[724/1778] D loss: 1.3555, G loss: 0.7361\n",
      "[844/1778] D loss: 1.4205, G loss: 0.7552\n",
      "[964/1778] D loss: 1.4451, G loss: 0.9529\n",
      "[1084/1778] D loss: 1.3956, G loss: 0.6102\n",
      "[1204/1778] D loss: 1.3895, G loss: 0.6358\n",
      "[1324/1778] D loss: 1.4011, G loss: 0.6711\n",
      "[1444/1778] D loss: 1.3927, G loss: 0.7024\n",
      "[1564/1778] D loss: 1.4023, G loss: 0.7342\n",
      "[1684/1778] D loss: 1.3920, G loss: 0.6438\n",
      "train error: \n",
      " D loss: 1.379539, G loss: 0.698538, D accuracy: 52.5%, cell accuracy: 21932.8%%, board accuracy: 73.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.377038, G loss: 0.701954, D accuracy: 52.8%, cell accuracy: 21919.4%%, board accuracy: 69.1% \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3923, G loss: 0.6197\n",
      "[124/1778] D loss: 1.4087, G loss: 0.6336\n",
      "[244/1778] D loss: 1.3782, G loss: 0.6635\n",
      "[364/1778] D loss: 1.3856, G loss: 0.6788\n",
      "[484/1778] D loss: 1.3820, G loss: 0.7318\n",
      "[604/1778] D loss: 1.4069, G loss: 0.7181\n",
      "[724/1778] D loss: 1.2102, G loss: 0.7123\n",
      "[844/1778] D loss: 1.2922, G loss: 0.6192\n",
      "[964/1778] D loss: 1.3832, G loss: 0.7307\n",
      "[1084/1778] D loss: 1.3893, G loss: 0.7223\n",
      "[1204/1778] D loss: 1.3759, G loss: 0.6694\n",
      "[1324/1778] D loss: 1.2592, G loss: 0.7729\n",
      "[1444/1778] D loss: 1.3885, G loss: 0.6695\n",
      "[1564/1778] D loss: 1.3960, G loss: 0.7697\n",
      "[1684/1778] D loss: 1.3913, G loss: 0.6471\n",
      "train error: \n",
      " D loss: 1.366734, G loss: 0.795621, D accuracy: 53.6%, cell accuracy: 21933.4%%, board accuracy: 73.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.358519, G loss: 0.800961, D accuracy: 55.0%, cell accuracy: 21919.8%%, board accuracy: 69.4% \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3822, G loss: 0.7472\n",
      "[124/1778] D loss: 1.4076, G loss: 0.8602\n",
      "[244/1778] D loss: 1.3785, G loss: 0.7343\n",
      "[364/1778] D loss: 1.3825, G loss: 0.6698\n",
      "[484/1778] D loss: 1.3154, G loss: 0.6889\n",
      "[604/1778] D loss: 1.3881, G loss: 0.6988\n",
      "[724/1778] D loss: 1.4141, G loss: 0.6913\n",
      "[844/1778] D loss: 1.3935, G loss: 0.6088\n",
      "[964/1778] D loss: 1.4171, G loss: 0.6850\n",
      "[1084/1778] D loss: 1.4036, G loss: 0.5670\n",
      "[1204/1778] D loss: 1.3667, G loss: 0.8183\n",
      "[1324/1778] D loss: 1.2664, G loss: 0.7813\n",
      "[1444/1778] D loss: 1.4433, G loss: 0.5728\n",
      "[1564/1778] D loss: 1.3645, G loss: 0.6622\n",
      "[1684/1778] D loss: 1.4027, G loss: 0.6811\n",
      "train error: \n",
      " D loss: 1.352721, G loss: 0.784261, D accuracy: 54.0%, cell accuracy: 21928.6%%, board accuracy: 70.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.346238, G loss: 0.783262, D accuracy: 54.1%, cell accuracy: 21916.2%%, board accuracy: 66.7% \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3923, G loss: 0.7904\n",
      "[124/1778] D loss: 1.3998, G loss: 0.7348\n",
      "[244/1778] D loss: 1.3756, G loss: 0.7168\n",
      "[364/1778] D loss: 1.4166, G loss: 0.6668\n",
      "[484/1778] D loss: 1.3737, G loss: 0.7173\n",
      "[604/1778] D loss: 1.4038, G loss: 0.6715\n",
      "[724/1778] D loss: 1.3767, G loss: 0.7315\n",
      "[844/1778] D loss: 1.2608, G loss: 0.9044\n",
      "[964/1778] D loss: 1.4056, G loss: 0.7838\n",
      "[1084/1778] D loss: 1.4124, G loss: 0.6828\n",
      "[1204/1778] D loss: 1.6450, G loss: 0.6042\n",
      "[1324/1778] D loss: 1.4951, G loss: 0.7701\n",
      "[1444/1778] D loss: 1.3943, G loss: 0.6190\n",
      "[1564/1778] D loss: 1.1772, G loss: 0.6842\n",
      "[1684/1778] D loss: 1.4130, G loss: 0.5303\n",
      "train error: \n",
      " D loss: 1.415417, G loss: 0.767957, D accuracy: 49.6%, cell accuracy: 21931.9%%, board accuracy: 72.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.416090, G loss: 0.773225, D accuracy: 49.1%, cell accuracy: 21918.2%%, board accuracy: 68.5% \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.5336, G loss: 0.7563\n",
      "[124/1778] D loss: 1.4045, G loss: 0.7292\n",
      "[244/1778] D loss: 1.3826, G loss: 0.6772\n",
      "[364/1778] D loss: 1.3830, G loss: 0.6880\n",
      "[484/1778] D loss: 1.4004, G loss: 0.7900\n",
      "[604/1778] D loss: 1.3443, G loss: 0.6748\n",
      "[724/1778] D loss: 1.3854, G loss: 0.6791\n",
      "[844/1778] D loss: 1.4208, G loss: 0.6693\n",
      "[964/1778] D loss: 1.3755, G loss: 0.7115\n",
      "[1084/1778] D loss: 1.4073, G loss: 0.7567\n",
      "[1204/1778] D loss: 1.4304, G loss: 0.7205\n",
      "[1324/1778] D loss: 1.3679, G loss: 0.7494\n",
      "[1444/1778] D loss: 1.4161, G loss: 0.6611\n",
      "[1564/1778] D loss: 1.3666, G loss: 0.7806\n",
      "[1684/1778] D loss: 1.3893, G loss: 0.6252\n",
      "train error: \n",
      " D loss: 1.378893, G loss: 0.689098, D accuracy: 52.3%, cell accuracy: 21938.4%%, board accuracy: 75.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.376316, G loss: 0.692051, D accuracy: 52.9%, cell accuracy: 21927.9%%, board accuracy: 74.1% \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3903, G loss: 0.7335\n",
      "[124/1778] D loss: 1.3837, G loss: 0.6860\n",
      "[244/1778] D loss: 1.4007, G loss: 0.6506\n",
      "[364/1778] D loss: 1.3925, G loss: 0.7539\n",
      "[484/1778] D loss: 1.3322, G loss: 0.7210\n",
      "[604/1778] D loss: 1.3942, G loss: 0.7862\n",
      "[724/1778] D loss: 1.3917, G loss: 0.6579\n",
      "[844/1778] D loss: 1.4423, G loss: 0.7059\n",
      "[964/1778] D loss: 1.3929, G loss: 0.7961\n",
      "[1084/1778] D loss: 1.3866, G loss: 0.7299\n",
      "[1204/1778] D loss: 1.2358, G loss: 0.7243\n",
      "[1324/1778] D loss: 1.2970, G loss: 0.8399\n",
      "[1444/1778] D loss: 1.3822, G loss: 0.6980\n",
      "[1564/1778] D loss: 1.2601, G loss: 0.8040\n",
      "[1684/1778] D loss: 1.2904, G loss: 0.8660\n",
      "train error: \n",
      " D loss: 1.354744, G loss: 0.698125, D accuracy: 54.2%, cell accuracy: 21940.4%%, board accuracy: 76.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.345715, G loss: 0.704312, D accuracy: 55.1%, cell accuracy: 21929.5%%, board accuracy: 73.0% \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1919, G loss: 0.7889\n",
      "[124/1778] D loss: 1.3979, G loss: 0.6755\n",
      "[244/1778] D loss: 1.2358, G loss: 0.6986\n",
      "[364/1778] D loss: 1.3869, G loss: 0.7428\n",
      "[484/1778] D loss: 1.3434, G loss: 0.7539\n",
      "[604/1778] D loss: 1.4046, G loss: 0.5750\n",
      "[724/1778] D loss: 1.3917, G loss: 0.5954\n",
      "[844/1778] D loss: 1.4007, G loss: 0.7646\n",
      "[964/1778] D loss: 1.0707, G loss: 0.8194\n",
      "[1084/1778] D loss: 1.3720, G loss: 0.9235\n",
      "[1204/1778] D loss: 1.1193, G loss: 0.7350\n",
      "[1324/1778] D loss: 1.3819, G loss: 0.6482\n",
      "[1444/1778] D loss: 1.2153, G loss: 0.7931\n",
      "[1564/1778] D loss: 1.1835, G loss: 0.7356\n",
      "[1684/1778] D loss: 1.1705, G loss: 0.9191\n",
      "train error: \n",
      " D loss: 1.344918, G loss: 0.762558, D accuracy: 53.7%, cell accuracy: 21937.3%%, board accuracy: 74.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.334030, G loss: 0.767920, D accuracy: 54.7%, cell accuracy: 21920.9%%, board accuracy: 71.8% \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3783, G loss: 0.7728\n",
      "[124/1778] D loss: 1.1718, G loss: 0.7805\n",
      "[244/1778] D loss: 1.2076, G loss: 0.7823\n",
      "[364/1778] D loss: 1.2207, G loss: 0.9140\n",
      "[484/1778] D loss: 1.3294, G loss: 0.7703\n",
      "[604/1778] D loss: 1.3939, G loss: 0.8052\n",
      "[724/1778] D loss: 1.4130, G loss: 0.6361\n",
      "[844/1778] D loss: 1.4059, G loss: 0.7062\n",
      "[964/1778] D loss: 1.2083, G loss: 0.8928\n",
      "[1084/1778] D loss: 1.3959, G loss: 0.7606\n",
      "[1204/1778] D loss: 1.2180, G loss: 0.7393\n",
      "[1324/1778] D loss: 1.3949, G loss: 0.8600\n",
      "[1444/1778] D loss: 1.3868, G loss: 0.7176\n",
      "[1564/1778] D loss: 1.1867, G loss: 0.7242\n",
      "[1684/1778] D loss: 1.3817, G loss: 0.6266\n",
      "train error: \n",
      " D loss: 1.350822, G loss: 0.634773, D accuracy: 53.8%, cell accuracy: 21931.6%%, board accuracy: 71.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.337330, G loss: 0.642134, D accuracy: 54.7%, cell accuracy: 21920.7%%, board accuracy: 70.3% \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2029, G loss: 0.6760\n",
      "[124/1778] D loss: 1.3909, G loss: 0.6711\n",
      "[244/1778] D loss: 1.4039, G loss: 0.7992\n",
      "[364/1778] D loss: 1.4953, G loss: 0.6934\n",
      "[484/1778] D loss: 1.2257, G loss: 0.9347\n",
      "[604/1778] D loss: 1.6471, G loss: 0.6404\n",
      "[724/1778] D loss: 1.2661, G loss: 0.6190\n",
      "[844/1778] D loss: 1.3510, G loss: 0.7831\n",
      "[964/1778] D loss: 1.4209, G loss: 0.6319\n",
      "[1084/1778] D loss: 1.3879, G loss: 0.6948\n",
      "[1204/1778] D loss: 1.3939, G loss: 0.6610\n",
      "[1324/1778] D loss: 1.3839, G loss: 0.7315\n",
      "[1444/1778] D loss: 1.3665, G loss: 0.8559\n",
      "[1564/1778] D loss: 1.3893, G loss: 0.6680\n",
      "[1684/1778] D loss: 1.3765, G loss: 0.7198\n",
      "train error: \n",
      " D loss: 1.384542, G loss: 0.663182, D accuracy: 52.0%, cell accuracy: 21937.5%%, board accuracy: 74.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.382204, G loss: 0.667208, D accuracy: 51.8%, cell accuracy: 21926.4%%, board accuracy: 70.9% \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4035, G loss: 0.6528\n",
      "[124/1778] D loss: 1.3932, G loss: 0.7621\n",
      "[244/1778] D loss: 1.3869, G loss: 0.6948\n",
      "[364/1778] D loss: 1.3700, G loss: 0.7091\n",
      "[484/1778] D loss: 1.3974, G loss: 0.7147\n",
      "[604/1778] D loss: 1.3918, G loss: 0.7100\n",
      "[724/1778] D loss: 1.3052, G loss: 0.7506\n",
      "[844/1778] D loss: 1.3373, G loss: 0.7638\n",
      "[964/1778] D loss: 1.3858, G loss: 0.6708\n",
      "[1084/1778] D loss: 1.3881, G loss: 0.6668\n",
      "[1204/1778] D loss: 1.3794, G loss: 0.7247\n",
      "[1324/1778] D loss: 1.3729, G loss: 0.7461\n",
      "[1444/1778] D loss: 1.2713, G loss: 0.7288\n",
      "[1564/1778] D loss: 1.4226, G loss: 0.6339\n",
      "[1684/1778] D loss: 1.4050, G loss: 0.6408\n",
      "train error: \n",
      " D loss: 1.355316, G loss: 0.748367, D accuracy: 53.7%, cell accuracy: 21944.3%%, board accuracy: 78.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.346918, G loss: 0.755773, D accuracy: 55.9%, cell accuracy: 21935.8%%, board accuracy: 77.7% \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3932, G loss: 0.6716\n",
      "[124/1778] D loss: 1.3758, G loss: 0.6910\n",
      "[244/1778] D loss: 1.3673, G loss: 0.7843\n",
      "[364/1778] D loss: 1.3933, G loss: 0.7286\n",
      "[484/1778] D loss: 1.2342, G loss: 0.8234\n",
      "[604/1778] D loss: 1.3918, G loss: 0.6318\n",
      "[724/1778] D loss: 1.3926, G loss: 0.8403\n",
      "[844/1778] D loss: 1.4006, G loss: 0.6928\n",
      "[964/1778] D loss: 1.3897, G loss: 0.7458\n",
      "[1084/1778] D loss: 1.2142, G loss: 0.8029\n",
      "[1204/1778] D loss: 1.3953, G loss: 0.6726\n",
      "[1324/1778] D loss: 1.3645, G loss: 0.6938\n",
      "[1444/1778] D loss: 1.3937, G loss: 0.8450\n",
      "[1564/1778] D loss: 1.4612, G loss: 0.5565\n",
      "[1684/1778] D loss: 1.4247, G loss: 0.7218\n",
      "train error: \n",
      " D loss: 1.346514, G loss: 0.818943, D accuracy: 53.5%, cell accuracy: 21936.2%%, board accuracy: 72.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.332236, G loss: 0.829759, D accuracy: 54.4%, cell accuracy: 21920.9%%, board accuracy: 66.7% \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4022, G loss: 0.7888\n",
      "[124/1778] D loss: 1.3828, G loss: 0.6879\n",
      "[244/1778] D loss: 1.4182, G loss: 0.6848\n",
      "[364/1778] D loss: 1.2013, G loss: 0.8168\n",
      "[484/1778] D loss: 1.3935, G loss: 0.6222\n",
      "[604/1778] D loss: 1.4104, G loss: 0.7947\n",
      "[724/1778] D loss: 1.4035, G loss: 0.7493\n",
      "[844/1778] D loss: 1.3878, G loss: 0.6928\n",
      "[964/1778] D loss: 1.3966, G loss: 0.7385\n",
      "[1084/1778] D loss: 1.4054, G loss: 0.5493\n",
      "[1204/1778] D loss: 1.1778, G loss: 0.7817\n",
      "[1324/1778] D loss: 1.3956, G loss: 0.6736\n",
      "[1444/1778] D loss: 1.4142, G loss: 0.7190\n",
      "[1564/1778] D loss: 1.3889, G loss: 0.7287\n",
      "[1684/1778] D loss: 1.3932, G loss: 0.6169\n",
      "train error: \n",
      " D loss: 1.335923, G loss: 0.809445, D accuracy: 53.6%, cell accuracy: 21931.0%%, board accuracy: 70.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.319818, G loss: 0.823104, D accuracy: 54.2%, cell accuracy: 21917.1%%, board accuracy: 66.0% \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1619, G loss: 0.8231\n",
      "[124/1778] D loss: 1.4029, G loss: 0.7547\n",
      "[244/1778] D loss: 1.3903, G loss: 0.6639\n",
      "[364/1778] D loss: 1.5247, G loss: 0.5454\n",
      "[484/1778] D loss: 1.4703, G loss: 0.6295\n",
      "[604/1778] D loss: 1.2026, G loss: 0.8510\n",
      "[724/1778] D loss: 1.2842, G loss: 1.0434\n",
      "[844/1778] D loss: 1.4024, G loss: 0.6722\n",
      "[964/1778] D loss: 1.3881, G loss: 0.6577\n",
      "[1084/1778] D loss: 1.2934, G loss: 0.8486\n",
      "[1204/1778] D loss: 1.3753, G loss: 0.6760\n",
      "[1324/1778] D loss: 1.2983, G loss: 0.8726\n",
      "[1444/1778] D loss: 1.3434, G loss: 0.7197\n",
      "[1564/1778] D loss: 1.3079, G loss: 0.7754\n",
      "[1684/1778] D loss: 1.3909, G loss: 0.6700\n",
      "train error: \n",
      " D loss: 1.355954, G loss: 0.719257, D accuracy: 53.8%, cell accuracy: 21938.6%%, board accuracy: 74.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.348233, G loss: 0.726003, D accuracy: 55.0%, cell accuracy: 21925.9%%, board accuracy: 69.1% \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3881, G loss: 0.7478\n",
      "[124/1778] D loss: 1.2837, G loss: 0.8252\n",
      "[244/1778] D loss: 1.3803, G loss: 0.7265\n",
      "[364/1778] D loss: 1.3954, G loss: 0.6337\n",
      "[484/1778] D loss: 1.4037, G loss: 0.6913\n",
      "[604/1778] D loss: 1.3875, G loss: 0.6647\n",
      "[724/1778] D loss: 1.2260, G loss: 0.7762\n",
      "[844/1778] D loss: 1.1938, G loss: 0.7849\n",
      "[964/1778] D loss: 1.2793, G loss: 0.6461\n",
      "[1084/1778] D loss: 1.3939, G loss: 0.7399\n",
      "[1204/1778] D loss: 1.3991, G loss: 0.6064\n",
      "[1324/1778] D loss: 1.1676, G loss: 0.7882\n",
      "[1444/1778] D loss: 1.3965, G loss: 0.6942\n",
      "[1564/1778] D loss: 1.3023, G loss: 0.7577\n",
      "[1684/1778] D loss: 1.1989, G loss: 0.7449\n",
      "train error: \n",
      " D loss: 1.328481, G loss: 0.750965, D accuracy: 54.3%, cell accuracy: 21933.0%%, board accuracy: 72.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.316442, G loss: 0.759536, D accuracy: 55.4%, cell accuracy: 21919.4%%, board accuracy: 66.4% \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3925, G loss: 0.7224\n",
      "[124/1778] D loss: 1.4051, G loss: 0.6654\n",
      "[244/1778] D loss: 1.4023, G loss: 0.7719\n",
      "[364/1778] D loss: 1.3819, G loss: 0.6340\n",
      "[484/1778] D loss: 1.2813, G loss: 0.7761\n",
      "[604/1778] D loss: 1.3845, G loss: 0.7963\n",
      "[724/1778] D loss: 1.3760, G loss: 0.6963\n",
      "[844/1778] D loss: 1.3895, G loss: 0.7637\n",
      "[964/1778] D loss: 1.2180, G loss: 0.6240\n",
      "[1084/1778] D loss: 1.3984, G loss: 0.6677\n",
      "[1204/1778] D loss: 1.1403, G loss: 0.7068\n",
      "[1324/1778] D loss: 1.4256, G loss: 0.7401\n",
      "[1444/1778] D loss: 1.3919, G loss: 0.7338\n",
      "[1564/1778] D loss: 1.4220, G loss: 0.8297\n",
      "[1684/1778] D loss: 1.4291, G loss: 0.7546\n",
      "train error: \n",
      " D loss: 1.326476, G loss: 0.755189, D accuracy: 54.1%, cell accuracy: 21944.8%%, board accuracy: 79.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.311135, G loss: 0.766383, D accuracy: 55.0%, cell accuracy: 21935.1%%, board accuracy: 76.4% \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2032, G loss: 0.8435\n",
      "[124/1778] D loss: 1.3994, G loss: 0.7085\n",
      "[244/1778] D loss: 1.4094, G loss: 0.7743\n",
      "[364/1778] D loss: 1.3893, G loss: 0.6000\n",
      "[484/1778] D loss: 1.3862, G loss: 0.6065\n",
      "[604/1778] D loss: 1.5007, G loss: 0.7848\n",
      "[724/1778] D loss: 1.3944, G loss: 0.7417\n",
      "[844/1778] D loss: 1.4294, G loss: 0.7156\n",
      "[964/1778] D loss: 0.9836, G loss: 0.9468\n",
      "[1084/1778] D loss: 1.2591, G loss: 0.8999\n",
      "[1204/1778] D loss: 1.4360, G loss: 0.6144\n",
      "[1324/1778] D loss: 1.5586, G loss: 0.7357\n",
      "[1444/1778] D loss: 1.3938, G loss: 0.6944\n",
      "[1564/1778] D loss: 1.4469, G loss: 0.7858\n",
      "[1684/1778] D loss: 1.3882, G loss: 0.7440\n",
      "train error: \n",
      " D loss: 1.413827, G loss: 0.690377, D accuracy: 47.6%, cell accuracy: 21950.1%%, board accuracy: 83.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.416759, G loss: 0.690560, D accuracy: 46.8%, cell accuracy: 21940.3%%, board accuracy: 80.6% \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4566, G loss: 0.6220\n",
      "[124/1778] D loss: 1.3774, G loss: 0.6794\n",
      "[244/1778] D loss: 1.3946, G loss: 0.7558\n",
      "[364/1778] D loss: 1.4082, G loss: 0.6792\n",
      "[484/1778] D loss: 1.4243, G loss: 0.7282\n",
      "[604/1778] D loss: 1.3274, G loss: 0.7654\n",
      "[724/1778] D loss: 1.4013, G loss: 0.6935\n",
      "[844/1778] D loss: 1.4021, G loss: 0.7002\n",
      "[964/1778] D loss: 1.3956, G loss: 0.6307\n",
      "[1084/1778] D loss: 1.3827, G loss: 0.7027\n",
      "[1204/1778] D loss: 1.4001, G loss: 0.6879\n",
      "[1324/1778] D loss: 1.4006, G loss: 0.7299\n",
      "[1444/1778] D loss: 1.3913, G loss: 0.7069\n",
      "[1564/1778] D loss: 1.3937, G loss: 0.6778\n",
      "[1684/1778] D loss: 1.3864, G loss: 0.7586\n",
      "train error: \n",
      " D loss: 1.391896, G loss: 0.695507, D accuracy: 50.7%, cell accuracy: 21946.7%%, board accuracy: 80.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.390795, G loss: 0.693147, D accuracy: 50.6%, cell accuracy: 21939.4%%, board accuracy: 80.0% \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4087, G loss: 0.7399\n",
      "[124/1778] D loss: 1.3944, G loss: 0.7373\n",
      "[244/1778] D loss: 1.3930, G loss: 0.7286\n",
      "[364/1778] D loss: 1.3799, G loss: 0.7449\n",
      "[484/1778] D loss: 1.4075, G loss: 0.5736\n",
      "[604/1778] D loss: 1.3906, G loss: 0.6499\n",
      "[724/1778] D loss: 1.3878, G loss: 0.6976\n",
      "[844/1778] D loss: 1.4132, G loss: 0.7128\n",
      "[964/1778] D loss: 1.3422, G loss: 0.7091\n",
      "[1084/1778] D loss: 1.3895, G loss: 0.6965\n",
      "[1204/1778] D loss: 1.3760, G loss: 0.6685\n",
      "[1324/1778] D loss: 1.3793, G loss: 0.7553\n",
      "[1444/1778] D loss: 1.3904, G loss: 0.7152\n",
      "[1564/1778] D loss: 1.3469, G loss: 0.6328\n",
      "[1684/1778] D loss: 1.3946, G loss: 0.6399\n",
      "train error: \n",
      " D loss: 1.377771, G loss: 0.717645, D accuracy: 52.8%, cell accuracy: 21946.9%%, board accuracy: 79.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.373117, G loss: 0.719055, D accuracy: 54.1%, cell accuracy: 21940.3%%, board accuracy: 80.2% \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3972, G loss: 0.7635\n",
      "[124/1778] D loss: 1.3264, G loss: 0.7549\n",
      "[244/1778] D loss: 1.3932, G loss: 0.6067\n",
      "[364/1778] D loss: 1.3386, G loss: 0.6534\n",
      "[484/1778] D loss: 1.3852, G loss: 0.7389\n",
      "[604/1778] D loss: 1.3650, G loss: 0.6754\n",
      "[724/1778] D loss: 1.3972, G loss: 0.7001\n",
      "[844/1778] D loss: 1.3852, G loss: 0.6739\n",
      "[964/1778] D loss: 1.4094, G loss: 0.7101\n",
      "[1084/1778] D loss: 1.3595, G loss: 0.7474\n",
      "[1204/1778] D loss: 1.4196, G loss: 0.7045\n",
      "[1324/1778] D loss: 1.3864, G loss: 0.7319\n",
      "[1444/1778] D loss: 1.3956, G loss: 0.7750\n",
      "[1564/1778] D loss: 1.2307, G loss: 0.8559\n",
      "[1684/1778] D loss: 1.2201, G loss: 0.8041\n",
      "train error: \n",
      " D loss: 1.364485, G loss: 0.707334, D accuracy: 53.9%, cell accuracy: 21943.9%%, board accuracy: 77.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.356679, G loss: 0.709569, D accuracy: 55.2%, cell accuracy: 21935.1%%, board accuracy: 76.1% \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1580, G loss: 0.7747\n",
      "[124/1778] D loss: 1.4039, G loss: 0.8107\n",
      "[244/1778] D loss: 1.3138, G loss: 0.7059\n",
      "[364/1778] D loss: 1.3694, G loss: 0.6999\n",
      "[484/1778] D loss: 1.3879, G loss: 0.6801\n",
      "[604/1778] D loss: 1.4038, G loss: 0.6425\n",
      "[724/1778] D loss: 1.3931, G loss: 0.7445\n",
      "[844/1778] D loss: 1.4124, G loss: 0.7594\n",
      "[964/1778] D loss: 1.3888, G loss: 0.6499\n",
      "[1084/1778] D loss: 1.3858, G loss: 0.6934\n",
      "[1204/1778] D loss: 1.3988, G loss: 0.6336\n",
      "[1324/1778] D loss: 1.3999, G loss: 0.7860\n",
      "[1444/1778] D loss: 1.4156, G loss: 0.8312\n",
      "[1564/1778] D loss: 1.4145, G loss: 0.6502\n",
      "[1684/1778] D loss: 1.3472, G loss: 0.7202\n",
      "train error: \n",
      " D loss: 1.411866, G loss: 0.640285, D accuracy: 53.4%, cell accuracy: 21700.2%%, board accuracy: 14.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.397479, G loss: 0.652751, D accuracy: 53.8%, cell accuracy: 21700.7%%, board accuracy: 14.6% \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4438, G loss: 0.6431\n",
      "[124/1778] D loss: 1.4020, G loss: 0.5870\n",
      "[244/1778] D loss: 1.3315, G loss: 0.7309\n",
      "[364/1778] D loss: 1.3314, G loss: 0.7457\n",
      "[484/1778] D loss: 1.3971, G loss: 0.6310\n",
      "[604/1778] D loss: 1.3865, G loss: 0.6239\n",
      "[724/1778] D loss: 1.3897, G loss: 0.6964\n",
      "[844/1778] D loss: 1.4045, G loss: 0.6781\n",
      "[964/1778] D loss: 1.4204, G loss: 0.6802\n",
      "[1084/1778] D loss: 1.3919, G loss: 0.6328\n",
      "[1204/1778] D loss: 1.4143, G loss: 0.7071\n",
      "[1324/1778] D loss: 1.2641, G loss: 0.6838\n",
      "[1444/1778] D loss: 1.3929, G loss: 0.6364\n",
      "[1564/1778] D loss: 1.4078, G loss: 0.6596\n",
      "[1684/1778] D loss: 1.3713, G loss: 0.8202\n",
      "train error: \n",
      " D loss: 1.351537, G loss: 0.752955, D accuracy: 53.7%, cell accuracy: 21945.6%%, board accuracy: 79.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.340847, G loss: 0.758622, D accuracy: 54.7%, cell accuracy: 21933.8%%, board accuracy: 77.5% \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3961, G loss: 0.7593\n",
      "[124/1778] D loss: 1.2441, G loss: 0.6847\n",
      "[244/1778] D loss: 1.1129, G loss: 0.7675\n",
      "[364/1778] D loss: 1.2458, G loss: 0.7488\n",
      "[484/1778] D loss: 1.3976, G loss: 0.7732\n",
      "[604/1778] D loss: 1.3955, G loss: 0.6393\n",
      "[724/1778] D loss: 1.3903, G loss: 0.6998\n",
      "[844/1778] D loss: 1.2330, G loss: 0.8453\n",
      "[964/1778] D loss: 1.4149, G loss: 0.6820\n",
      "[1084/1778] D loss: 1.3531, G loss: 0.7139\n",
      "[1204/1778] D loss: 1.3901, G loss: 0.7407\n",
      "[1324/1778] D loss: 1.5482, G loss: 0.6857\n",
      "[1444/1778] D loss: 1.4204, G loss: 0.4847\n",
      "[1564/1778] D loss: 1.3254, G loss: 0.5748\n",
      "[1684/1778] D loss: 1.4273, G loss: 0.7703\n",
      "train error: \n",
      " D loss: 1.452254, G loss: 0.619039, D accuracy: 47.3%, cell accuracy: 21939.8%%, board accuracy: 72.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.453566, G loss: 0.630562, D accuracy: 47.5%, cell accuracy: 21929.7%%, board accuracy: 70.7% \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4656, G loss: 0.6809\n",
      "[124/1778] D loss: 1.4123, G loss: 0.6390\n",
      "[244/1778] D loss: 1.4227, G loss: 0.7296\n",
      "[364/1778] D loss: 1.3939, G loss: 0.6567\n",
      "[484/1778] D loss: 1.3705, G loss: 0.7465\n",
      "[604/1778] D loss: 1.3936, G loss: 0.7492\n",
      "[724/1778] D loss: 1.3539, G loss: 0.7079\n",
      "[844/1778] D loss: 1.3830, G loss: 0.6730\n",
      "[964/1778] D loss: 1.3873, G loss: 0.6650\n",
      "[1084/1778] D loss: 1.3468, G loss: 0.6818\n",
      "[1204/1778] D loss: 1.3849, G loss: 0.7519\n",
      "[1324/1778] D loss: 1.2553, G loss: 0.8356\n",
      "[1444/1778] D loss: 1.3906, G loss: 0.7417\n",
      "[1564/1778] D loss: 1.3869, G loss: 0.6992\n",
      "[1684/1778] D loss: 1.2875, G loss: 0.7303\n",
      "train error: \n",
      " D loss: 1.364516, G loss: 0.691404, D accuracy: 53.3%, cell accuracy: 21949.3%%, board accuracy: 80.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.357093, G loss: 0.699235, D accuracy: 55.5%, cell accuracy: 21941.4%%, board accuracy: 79.3% \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3985, G loss: 0.6596\n",
      "[124/1778] D loss: 1.3019, G loss: 0.7389\n",
      "[244/1778] D loss: 1.3850, G loss: 0.6324\n",
      "[364/1778] D loss: 1.3988, G loss: 0.6059\n",
      "[484/1778] D loss: 1.3850, G loss: 0.6302\n",
      "[604/1778] D loss: 1.3877, G loss: 0.6705\n",
      "[724/1778] D loss: 1.3964, G loss: 0.7486\n",
      "[844/1778] D loss: 1.2577, G loss: 0.8194\n",
      "[964/1778] D loss: 1.3869, G loss: 0.6721\n",
      "[1084/1778] D loss: 1.2595, G loss: 0.7952\n",
      "[1204/1778] D loss: 1.3981, G loss: 0.7644\n",
      "[1324/1778] D loss: 1.3914, G loss: 0.7360\n",
      "[1444/1778] D loss: 1.1101, G loss: 0.9429\n",
      "[1564/1778] D loss: 1.3928, G loss: 0.6898\n",
      "[1684/1778] D loss: 1.3868, G loss: 0.7083\n",
      "train error: \n",
      " D loss: 1.346812, G loss: 0.720249, D accuracy: 54.1%, cell accuracy: 21947.2%%, board accuracy: 79.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.336060, G loss: 0.728634, D accuracy: 55.4%, cell accuracy: 21939.2%%, board accuracy: 76.8% \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3799, G loss: 0.7408\n",
      "[124/1778] D loss: 1.3907, G loss: 0.7151\n",
      "[244/1778] D loss: 1.3943, G loss: 0.6663\n",
      "[364/1778] D loss: 1.3930, G loss: 0.6929\n",
      "[484/1778] D loss: 1.3910, G loss: 0.6467\n",
      "[604/1778] D loss: 1.3927, G loss: 0.6798\n",
      "[724/1778] D loss: 1.3954, G loss: 0.7392\n",
      "[844/1778] D loss: 1.3807, G loss: 0.7703\n",
      "[964/1778] D loss: 1.4221, G loss: 0.6653\n",
      "[1084/1778] D loss: 1.3878, G loss: 0.6989\n",
      "[1204/1778] D loss: 1.3935, G loss: 0.6518\n",
      "[1324/1778] D loss: 1.1774, G loss: 0.8745\n",
      "[1444/1778] D loss: 1.4006, G loss: 0.6187\n",
      "[1564/1778] D loss: 1.3781, G loss: 0.7766\n",
      "[1684/1778] D loss: 1.6853, G loss: 0.5317\n",
      "train error: \n",
      " D loss: 1.532932, G loss: 0.648281, D accuracy: 39.7%, cell accuracy: 21484.4%%, board accuracy: 9.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.520608, G loss: 0.665728, D accuracy: 40.0%, cell accuracy: 21499.8%%, board accuracy: 9.9% \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.5480, G loss: 0.5706\n",
      "[124/1778] D loss: 1.1460, G loss: 0.8087\n",
      "[244/1778] D loss: 1.1096, G loss: 0.9303\n",
      "[364/1778] D loss: 1.4398, G loss: 0.6713\n",
      "[484/1778] D loss: 1.4046, G loss: 0.7432\n",
      "[604/1778] D loss: 1.4326, G loss: 0.9132\n",
      "[724/1778] D loss: 1.4044, G loss: 0.6532\n",
      "[844/1778] D loss: 1.3911, G loss: 0.7389\n",
      "[964/1778] D loss: 1.4020, G loss: 0.7562\n",
      "[1084/1778] D loss: 1.3973, G loss: 0.5959\n",
      "[1204/1778] D loss: 1.3888, G loss: 0.7457\n",
      "[1324/1778] D loss: 1.3901, G loss: 0.6824\n",
      "[1444/1778] D loss: 1.3876, G loss: 0.6637\n",
      "[1564/1778] D loss: 1.3959, G loss: 0.6812\n",
      "[1684/1778] D loss: 1.3902, G loss: 0.7588\n",
      "train error: \n",
      " D loss: 1.380001, G loss: 0.667373, D accuracy: 53.0%, cell accuracy: 21950.8%%, board accuracy: 82.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.378660, G loss: 0.670864, D accuracy: 53.2%, cell accuracy: 21945.5%%, board accuracy: 81.8% \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3019, G loss: 0.7050\n",
      "[124/1778] D loss: 1.3870, G loss: 0.6942\n",
      "[244/1778] D loss: 1.3858, G loss: 0.7208\n",
      "[364/1778] D loss: 1.3909, G loss: 0.7317\n",
      "[484/1778] D loss: 1.3712, G loss: 0.6884\n",
      "[604/1778] D loss: 1.3307, G loss: 0.7111\n",
      "[724/1778] D loss: 1.3930, G loss: 0.7172\n",
      "[844/1778] D loss: 1.3883, G loss: 0.6680\n",
      "[964/1778] D loss: 1.3887, G loss: 0.7185\n",
      "[1084/1778] D loss: 1.3217, G loss: 0.7137\n",
      "[1204/1778] D loss: 1.3136, G loss: 0.7756\n",
      "[1324/1778] D loss: 1.3937, G loss: 0.7490\n",
      "[1444/1778] D loss: 1.4053, G loss: 0.7197\n",
      "[1564/1778] D loss: 1.3881, G loss: 0.7049\n",
      "[1684/1778] D loss: 1.3771, G loss: 0.7651\n",
      "train error: \n",
      " D loss: 1.366208, G loss: 0.641664, D accuracy: 53.8%, cell accuracy: 21955.7%%, board accuracy: 85.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.359790, G loss: 0.647438, D accuracy: 54.8%, cell accuracy: 21948.4%%, board accuracy: 83.8% \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3008, G loss: 0.6537\n",
      "[124/1778] D loss: 1.2900, G loss: 0.7464\n",
      "[244/1778] D loss: 1.3893, G loss: 0.6708\n",
      "[364/1778] D loss: 1.2986, G loss: 0.7155\n",
      "[484/1778] D loss: 1.3665, G loss: 0.8154\n",
      "[604/1778] D loss: 1.3678, G loss: 0.6795\n",
      "[724/1778] D loss: 1.3074, G loss: 0.6737\n",
      "[844/1778] D loss: 1.3917, G loss: 0.6356\n",
      "[964/1778] D loss: 1.3872, G loss: 0.6877\n",
      "[1084/1778] D loss: 1.3871, G loss: 0.7128\n",
      "[1204/1778] D loss: 1.4088, G loss: 0.8150\n",
      "[1324/1778] D loss: 1.2416, G loss: 0.8060\n",
      "[1444/1778] D loss: 1.3837, G loss: 0.7458\n",
      "[1564/1778] D loss: 1.2298, G loss: 0.8233\n",
      "[1684/1778] D loss: 1.3916, G loss: 0.7713\n",
      "train error: \n",
      " D loss: 1.345224, G loss: 0.739191, D accuracy: 54.0%, cell accuracy: 21956.4%%, board accuracy: 86.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.333829, G loss: 0.744248, D accuracy: 55.4%, cell accuracy: 21946.8%%, board accuracy: 83.3% \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3962, G loss: 0.7657\n",
      "[124/1778] D loss: 1.3886, G loss: 0.7090\n",
      "[244/1778] D loss: 1.3065, G loss: 0.7302\n",
      "[364/1778] D loss: 1.4013, G loss: 0.6360\n",
      "[484/1778] D loss: 1.2121, G loss: 0.8790\n",
      "[604/1778] D loss: 1.3894, G loss: 0.6665\n",
      "[724/1778] D loss: 1.3917, G loss: 0.6721\n",
      "[844/1778] D loss: 1.3973, G loss: 0.7881\n",
      "[964/1778] D loss: 1.1916, G loss: 0.8205\n",
      "[1084/1778] D loss: 0.9777, G loss: 1.0340\n",
      "[1204/1778] D loss: 1.5432, G loss: 0.6129\n",
      "[1324/1778] D loss: 1.5902, G loss: 0.5720\n",
      "[1444/1778] D loss: 1.2977, G loss: 0.8137\n",
      "[1564/1778] D loss: 0.8290, G loss: 1.0906\n",
      "[1684/1778] D loss: 1.2924, G loss: 0.8367\n",
      "train error: \n",
      " D loss: 1.481729, G loss: 0.848111, D accuracy: 48.2%, cell accuracy: 21955.1%%, board accuracy: 85.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.492677, G loss: 0.852458, D accuracy: 48.0%, cell accuracy: 21945.9%%, board accuracy: 82.9% \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.5876, G loss: 0.7470\n",
      "[124/1778] D loss: 1.4148, G loss: 0.6659\n",
      "[244/1778] D loss: 1.4446, G loss: 0.9453\n",
      "[364/1778] D loss: 1.3842, G loss: 0.7492\n",
      "[484/1778] D loss: 1.3874, G loss: 0.7792\n",
      "[604/1778] D loss: 1.3795, G loss: 0.7010\n",
      "[724/1778] D loss: 1.3461, G loss: 0.7230\n",
      "[844/1778] D loss: 1.3885, G loss: 0.7207\n",
      "[964/1778] D loss: 1.3878, G loss: 0.6742\n",
      "[1084/1778] D loss: 1.3941, G loss: 0.7172\n",
      "[1204/1778] D loss: 1.3921, G loss: 0.7556\n",
      "[1324/1778] D loss: 1.3248, G loss: 0.7218\n",
      "[1444/1778] D loss: 1.3799, G loss: 0.7123\n",
      "[1564/1778] D loss: 1.3882, G loss: 0.7439\n",
      "[1684/1778] D loss: 1.3877, G loss: 0.6710\n",
      "train error: \n",
      " D loss: 1.359795, G loss: 0.705219, D accuracy: 54.0%, cell accuracy: 21958.5%%, board accuracy: 87.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.353271, G loss: 0.713782, D accuracy: 54.7%, cell accuracy: 21947.3%%, board accuracy: 84.0% \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3908, G loss: 0.6694\n",
      "[124/1778] D loss: 1.3824, G loss: 0.6725\n",
      "[244/1778] D loss: 1.3867, G loss: 0.6863\n",
      "[364/1778] D loss: 1.3924, G loss: 0.7284\n",
      "[484/1778] D loss: 1.2654, G loss: 0.8113\n",
      "[604/1778] D loss: 1.3835, G loss: 0.6741\n",
      "[724/1778] D loss: 1.7161, G loss: 0.5173\n",
      "[844/1778] D loss: 1.3961, G loss: 0.6859\n",
      "[964/1778] D loss: 1.3252, G loss: 0.6629\n",
      "[1084/1778] D loss: 1.2738, G loss: 0.8453\n",
      "[1204/1778] D loss: 1.3978, G loss: 0.6599\n",
      "[1324/1778] D loss: 1.3909, G loss: 0.7427\n",
      "[1444/1778] D loss: 1.3595, G loss: 0.7517\n",
      "[1564/1778] D loss: 1.3962, G loss: 0.6473\n",
      "[1684/1778] D loss: 1.3799, G loss: 0.6713\n",
      "train error: \n",
      " D loss: 1.374048, G loss: 0.696911, D accuracy: 53.5%, cell accuracy: 21959.7%%, board accuracy: 88.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.369523, G loss: 0.703874, D accuracy: 54.8%, cell accuracy: 21949.8%%, board accuracy: 85.6% \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3852, G loss: 0.6956\n",
      "[124/1778] D loss: 1.3923, G loss: 0.6493\n",
      "[244/1778] D loss: 1.3916, G loss: 0.6928\n",
      "[364/1778] D loss: 1.3872, G loss: 0.7210\n",
      "[484/1778] D loss: 1.3864, G loss: 0.6958\n",
      "[604/1778] D loss: 1.3973, G loss: 0.7529\n",
      "[724/1778] D loss: 1.3922, G loss: 0.6693\n",
      "[844/1778] D loss: 1.3846, G loss: 0.7043\n",
      "[964/1778] D loss: 1.3904, G loss: 0.7499\n",
      "[1084/1778] D loss: 1.3890, G loss: 0.6643\n",
      "[1204/1778] D loss: 1.3994, G loss: 0.7750\n",
      "[1324/1778] D loss: 0.9818, G loss: 0.8844\n",
      "[1444/1778] D loss: 1.7041, G loss: 0.5101\n",
      "[1564/1778] D loss: 1.4156, G loss: 0.6458\n",
      "[1684/1778] D loss: 1.3511, G loss: 0.8145\n",
      "train error: \n",
      " D loss: 1.345158, G loss: 0.699205, D accuracy: 58.3%, cell accuracy: 21863.2%%, board accuracy: 67.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.352212, G loss: 0.705157, D accuracy: 57.4%, cell accuracy: 21856.5%%, board accuracy: 68.5% \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3328, G loss: 0.7127\n",
      "[124/1778] D loss: 1.4768, G loss: 0.6220\n",
      "[244/1778] D loss: 1.5076, G loss: 0.7966\n",
      "[364/1778] D loss: 1.3938, G loss: 0.6883\n",
      "[484/1778] D loss: 1.3897, G loss: 0.7522\n",
      "[604/1778] D loss: 1.3899, G loss: 0.7512\n",
      "[724/1778] D loss: 1.3923, G loss: 0.6799\n",
      "[844/1778] D loss: 1.3897, G loss: 0.6342\n",
      "[964/1778] D loss: 1.3929, G loss: 0.7486\n",
      "[1084/1778] D loss: 1.3839, G loss: 0.7369\n",
      "[1204/1778] D loss: 1.2922, G loss: 0.7824\n",
      "[1324/1778] D loss: 1.3890, G loss: 0.7341\n",
      "[1444/1778] D loss: 1.3633, G loss: 0.6832\n",
      "[1564/1778] D loss: 1.2786, G loss: 0.8213\n",
      "[1684/1778] D loss: 1.3891, G loss: 0.6821\n",
      "train error: \n",
      " D loss: 1.376797, G loss: 0.778615, D accuracy: 51.9%, cell accuracy: 21826.4%%, board accuracy: 61.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.379628, G loss: 0.782745, D accuracy: 52.3%, cell accuracy: 21813.1%%, board accuracy: 57.4% \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3783, G loss: 0.7566\n",
      "[124/1778] D loss: 1.3578, G loss: 0.6647\n",
      "[244/1778] D loss: 1.4606, G loss: 0.6773\n",
      "[364/1778] D loss: 1.3794, G loss: 0.6357\n",
      "[484/1778] D loss: 1.3917, G loss: 0.6578\n",
      "[604/1778] D loss: 1.3916, G loss: 0.6904\n",
      "[724/1778] D loss: 1.3918, G loss: 0.7223\n",
      "[844/1778] D loss: 1.3820, G loss: 0.7268\n",
      "[964/1778] D loss: 1.3885, G loss: 0.7214\n",
      "[1084/1778] D loss: 1.3564, G loss: 0.7233\n",
      "[1204/1778] D loss: 1.3882, G loss: 0.7078\n",
      "[1324/1778] D loss: 1.2904, G loss: 0.7476\n",
      "[1444/1778] D loss: 1.3793, G loss: 0.6539\n",
      "[1564/1778] D loss: 1.3375, G loss: 0.7001\n",
      "[1684/1778] D loss: 1.3884, G loss: 0.6479\n",
      "train error: \n",
      " D loss: 1.375749, G loss: 0.712701, D accuracy: 53.2%, cell accuracy: 21918.5%%, board accuracy: 75.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.372456, G loss: 0.718440, D accuracy: 53.5%, cell accuracy: 21888.3%%, board accuracy: 68.2% \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3830, G loss: 0.6943\n",
      "[124/1778] D loss: 1.3697, G loss: 0.6956\n",
      "[244/1778] D loss: 1.3988, G loss: 0.5982\n",
      "[364/1778] D loss: 1.3148, G loss: 0.6135\n",
      "[484/1778] D loss: 1.3937, G loss: 0.7244\n",
      "[604/1778] D loss: 1.3843, G loss: 0.7445\n",
      "[724/1778] D loss: 1.3551, G loss: 0.7090\n",
      "[844/1778] D loss: 1.3016, G loss: 0.7546\n",
      "[964/1778] D loss: 1.3859, G loss: 0.7115\n",
      "[1084/1778] D loss: 1.3881, G loss: 0.6520\n",
      "[1204/1778] D loss: 1.4463, G loss: 0.6631\n",
      "[1324/1778] D loss: 1.3865, G loss: 0.7166\n",
      "[1444/1778] D loss: 1.3163, G loss: 0.7009\n",
      "[1564/1778] D loss: 1.3888, G loss: 0.6606\n",
      "[1684/1778] D loss: 1.3708, G loss: 0.7193\n",
      "train error: \n",
      " D loss: 1.377217, G loss: 0.659456, D accuracy: 52.8%, cell accuracy: 21935.2%%, board accuracy: 82.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.373891, G loss: 0.665192, D accuracy: 53.0%, cell accuracy: 21930.2%%, board accuracy: 80.9% \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3931, G loss: 0.6391\n",
      "[124/1778] D loss: 1.3377, G loss: 0.7698\n",
      "[244/1778] D loss: 1.3508, G loss: 0.7365\n",
      "[364/1778] D loss: 1.3658, G loss: 0.6275\n",
      "[484/1778] D loss: 1.3932, G loss: 0.6279\n",
      "[604/1778] D loss: 1.3427, G loss: 0.7525\n",
      "[724/1778] D loss: 1.3516, G loss: 0.7558\n",
      "[844/1778] D loss: 1.4116, G loss: 0.6212\n",
      "[964/1778] D loss: 1.3867, G loss: 0.7058\n",
      "[1084/1778] D loss: 1.3394, G loss: 0.7609\n",
      "[1204/1778] D loss: 1.3901, G loss: 0.7069\n",
      "[1324/1778] D loss: 1.3860, G loss: 0.6819\n",
      "[1444/1778] D loss: 1.3918, G loss: 0.6438\n",
      "[1564/1778] D loss: 1.3890, G loss: 0.6985\n",
      "[1684/1778] D loss: 1.3931, G loss: 0.7169\n",
      "train error: \n",
      " D loss: 1.363139, G loss: 0.743389, D accuracy: 53.0%, cell accuracy: 21956.6%%, board accuracy: 86.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.355684, G loss: 0.750725, D accuracy: 53.5%, cell accuracy: 21944.8%%, board accuracy: 82.4% \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3907, G loss: 0.7204\n",
      "[124/1778] D loss: 1.3946, G loss: 0.7037\n",
      "[244/1778] D loss: 1.3906, G loss: 0.6502\n",
      "[364/1778] D loss: 1.3540, G loss: 0.8043\n",
      "[484/1778] D loss: 1.3772, G loss: 0.6811\n",
      "[604/1778] D loss: 1.4294, G loss: 0.8395\n",
      "[724/1778] D loss: 1.3914, G loss: 0.6879\n",
      "[844/1778] D loss: 1.5578, G loss: 0.6020\n",
      "[964/1778] D loss: 1.3891, G loss: 0.7435\n",
      "[1084/1778] D loss: 1.3294, G loss: 0.8195\n",
      "[1204/1778] D loss: 1.3898, G loss: 0.7122\n",
      "[1324/1778] D loss: 1.3905, G loss: 0.7251\n",
      "[1444/1778] D loss: 1.3627, G loss: 0.6898\n",
      "[1564/1778] D loss: 1.2422, G loss: 0.7431\n",
      "[1684/1778] D loss: 1.3459, G loss: 0.7511\n",
      "train error: \n",
      " D loss: 1.371366, G loss: 0.679538, D accuracy: 54.3%, cell accuracy: 21945.4%%, board accuracy: 84.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.367258, G loss: 0.686241, D accuracy: 55.0%, cell accuracy: 21940.1%%, board accuracy: 81.5% \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3886, G loss: 0.6542\n",
      "[124/1778] D loss: 1.4106, G loss: 0.7324\n",
      "[244/1778] D loss: 1.4000, G loss: 0.5957\n",
      "[364/1778] D loss: 1.3907, G loss: 0.7388\n",
      "[484/1778] D loss: 1.3591, G loss: 0.7101\n",
      "[604/1778] D loss: 1.3862, G loss: 0.7025\n",
      "[724/1778] D loss: 1.3876, G loss: 0.6743\n",
      "[844/1778] D loss: 1.2725, G loss: 0.7594\n",
      "[964/1778] D loss: 1.4126, G loss: 0.6763\n",
      "[1084/1778] D loss: 1.3460, G loss: 0.6771\n",
      "[1204/1778] D loss: 1.3951, G loss: 0.7447\n",
      "[1324/1778] D loss: 1.2993, G loss: 0.7289\n",
      "[1444/1778] D loss: 1.2140, G loss: 0.8146\n",
      "[1564/1778] D loss: 1.3917, G loss: 0.7408\n",
      "[1684/1778] D loss: 1.3822, G loss: 0.6651\n",
      "train error: \n",
      " D loss: 1.371906, G loss: 0.752556, D accuracy: 52.6%, cell accuracy: 21945.1%%, board accuracy: 82.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.364123, G loss: 0.760652, D accuracy: 53.6%, cell accuracy: 21937.8%%, board accuracy: 80.2% \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3919, G loss: 0.7332\n",
      "[124/1778] D loss: 1.3884, G loss: 0.6705\n",
      "[244/1778] D loss: 1.3974, G loss: 0.6588\n",
      "[364/1778] D loss: 1.3412, G loss: 0.6525\n",
      "[484/1778] D loss: 1.3878, G loss: 0.6658\n",
      "[604/1778] D loss: 1.2174, G loss: 0.7104\n",
      "[724/1778] D loss: 1.2860, G loss: 0.8431\n",
      "[844/1778] D loss: 1.2329, G loss: 0.8231\n",
      "[964/1778] D loss: 1.3949, G loss: 0.7064\n",
      "[1084/1778] D loss: 1.3868, G loss: 0.6968\n",
      "[1204/1778] D loss: 1.3996, G loss: 0.7905\n",
      "[1324/1778] D loss: 1.3154, G loss: 0.7519\n",
      "[1444/1778] D loss: 1.3795, G loss: 0.6996\n",
      "[1564/1778] D loss: 1.3682, G loss: 0.7021\n",
      "[1684/1778] D loss: 1.3352, G loss: 0.7093\n",
      "train error: \n",
      " D loss: 1.364517, G loss: 0.705305, D accuracy: 55.0%, cell accuracy: 21929.3%%, board accuracy: 79.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.360317, G loss: 0.710363, D accuracy: 55.4%, cell accuracy: 21919.6%%, board accuracy: 77.5% \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3867, G loss: 0.6719\n",
      "[124/1778] D loss: 1.3667, G loss: 0.7259\n",
      "[244/1778] D loss: 1.3841, G loss: 0.6964\n",
      "[364/1778] D loss: 1.3884, G loss: 0.6661\n",
      "[484/1778] D loss: 1.4164, G loss: 0.7118\n",
      "[604/1778] D loss: 1.3881, G loss: 0.6904\n",
      "[724/1778] D loss: 1.1913, G loss: 0.7592\n",
      "[844/1778] D loss: 1.4744, G loss: 0.6160\n",
      "[964/1778] D loss: 1.3875, G loss: 0.7162\n",
      "[1084/1778] D loss: 1.2726, G loss: 0.7698\n",
      "[1204/1778] D loss: 1.3860, G loss: 0.7587\n",
      "[1324/1778] D loss: 1.3652, G loss: 0.7124\n",
      "[1444/1778] D loss: 1.3925, G loss: 0.7244\n",
      "[1564/1778] D loss: 1.2608, G loss: 0.7404\n",
      "[1684/1778] D loss: 1.3817, G loss: 0.7591\n",
      "train error: \n",
      " D loss: 1.372633, G loss: 0.722727, D accuracy: 53.4%, cell accuracy: 21919.6%%, board accuracy: 75.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.360876, G loss: 0.738819, D accuracy: 53.5%, cell accuracy: 21909.7%%, board accuracy: 73.2% \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3893, G loss: 0.7435\n",
      "[124/1778] D loss: 1.3758, G loss: 0.7768\n",
      "[244/1778] D loss: 1.3584, G loss: 0.6781\n",
      "[364/1778] D loss: 1.3274, G loss: 0.7444\n",
      "[484/1778] D loss: 1.3880, G loss: 0.7454\n",
      "[604/1778] D loss: 1.3394, G loss: 0.7751\n",
      "[724/1778] D loss: 1.3868, G loss: 0.7365\n",
      "[844/1778] D loss: 1.3751, G loss: 0.7126\n",
      "[964/1778] D loss: 1.1289, G loss: 0.8030\n",
      "[1084/1778] D loss: 1.3809, G loss: 0.7966\n",
      "[1204/1778] D loss: 1.3904, G loss: 0.7563\n",
      "[1324/1778] D loss: 1.1275, G loss: 0.8763\n",
      "[1444/1778] D loss: 1.3936, G loss: 0.7783\n",
      "[1564/1778] D loss: 1.3948, G loss: 0.6991\n",
      "[1684/1778] D loss: 1.2572, G loss: 0.9496\n",
      "train error: \n",
      " D loss: 1.362388, G loss: 0.717991, D accuracy: 53.8%, cell accuracy: 21923.1%%, board accuracy: 79.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.355500, G loss: 0.729704, D accuracy: 54.1%, cell accuracy: 21915.3%%, board accuracy: 77.3% \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4008, G loss: 0.6861\n",
      "[124/1778] D loss: 1.3888, G loss: 0.7000\n",
      "[244/1778] D loss: 1.4271, G loss: 0.6553\n",
      "[364/1778] D loss: 1.3898, G loss: 0.7389\n",
      "[484/1778] D loss: 1.2262, G loss: 0.7838\n",
      "[604/1778] D loss: 1.3546, G loss: 0.7689\n",
      "[724/1778] D loss: 1.4107, G loss: 0.8255\n",
      "[844/1778] D loss: 1.3900, G loss: 0.6872\n",
      "[964/1778] D loss: 0.9863, G loss: 0.9066\n",
      "[1084/1778] D loss: 1.3926, G loss: 0.7084\n",
      "[1204/1778] D loss: 1.0704, G loss: 0.7859\n",
      "[1324/1778] D loss: 1.3894, G loss: 0.6401\n",
      "[1444/1778] D loss: 1.2186, G loss: 0.8501\n",
      "[1564/1778] D loss: 1.3891, G loss: 0.7130\n",
      "[1684/1778] D loss: 1.2282, G loss: 0.7991\n",
      "train error: \n",
      " D loss: 1.366199, G loss: 0.720727, D accuracy: 51.5%, cell accuracy: 21922.9%%, board accuracy: 78.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.356018, G loss: 0.733692, D accuracy: 52.7%, cell accuracy: 21924.8%%, board accuracy: 78.4% \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3693, G loss: 0.7237\n",
      "[124/1778] D loss: 1.4023, G loss: 0.7184\n",
      "[244/1778] D loss: 1.1928, G loss: 0.8311\n",
      "[364/1778] D loss: 1.3934, G loss: 0.7175\n",
      "[484/1778] D loss: 1.3107, G loss: 0.7979\n",
      "[604/1778] D loss: 1.4094, G loss: 0.6541\n",
      "[724/1778] D loss: 1.3890, G loss: 0.7543\n",
      "[844/1778] D loss: 1.3913, G loss: 0.7288\n",
      "[964/1778] D loss: 1.3896, G loss: 0.7181\n",
      "[1084/1778] D loss: 1.4007, G loss: 0.7702\n",
      "[1204/1778] D loss: 1.3903, G loss: 0.6339\n",
      "[1324/1778] D loss: 1.2976, G loss: 0.7630\n",
      "[1444/1778] D loss: 1.3877, G loss: 0.6881\n",
      "[1564/1778] D loss: 1.4315, G loss: 0.6675\n",
      "[1684/1778] D loss: 1.3986, G loss: 0.6674\n",
      "train error: \n",
      " D loss: 1.362503, G loss: 0.640190, D accuracy: 55.0%, cell accuracy: 21916.0%%, board accuracy: 77.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.349690, G loss: 0.657779, D accuracy: 56.5%, cell accuracy: 21896.8%%, board accuracy: 72.3% \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3850, G loss: 0.6612\n",
      "[124/1778] D loss: 1.3753, G loss: 0.6605\n",
      "[244/1778] D loss: 1.3957, G loss: 0.7141\n",
      "[364/1778] D loss: 1.3380, G loss: 0.7006\n",
      "[484/1778] D loss: 1.4160, G loss: 0.7085\n",
      "[604/1778] D loss: 1.3904, G loss: 0.7046\n",
      "[724/1778] D loss: 1.3850, G loss: 0.6636\n",
      "[844/1778] D loss: 1.1962, G loss: 0.8946\n",
      "[964/1778] D loss: 1.3616, G loss: 0.7360\n",
      "[1084/1778] D loss: 1.2978, G loss: 0.7156\n",
      "[1204/1778] D loss: 1.2591, G loss: 0.8360\n",
      "[1324/1778] D loss: 1.2310, G loss: 0.8527\n",
      "[1444/1778] D loss: 1.3985, G loss: 0.7191\n",
      "[1564/1778] D loss: 1.3277, G loss: 0.7251\n",
      "[1684/1778] D loss: 1.3937, G loss: 0.7026\n",
      "train error: \n",
      " D loss: 1.352242, G loss: 0.736282, D accuracy: 55.6%, cell accuracy: 21922.3%%, board accuracy: 79.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.344457, G loss: 0.750675, D accuracy: 55.4%, cell accuracy: 21911.9%%, board accuracy: 77.5% \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3511, G loss: 0.7873\n",
      "[124/1778] D loss: 1.3951, G loss: 0.6514\n",
      "[244/1778] D loss: 1.2823, G loss: 0.7719\n",
      "[364/1778] D loss: 1.3927, G loss: 0.6181\n",
      "[484/1778] D loss: 1.3036, G loss: 0.7597\n",
      "[604/1778] D loss: 1.3904, G loss: 0.7307\n",
      "[724/1778] D loss: 1.1660, G loss: 0.9176\n",
      "[844/1778] D loss: 1.3665, G loss: 0.7199\n",
      "[964/1778] D loss: 1.3052, G loss: 0.7175\n",
      "[1084/1778] D loss: 1.4073, G loss: 0.6824\n",
      "[1204/1778] D loss: 1.1506, G loss: 0.8102\n",
      "[1324/1778] D loss: 1.3924, G loss: 0.7506\n",
      "[1444/1778] D loss: 1.3816, G loss: 0.7202\n",
      "[1564/1778] D loss: 1.6386, G loss: 0.6159\n",
      "[1684/1778] D loss: 1.4129, G loss: 0.7435\n",
      "train error: \n",
      " D loss: 1.338919, G loss: 0.793998, D accuracy: 53.3%, cell accuracy: 21958.3%%, board accuracy: 87.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.322136, G loss: 0.815448, D accuracy: 54.4%, cell accuracy: 21951.1%%, board accuracy: 85.6% \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3746, G loss: 0.8152\n",
      "[124/1778] D loss: 1.5427, G loss: 0.6807\n",
      "[244/1778] D loss: 1.3873, G loss: 0.7187\n",
      "[364/1778] D loss: 1.1361, G loss: 0.9880\n",
      "[484/1778] D loss: 1.3916, G loss: 0.7596\n",
      "[604/1778] D loss: 1.3926, G loss: 0.7587\n",
      "[724/1778] D loss: 1.2842, G loss: 0.7943\n",
      "[844/1778] D loss: 1.3893, G loss: 0.6676\n",
      "[964/1778] D loss: 1.1836, G loss: 0.7834\n",
      "[1084/1778] D loss: 1.1438, G loss: 0.8231\n",
      "[1204/1778] D loss: 1.1748, G loss: 0.8498\n",
      "[1324/1778] D loss: 1.3928, G loss: 0.6421\n",
      "[1444/1778] D loss: 1.3257, G loss: 0.7007\n",
      "[1564/1778] D loss: 1.3909, G loss: 0.6435\n",
      "[1684/1778] D loss: 1.2811, G loss: 0.8066\n",
      "train error: \n",
      " D loss: 1.344285, G loss: 0.727966, D accuracy: 53.7%, cell accuracy: 21953.1%%, board accuracy: 85.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.336258, G loss: 0.744031, D accuracy: 54.3%, cell accuracy: 21942.6%%, board accuracy: 83.1% \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3883, G loss: 0.6872\n",
      "[124/1778] D loss: 1.2693, G loss: 0.8029\n",
      "[244/1778] D loss: 1.3881, G loss: 0.7048\n",
      "[364/1778] D loss: 1.2484, G loss: 0.7737\n",
      "[484/1778] D loss: 0.9854, G loss: 0.9787\n",
      "[604/1778] D loss: 1.2272, G loss: 0.7838\n",
      "[724/1778] D loss: 1.3834, G loss: 0.7242\n",
      "[844/1778] D loss: 1.3903, G loss: 0.7131\n",
      "[964/1778] D loss: 1.3941, G loss: 0.6139\n",
      "[1084/1778] D loss: 1.4453, G loss: 0.6347\n",
      "[1204/1778] D loss: 1.3974, G loss: 0.7018\n",
      "[1324/1778] D loss: 1.4171, G loss: 0.8061\n",
      "[1444/1778] D loss: 1.3772, G loss: 0.7818\n",
      "[1564/1778] D loss: 1.3870, G loss: 0.6762\n",
      "[1684/1778] D loss: 1.3005, G loss: 0.7089\n",
      "train error: \n",
      " D loss: 1.349894, G loss: 0.778078, D accuracy: 53.7%, cell accuracy: 21927.7%%, board accuracy: 79.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.344955, G loss: 0.784029, D accuracy: 53.9%, cell accuracy: 21918.5%%, board accuracy: 78.4% \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1850, G loss: 0.8378\n",
      "[124/1778] D loss: 1.3817, G loss: 0.7678\n",
      "[244/1778] D loss: 1.1299, G loss: 0.8504\n",
      "[364/1778] D loss: 1.3924, G loss: 0.6420\n",
      "[484/1778] D loss: 1.3999, G loss: 0.8145\n",
      "[604/1778] D loss: 1.3614, G loss: 0.8259\n",
      "[724/1778] D loss: 1.0828, G loss: 0.8971\n",
      "[844/1778] D loss: 1.4097, G loss: 0.8238\n",
      "[964/1778] D loss: 1.3967, G loss: 0.7263\n",
      "[1084/1778] D loss: 1.3736, G loss: 0.7155\n",
      "[1204/1778] D loss: 1.2571, G loss: 0.7635\n",
      "[1324/1778] D loss: 1.3292, G loss: 0.7804\n",
      "[1444/1778] D loss: 0.8647, G loss: 1.2686\n",
      "[1564/1778] D loss: 1.3177, G loss: 0.7048\n",
      "[1684/1778] D loss: 1.2997, G loss: 0.8118\n",
      "train error: \n",
      " D loss: 1.353171, G loss: 0.743210, D accuracy: 54.2%, cell accuracy: 21944.9%%, board accuracy: 83.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.347507, G loss: 0.754095, D accuracy: 54.5%, cell accuracy: 21935.6%%, board accuracy: 81.1% \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3239, G loss: 0.7445\n",
      "[124/1778] D loss: 1.3867, G loss: 0.6751\n",
      "[244/1778] D loss: 1.2813, G loss: 0.7560\n",
      "[364/1778] D loss: 1.2631, G loss: 0.7605\n",
      "[484/1778] D loss: 1.4247, G loss: 0.7215\n",
      "[604/1778] D loss: 1.3991, G loss: 0.8135\n",
      "[724/1778] D loss: 1.3870, G loss: 0.6891\n",
      "[844/1778] D loss: 1.3681, G loss: 0.6861\n",
      "[964/1778] D loss: 1.3636, G loss: 0.8584\n",
      "[1084/1778] D loss: 1.3965, G loss: 0.7797\n",
      "[1204/1778] D loss: 1.3899, G loss: 0.6891\n",
      "[1324/1778] D loss: 1.3888, G loss: 0.6941\n",
      "[1444/1778] D loss: 1.3602, G loss: 0.7920\n",
      "[1564/1778] D loss: 1.3907, G loss: 0.6424\n",
      "[1684/1778] D loss: 1.2564, G loss: 0.7654\n",
      "train error: \n",
      " D loss: 1.353210, G loss: 0.734377, D accuracy: 54.2%, cell accuracy: 21939.5%%, board accuracy: 81.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.341571, G loss: 0.751108, D accuracy: 54.7%, cell accuracy: 21920.0%%, board accuracy: 77.3% \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3278, G loss: 0.7758\n",
      "[124/1778] D loss: 1.3942, G loss: 0.7515\n",
      "[244/1778] D loss: 1.3212, G loss: 0.7589\n",
      "[364/1778] D loss: 1.3870, G loss: 0.6625\n",
      "[484/1778] D loss: 1.4185, G loss: 0.6559\n",
      "[604/1778] D loss: 1.4039, G loss: 0.7724\n",
      "[724/1778] D loss: 1.4123, G loss: 0.6392\n",
      "[844/1778] D loss: 1.0413, G loss: 1.0265\n",
      "[964/1778] D loss: 1.3213, G loss: 0.8322\n",
      "[1084/1778] D loss: 1.3968, G loss: 0.6915\n",
      "[1204/1778] D loss: 1.2335, G loss: 0.7443\n",
      "[1324/1778] D loss: 1.3930, G loss: 0.7209\n",
      "[1444/1778] D loss: 1.3184, G loss: 0.7842\n",
      "[1564/1778] D loss: 1.4248, G loss: 0.6334\n",
      "[1684/1778] D loss: 1.3911, G loss: 0.6820\n",
      "train error: \n",
      " D loss: 1.346204, G loss: 0.718285, D accuracy: 55.2%, cell accuracy: 21946.9%%, board accuracy: 82.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.338597, G loss: 0.724103, D accuracy: 56.1%, cell accuracy: 21939.2%%, board accuracy: 81.3% \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2053, G loss: 0.8181\n",
      "[124/1778] D loss: 1.3201, G loss: 0.7951\n",
      "[244/1778] D loss: 1.3875, G loss: 0.7526\n",
      "[364/1778] D loss: 1.3999, G loss: 0.6205\n",
      "[484/1778] D loss: 1.3904, G loss: 0.6480\n",
      "[604/1778] D loss: 1.3996, G loss: 0.7863\n",
      "[724/1778] D loss: 1.2511, G loss: 0.6871\n",
      "[844/1778] D loss: 1.3978, G loss: 0.7529\n",
      "[964/1778] D loss: 1.4215, G loss: 0.8781\n",
      "[1084/1778] D loss: 1.4321, G loss: 0.8704\n",
      "[1204/1778] D loss: 1.3893, G loss: 0.7115\n",
      "[1324/1778] D loss: 1.3908, G loss: 0.6976\n",
      "[1444/1778] D loss: 1.4094, G loss: 0.6329\n",
      "[1564/1778] D loss: 1.2591, G loss: 0.8619\n",
      "[1684/1778] D loss: 1.3947, G loss: 0.7068\n",
      "train error: \n",
      " D loss: 1.332412, G loss: 0.803086, D accuracy: 54.1%, cell accuracy: 21947.2%%, board accuracy: 82.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.321840, G loss: 0.814426, D accuracy: 55.1%, cell accuracy: 21935.4%%, board accuracy: 78.4% \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2311, G loss: 0.8009\n",
      "[124/1778] D loss: 1.4012, G loss: 0.8066\n",
      "[244/1778] D loss: 1.1942, G loss: 0.7590\n",
      "[364/1778] D loss: 1.4008, G loss: 0.8155\n",
      "[484/1778] D loss: 1.3124, G loss: 0.8725\n",
      "[604/1778] D loss: 1.2771, G loss: 0.7715\n",
      "[724/1778] D loss: 1.3673, G loss: 0.6380\n",
      "[844/1778] D loss: 1.4064, G loss: 0.8051\n",
      "[964/1778] D loss: 1.2018, G loss: 0.8185\n",
      "[1084/1778] D loss: 1.2780, G loss: 0.8910\n",
      "[1204/1778] D loss: 1.3879, G loss: 0.7133\n",
      "[1324/1778] D loss: 1.3961, G loss: 0.7134\n",
      "[1444/1778] D loss: 1.3967, G loss: 0.6874\n",
      "[1564/1778] D loss: 1.3848, G loss: 0.6999\n",
      "[1684/1778] D loss: 1.3455, G loss: 0.7887\n",
      "train error: \n",
      " D loss: 1.328234, G loss: 0.699019, D accuracy: 55.8%, cell accuracy: 21939.1%%, board accuracy: 75.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.311077, G loss: 0.721105, D accuracy: 56.5%, cell accuracy: 21931.1%%, board accuracy: 74.3% \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3936, G loss: 0.6665\n",
      "[124/1778] D loss: 1.3174, G loss: 0.7922\n",
      "[244/1778] D loss: 1.4856, G loss: 0.6543\n",
      "[364/1778] D loss: 1.4484, G loss: 0.6746\n",
      "[484/1778] D loss: 1.3906, G loss: 0.6691\n",
      "[604/1778] D loss: 1.3297, G loss: 0.6343\n",
      "[724/1778] D loss: 1.5652, G loss: 0.7269\n",
      "[844/1778] D loss: 1.3942, G loss: 0.6852\n",
      "[964/1778] D loss: 1.3747, G loss: 0.7644\n",
      "[1084/1778] D loss: 1.5794, G loss: 0.7074\n",
      "[1204/1778] D loss: 1.4712, G loss: 0.7406\n",
      "[1324/1778] D loss: 1.3948, G loss: 0.7280\n",
      "[1444/1778] D loss: 1.3721, G loss: 0.6769\n",
      "[1564/1778] D loss: 1.3918, G loss: 0.7279\n",
      "[1684/1778] D loss: 1.3607, G loss: 0.6423\n",
      "train error: \n",
      " D loss: 1.334671, G loss: 0.786593, D accuracy: 58.5%, cell accuracy: 21878.1%%, board accuracy: 68.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.325047, G loss: 0.810592, D accuracy: 59.6%, cell accuracy: 21859.5%%, board accuracy: 65.3% \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2047, G loss: 0.9410\n",
      "[124/1778] D loss: 1.2736, G loss: 0.7743\n",
      "[244/1778] D loss: 1.3502, G loss: 0.7279\n",
      "[364/1778] D loss: 1.6680, G loss: 0.5781\n",
      "[484/1778] D loss: 1.3032, G loss: 0.6984\n",
      "[604/1778] D loss: 1.4099, G loss: 0.6197\n",
      "[724/1778] D loss: 1.3909, G loss: 0.6825\n",
      "[844/1778] D loss: 1.3007, G loss: 0.7206\n",
      "[964/1778] D loss: 1.2075, G loss: 0.9621\n",
      "[1084/1778] D loss: 1.2992, G loss: 0.7531\n",
      "[1204/1778] D loss: 1.2722, G loss: 0.8564\n",
      "[1324/1778] D loss: 1.3728, G loss: 0.7988\n",
      "[1444/1778] D loss: 1.1741, G loss: 0.8422\n",
      "[1564/1778] D loss: 1.3947, G loss: 0.6582\n",
      "[1684/1778] D loss: 1.4065, G loss: 0.6914\n",
      "train error: \n",
      " D loss: 1.357421, G loss: 0.755836, D accuracy: 52.6%, cell accuracy: 21947.6%%, board accuracy: 84.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.344037, G loss: 0.780449, D accuracy: 52.4%, cell accuracy: 21930.0%%, board accuracy: 80.4% \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3949, G loss: 0.7301\n",
      "[124/1778] D loss: 1.3974, G loss: 0.7748\n",
      "[244/1778] D loss: 1.2189, G loss: 0.7946\n",
      "[364/1778] D loss: 1.3249, G loss: 0.6507\n",
      "[484/1778] D loss: 1.2453, G loss: 0.9315\n",
      "[604/1778] D loss: 1.3878, G loss: 0.6952\n",
      "[724/1778] D loss: 1.4150, G loss: 0.7201\n",
      "[844/1778] D loss: 1.3283, G loss: 0.7741\n",
      "[964/1778] D loss: 1.3898, G loss: 0.7224\n",
      "[1084/1778] D loss: 1.3878, G loss: 0.6991\n",
      "[1204/1778] D loss: 1.3871, G loss: 0.7229\n",
      "[1324/1778] D loss: 1.3965, G loss: 0.7139\n",
      "[1444/1778] D loss: 1.3676, G loss: 0.7637\n",
      "[1564/1778] D loss: 1.3732, G loss: 0.7183\n",
      "[1684/1778] D loss: 1.2509, G loss: 0.7488\n",
      "train error: \n",
      " D loss: 1.348416, G loss: 0.670489, D accuracy: 55.5%, cell accuracy: 21930.9%%, board accuracy: 80.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.348523, G loss: 0.670671, D accuracy: 55.5%, cell accuracy: 21921.8%%, board accuracy: 79.3% \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3955, G loss: 0.7095\n",
      "[124/1778] D loss: 1.2937, G loss: 0.7806\n",
      "[244/1778] D loss: 1.4172, G loss: 0.7586\n",
      "[364/1778] D loss: 1.3896, G loss: 0.7243\n",
      "[484/1778] D loss: 1.3878, G loss: 0.6646\n",
      "[604/1778] D loss: 1.3897, G loss: 0.7252\n",
      "[724/1778] D loss: 1.3935, G loss: 0.6205\n",
      "[844/1778] D loss: 1.3866, G loss: 0.7144\n",
      "[964/1778] D loss: 1.1740, G loss: 0.8890\n",
      "[1084/1778] D loss: 1.3917, G loss: 0.7298\n",
      "[1204/1778] D loss: 1.0443, G loss: 1.1655\n",
      "[1324/1778] D loss: 1.3784, G loss: 0.6951\n",
      "[1444/1778] D loss: 1.3291, G loss: 0.7147\n",
      "[1564/1778] D loss: 1.3282, G loss: 0.8090\n",
      "[1684/1778] D loss: 1.3150, G loss: 0.9064\n",
      "train error: \n",
      " D loss: 1.335465, G loss: 0.791076, D accuracy: 55.2%, cell accuracy: 21926.3%%, board accuracy: 77.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.339431, G loss: 0.791418, D accuracy: 54.5%, cell accuracy: 21922.3%%, board accuracy: 78.2% \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3903, G loss: 0.7290\n",
      "[124/1778] D loss: 1.3892, G loss: 0.6901\n",
      "[244/1778] D loss: 1.2971, G loss: 0.7408\n",
      "[364/1778] D loss: 1.2533, G loss: 0.8015\n",
      "[484/1778] D loss: 1.2578, G loss: 0.6993\n",
      "[604/1778] D loss: 1.3915, G loss: 0.6476\n",
      "[724/1778] D loss: 1.4085, G loss: 0.5980\n",
      "[844/1778] D loss: 0.9525, G loss: 0.9644\n",
      "[964/1778] D loss: 1.3928, G loss: 0.7020\n",
      "[1084/1778] D loss: 1.3922, G loss: 0.5899\n",
      "[1204/1778] D loss: 1.3335, G loss: 0.6805\n",
      "[1324/1778] D loss: 1.2285, G loss: 0.6840\n",
      "[1444/1778] D loss: 1.3916, G loss: 0.7570\n",
      "[1564/1778] D loss: 1.4001, G loss: 0.6648\n",
      "[1684/1778] D loss: 1.3883, G loss: 0.6623\n",
      "train error: \n",
      " D loss: 1.339880, G loss: 0.765500, D accuracy: 53.6%, cell accuracy: 21956.7%%, board accuracy: 85.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.331744, G loss: 0.775017, D accuracy: 53.8%, cell accuracy: 21946.2%%, board accuracy: 83.8% \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1657, G loss: 0.8485\n",
      "[124/1778] D loss: 1.2033, G loss: 0.8690\n",
      "[244/1778] D loss: 1.4481, G loss: 0.7330\n",
      "[364/1778] D loss: 1.3947, G loss: 0.6758\n",
      "[484/1778] D loss: 1.3873, G loss: 0.6823\n",
      "[604/1778] D loss: 1.3446, G loss: 0.7353\n",
      "[724/1778] D loss: 1.3935, G loss: 0.7453\n",
      "[844/1778] D loss: 1.3872, G loss: 0.6656\n",
      "[964/1778] D loss: 1.3848, G loss: 0.6630\n",
      "[1084/1778] D loss: 1.3958, G loss: 0.6794\n",
      "[1204/1778] D loss: 1.3930, G loss: 0.7417\n",
      "[1324/1778] D loss: 1.4004, G loss: 0.6299\n",
      "[1444/1778] D loss: 1.3886, G loss: 0.6802\n",
      "[1564/1778] D loss: 1.3027, G loss: 0.6975\n",
      "[1684/1778] D loss: 1.3909, G loss: 0.6610\n",
      "train error: \n",
      " D loss: 1.340159, G loss: 0.635386, D accuracy: 56.7%, cell accuracy: 21926.0%%, board accuracy: 77.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.326167, G loss: 0.650096, D accuracy: 57.2%, cell accuracy: 21908.8%%, board accuracy: 75.0% \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2873, G loss: 0.8038\n",
      "[124/1778] D loss: 1.3568, G loss: 0.7123\n",
      "[244/1778] D loss: 1.2688, G loss: 0.6322\n",
      "[364/1778] D loss: 1.4069, G loss: 0.5764\n",
      "[484/1778] D loss: 1.3763, G loss: 0.7392\n",
      "[604/1778] D loss: 1.3853, G loss: 0.6998\n",
      "[724/1778] D loss: 1.1504, G loss: 0.7527\n",
      "[844/1778] D loss: 1.3888, G loss: 0.6653\n",
      "[964/1778] D loss: 1.1497, G loss: 0.9631\n",
      "[1084/1778] D loss: 1.3386, G loss: 0.6467\n",
      "[1204/1778] D loss: 1.2956, G loss: 0.6896\n",
      "[1324/1778] D loss: 1.3944, G loss: 0.7409\n",
      "[1444/1778] D loss: 1.3921, G loss: 0.7206\n",
      "[1564/1778] D loss: 1.3745, G loss: 0.7269\n",
      "[1684/1778] D loss: 1.3903, G loss: 0.6944\n",
      "train error: \n",
      " D loss: 1.341091, G loss: 0.709647, D accuracy: 55.1%, cell accuracy: 21948.4%%, board accuracy: 84.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.327502, G loss: 0.732181, D accuracy: 56.5%, cell accuracy: 21938.3%%, board accuracy: 82.9% \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3952, G loss: 0.6805\n",
      "[124/1778] D loss: 1.3612, G loss: 0.6960\n",
      "[244/1778] D loss: 1.2053, G loss: 0.7152\n",
      "[364/1778] D loss: 1.3954, G loss: 0.6485\n",
      "[484/1778] D loss: 1.3890, G loss: 0.7263\n",
      "[604/1778] D loss: 1.2054, G loss: 0.8480\n",
      "[724/1778] D loss: 1.3853, G loss: 0.6736\n",
      "[844/1778] D loss: 1.1558, G loss: 0.8921\n",
      "[964/1778] D loss: 1.2513, G loss: 0.8018\n",
      "[1084/1778] D loss: 1.1734, G loss: 0.8476\n",
      "[1204/1778] D loss: 1.4440, G loss: 0.6538\n",
      "[1324/1778] D loss: 1.2692, G loss: 0.8164\n",
      "[1444/1778] D loss: 1.3593, G loss: 0.7171\n",
      "[1564/1778] D loss: 1.1241, G loss: 0.9502\n",
      "[1684/1778] D loss: 1.3988, G loss: 0.7608\n",
      "train error: \n",
      " D loss: 1.334216, G loss: 0.683917, D accuracy: 54.2%, cell accuracy: 21958.5%%, board accuracy: 86.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.319104, G loss: 0.701731, D accuracy: 54.6%, cell accuracy: 21949.1%%, board accuracy: 83.8% \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3975, G loss: 0.5964\n",
      "[124/1778] D loss: 1.3702, G loss: 0.7631\n",
      "[244/1778] D loss: 1.3947, G loss: 0.6158\n",
      "[364/1778] D loss: 1.3817, G loss: 0.7597\n",
      "[484/1778] D loss: 1.3951, G loss: 0.7658\n",
      "[604/1778] D loss: 1.4007, G loss: 0.7619\n",
      "[724/1778] D loss: 1.7755, G loss: 0.4873\n",
      "[844/1778] D loss: 1.3830, G loss: 0.7216\n",
      "[964/1778] D loss: 1.3884, G loss: 0.6510\n",
      "[1084/1778] D loss: 1.3922, G loss: 0.7341\n",
      "[1204/1778] D loss: 1.3912, G loss: 0.7067\n",
      "[1324/1778] D loss: 1.3994, G loss: 0.7546\n",
      "[1444/1778] D loss: 1.3904, G loss: 0.7388\n",
      "[1564/1778] D loss: 1.3915, G loss: 0.7336\n",
      "[1684/1778] D loss: 1.1833, G loss: 0.8522\n",
      "train error: \n",
      " D loss: 1.325526, G loss: 0.755003, D accuracy: 54.0%, cell accuracy: 21959.3%%, board accuracy: 87.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.313534, G loss: 0.773998, D accuracy: 53.9%, cell accuracy: 21949.8%%, board accuracy: 85.4% \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1698, G loss: 0.8060\n",
      "[124/1778] D loss: 1.4032, G loss: 0.5746\n",
      "[244/1778] D loss: 1.3419, G loss: 0.6973\n",
      "[364/1778] D loss: 1.0673, G loss: 0.8230\n",
      "[484/1778] D loss: 1.3955, G loss: 0.7616\n",
      "[604/1778] D loss: 1.3235, G loss: 0.8472\n",
      "[724/1778] D loss: 1.3914, G loss: 0.6926\n",
      "[844/1778] D loss: 1.5036, G loss: 0.6239\n",
      "[964/1778] D loss: 1.3889, G loss: 0.6824\n",
      "[1084/1778] D loss: 1.4008, G loss: 0.7547\n",
      "[1204/1778] D loss: 1.3899, G loss: 0.7084\n",
      "[1324/1778] D loss: 1.4316, G loss: 0.6517\n",
      "[1444/1778] D loss: 1.4211, G loss: 0.6989\n",
      "[1564/1778] D loss: 1.0673, G loss: 1.2007\n",
      "[1684/1778] D loss: 1.4061, G loss: 0.8237\n",
      "train error: \n",
      " D loss: 1.341213, G loss: 0.815290, D accuracy: 53.8%, cell accuracy: 21949.9%%, board accuracy: 84.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.329146, G loss: 0.837155, D accuracy: 54.5%, cell accuracy: 21937.4%%, board accuracy: 79.7% \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2108, G loss: 0.7890\n",
      "[124/1778] D loss: 1.3895, G loss: 0.6612\n",
      "[244/1778] D loss: 1.1011, G loss: 0.9202\n",
      "[364/1778] D loss: 1.3654, G loss: 0.6902\n",
      "[484/1778] D loss: 1.3928, G loss: 0.7158\n",
      "[604/1778] D loss: 1.4090, G loss: 0.8072\n",
      "[724/1778] D loss: 1.1052, G loss: 1.2582\n",
      "[844/1778] D loss: 1.4012, G loss: 0.6195\n",
      "[964/1778] D loss: 1.3865, G loss: 0.7610\n",
      "[1084/1778] D loss: 1.3926, G loss: 0.7509\n",
      "[1204/1778] D loss: 1.3877, G loss: 0.7238\n",
      "[1324/1778] D loss: 1.1043, G loss: 0.9951\n",
      "[1444/1778] D loss: 1.3926, G loss: 0.7373\n",
      "[1564/1778] D loss: 1.4000, G loss: 0.7965\n",
      "[1684/1778] D loss: 1.2082, G loss: 0.8761\n",
      "train error: \n",
      " D loss: 1.317911, G loss: 0.728552, D accuracy: 54.4%, cell accuracy: 21955.4%%, board accuracy: 86.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.299013, G loss: 0.756351, D accuracy: 55.1%, cell accuracy: 21946.2%%, board accuracy: 83.6% \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3899, G loss: 0.7056\n",
      "[124/1778] D loss: 1.4364, G loss: 0.8942\n",
      "[244/1778] D loss: 1.1677, G loss: 0.7499\n",
      "[364/1778] D loss: 1.3877, G loss: 0.7093\n",
      "[484/1778] D loss: 1.3888, G loss: 0.6699\n",
      "[604/1778] D loss: 1.3719, G loss: 0.8050\n",
      "[724/1778] D loss: 1.3911, G loss: 0.6850\n",
      "[844/1778] D loss: 1.1161, G loss: 0.9049\n",
      "[964/1778] D loss: 1.1496, G loss: 0.8303\n",
      "[1084/1778] D loss: 1.3960, G loss: 0.7759\n",
      "[1204/1778] D loss: 1.3909, G loss: 0.6580\n",
      "[1324/1778] D loss: 1.3835, G loss: 0.6759\n",
      "[1444/1778] D loss: 1.0880, G loss: 1.2805\n",
      "[1564/1778] D loss: 1.3873, G loss: 0.7010\n",
      "[1684/1778] D loss: 1.3728, G loss: 0.7536\n",
      "train error: \n",
      " D loss: 1.308227, G loss: 0.805286, D accuracy: 54.0%, cell accuracy: 21956.0%%, board accuracy: 86.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.297477, G loss: 0.828181, D accuracy: 54.5%, cell accuracy: 21946.2%%, board accuracy: 82.7% \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3969, G loss: 0.7524\n",
      "[124/1778] D loss: 1.4368, G loss: 0.7165\n",
      "[244/1778] D loss: 1.4009, G loss: 0.6315\n",
      "[364/1778] D loss: 1.3881, G loss: 0.6973\n",
      "[484/1778] D loss: 1.1515, G loss: 0.8672\n",
      "[604/1778] D loss: 1.1411, G loss: 0.8243\n",
      "[724/1778] D loss: 1.3908, G loss: 0.6956\n",
      "[844/1778] D loss: 1.3920, G loss: 0.6195\n",
      "[964/1778] D loss: 1.2047, G loss: 0.8135\n",
      "[1084/1778] D loss: 1.0967, G loss: 1.0266\n",
      "[1204/1778] D loss: 1.4030, G loss: 0.7883\n",
      "[1324/1778] D loss: 1.3810, G loss: 0.7328\n",
      "[1444/1778] D loss: 1.4054, G loss: 0.7620\n",
      "[1564/1778] D loss: 1.3880, G loss: 0.6942\n",
      "[1684/1778] D loss: 1.4315, G loss: 0.8301\n",
      "train error: \n",
      " D loss: 1.306750, G loss: 0.783746, D accuracy: 54.3%, cell accuracy: 21958.5%%, board accuracy: 88.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.285089, G loss: 0.820967, D accuracy: 54.8%, cell accuracy: 21947.5%%, board accuracy: 84.5% \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3868, G loss: 0.6886\n",
      "[124/1778] D loss: 1.3981, G loss: 0.7310\n",
      "[244/1778] D loss: 1.4632, G loss: 0.6794\n",
      "[364/1778] D loss: 1.3898, G loss: 0.6576\n",
      "[484/1778] D loss: 1.1407, G loss: 0.7941\n",
      "[604/1778] D loss: 1.3750, G loss: 0.7218\n",
      "[724/1778] D loss: 1.3656, G loss: 0.7026\n",
      "[844/1778] D loss: 1.1577, G loss: 0.8069\n",
      "[964/1778] D loss: 1.1184, G loss: 0.8335\n",
      "[1084/1778] D loss: 1.3923, G loss: 0.7718\n",
      "[1204/1778] D loss: 1.1298, G loss: 0.8825\n",
      "[1324/1778] D loss: 1.4071, G loss: 0.8062\n",
      "[1444/1778] D loss: 1.2374, G loss: 0.7661\n",
      "[1564/1778] D loss: 1.3741, G loss: 0.7459\n",
      "[1684/1778] D loss: 1.1906, G loss: 1.0596\n",
      "train error: \n",
      " D loss: 1.344296, G loss: 0.706271, D accuracy: 55.0%, cell accuracy: 21910.2%%, board accuracy: 73.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.330058, G loss: 0.726621, D accuracy: 54.6%, cell accuracy: 21898.4%%, board accuracy: 72.3% \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3807, G loss: 0.6952\n",
      "[124/1778] D loss: 1.4009, G loss: 0.7393\n",
      "[244/1778] D loss: 1.0910, G loss: 1.1631\n",
      "[364/1778] D loss: 1.3783, G loss: 0.7520\n",
      "[484/1778] D loss: 1.4059, G loss: 0.7944\n",
      "[604/1778] D loss: 1.3275, G loss: 0.9244\n",
      "[724/1778] D loss: 1.3883, G loss: 0.7076\n",
      "[844/1778] D loss: 1.3914, G loss: 0.7528\n",
      "[964/1778] D loss: 1.3867, G loss: 0.6710\n",
      "[1084/1778] D loss: 1.3891, G loss: 0.6462\n",
      "[1204/1778] D loss: 1.3911, G loss: 0.6090\n",
      "[1324/1778] D loss: 1.4135, G loss: 0.6477\n",
      "[1444/1778] D loss: 1.3875, G loss: 0.6988\n",
      "[1564/1778] D loss: 1.2925, G loss: 0.8794\n",
      "[1684/1778] D loss: 1.3894, G loss: 0.7452\n",
      "train error: \n",
      " D loss: 1.340318, G loss: 0.848221, D accuracy: 52.5%, cell accuracy: 21950.2%%, board accuracy: 85.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.323843, G loss: 0.881106, D accuracy: 53.2%, cell accuracy: 21937.8%%, board accuracy: 81.8% \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3722, G loss: 0.7939\n",
      "[124/1778] D loss: 1.2634, G loss: 0.7804\n",
      "[244/1778] D loss: 1.1499, G loss: 0.8554\n",
      "[364/1778] D loss: 1.3834, G loss: 0.7461\n",
      "[484/1778] D loss: 1.1932, G loss: 0.8780\n",
      "[604/1778] D loss: 1.3944, G loss: 0.7294\n",
      "[724/1778] D loss: 1.3972, G loss: 0.6021\n",
      "[844/1778] D loss: 1.3892, G loss: 0.7270\n",
      "[964/1778] D loss: 1.0557, G loss: 1.3449\n",
      "[1084/1778] D loss: 1.3911, G loss: 0.7201\n",
      "[1204/1778] D loss: 1.0380, G loss: 1.0561\n",
      "[1324/1778] D loss: 1.3848, G loss: 0.6962\n",
      "[1444/1778] D loss: 1.3887, G loss: 0.6675\n",
      "[1564/1778] D loss: 1.2464, G loss: 0.7183\n",
      "[1684/1778] D loss: 1.2658, G loss: 0.7595\n",
      "train error: \n",
      " D loss: 1.342582, G loss: 0.730646, D accuracy: 54.0%, cell accuracy: 21947.2%%, board accuracy: 83.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.329432, G loss: 0.761821, D accuracy: 54.6%, cell accuracy: 21935.8%%, board accuracy: 79.5% \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3917, G loss: 0.6961\n",
      "[124/1778] D loss: 1.0320, G loss: 0.8865\n",
      "[244/1778] D loss: 1.3914, G loss: 0.7782\n",
      "[364/1778] D loss: 1.4133, G loss: 0.8380\n",
      "[484/1778] D loss: 1.3944, G loss: 0.7819\n",
      "[604/1778] D loss: 1.3557, G loss: 0.9450\n",
      "[724/1778] D loss: 1.3924, G loss: 0.7468\n",
      "[844/1778] D loss: 1.5485, G loss: 0.5801\n",
      "[964/1778] D loss: 1.3900, G loss: 0.7090\n",
      "[1084/1778] D loss: 1.1488, G loss: 1.0057\n",
      "[1204/1778] D loss: 1.3898, G loss: 0.7730\n",
      "[1324/1778] D loss: 1.3966, G loss: 0.7474\n",
      "[1444/1778] D loss: 1.3840, G loss: 0.7336\n",
      "[1564/1778] D loss: 1.3901, G loss: 0.6699\n",
      "[1684/1778] D loss: 1.1395, G loss: 0.9526\n",
      "train error: \n",
      " D loss: 1.335226, G loss: 0.725915, D accuracy: 54.2%, cell accuracy: 21946.6%%, board accuracy: 82.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.321928, G loss: 0.750530, D accuracy: 55.2%, cell accuracy: 21934.5%%, board accuracy: 79.7% \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3871, G loss: 0.7349\n",
      "[124/1778] D loss: 1.1528, G loss: 0.7653\n",
      "[244/1778] D loss: 1.3577, G loss: 0.7288\n",
      "[364/1778] D loss: 1.3889, G loss: 0.6642\n",
      "[484/1778] D loss: 1.3866, G loss: 0.6810\n",
      "[604/1778] D loss: 1.3812, G loss: 0.6758\n",
      "[724/1778] D loss: 1.0864, G loss: 1.2096\n",
      "[844/1778] D loss: 1.4385, G loss: 0.5220\n",
      "[964/1778] D loss: 1.3873, G loss: 0.6623\n",
      "[1084/1778] D loss: 1.1498, G loss: 0.7956\n",
      "[1204/1778] D loss: 1.3440, G loss: 0.7306\n",
      "[1324/1778] D loss: 1.3929, G loss: 0.7180\n",
      "[1444/1778] D loss: 1.5157, G loss: 0.6154\n",
      "[1564/1778] D loss: 1.1840, G loss: 0.9300\n",
      "[1684/1778] D loss: 1.3876, G loss: 0.6505\n",
      "train error: \n",
      " D loss: 1.336736, G loss: 0.727030, D accuracy: 54.3%, cell accuracy: 21943.8%%, board accuracy: 82.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.310954, G loss: 0.770142, D accuracy: 55.3%, cell accuracy: 21938.3%%, board accuracy: 80.6% \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3872, G loss: 0.6572\n",
      "[124/1778] D loss: 1.3848, G loss: 0.6957\n",
      "[244/1778] D loss: 1.3945, G loss: 0.7384\n",
      "[364/1778] D loss: 1.3860, G loss: 0.6571\n",
      "[484/1778] D loss: 1.4539, G loss: 0.5686\n",
      "[604/1778] D loss: 1.1216, G loss: 0.9921\n",
      "[724/1778] D loss: 1.3963, G loss: 0.7937\n",
      "[844/1778] D loss: 1.3814, G loss: 0.7290\n",
      "[964/1778] D loss: 1.4002, G loss: 0.6196\n",
      "[1084/1778] D loss: 1.3074, G loss: 0.8814\n",
      "[1204/1778] D loss: 1.4352, G loss: 0.8313\n",
      "[1324/1778] D loss: 1.3927, G loss: 0.6054\n",
      "[1444/1778] D loss: 1.3349, G loss: 0.7117\n",
      "[1564/1778] D loss: 1.2681, G loss: 0.8447\n",
      "[1684/1778] D loss: 1.4931, G loss: 0.6953\n",
      "train error: \n",
      " D loss: 1.336113, G loss: 0.684928, D accuracy: 54.6%, cell accuracy: 21945.3%%, board accuracy: 83.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.327220, G loss: 0.700619, D accuracy: 55.1%, cell accuracy: 21936.0%%, board accuracy: 81.3% \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4068, G loss: 0.5961\n",
      "[124/1778] D loss: 1.1682, G loss: 0.7738\n",
      "[244/1778] D loss: 1.1199, G loss: 1.0194\n",
      "[364/1778] D loss: 1.3901, G loss: 0.7247\n",
      "[484/1778] D loss: 1.0654, G loss: 1.2048\n",
      "[604/1778] D loss: 1.3893, G loss: 0.7346\n",
      "[724/1778] D loss: 1.3887, G loss: 0.6918\n",
      "[844/1778] D loss: 1.4590, G loss: 0.6952\n",
      "[964/1778] D loss: 1.3673, G loss: 0.7208\n",
      "[1084/1778] D loss: 1.2824, G loss: 0.7366\n",
      "[1204/1778] D loss: 1.3934, G loss: 0.7772\n",
      "[1324/1778] D loss: 1.3843, G loss: 0.6556\n",
      "[1444/1778] D loss: 1.3180, G loss: 0.7441\n",
      "[1564/1778] D loss: 1.1639, G loss: 0.8061\n",
      "[1684/1778] D loss: 1.2942, G loss: 0.7168\n",
      "train error: \n",
      " D loss: 1.339188, G loss: 0.708565, D accuracy: 54.3%, cell accuracy: 21946.6%%, board accuracy: 83.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.312759, G loss: 0.746963, D accuracy: 55.7%, cell accuracy: 21939.0%%, board accuracy: 80.6% \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0771, G loss: 0.9919\n",
      "[124/1778] D loss: 1.3861, G loss: 0.6880\n",
      "[244/1778] D loss: 1.0478, G loss: 1.2488\n",
      "[364/1778] D loss: 1.0272, G loss: 1.0439\n",
      "[484/1778] D loss: 1.3882, G loss: 0.6531\n",
      "[604/1778] D loss: 1.1725, G loss: 0.9823\n",
      "[724/1778] D loss: 1.1929, G loss: 0.8730\n",
      "[844/1778] D loss: 1.4020, G loss: 0.8214\n",
      "[964/1778] D loss: 1.3884, G loss: 0.6808\n",
      "[1084/1778] D loss: 1.2613, G loss: 0.8938\n",
      "[1204/1778] D loss: 1.3877, G loss: 0.6908\n",
      "[1324/1778] D loss: 1.3906, G loss: 0.7384\n",
      "[1444/1778] D loss: 1.3906, G loss: 0.6324\n",
      "[1564/1778] D loss: 1.3924, G loss: 0.7652\n",
      "[1684/1778] D loss: 1.4482, G loss: 0.8974\n",
      "train error: \n",
      " D loss: 1.327886, G loss: 0.691020, D accuracy: 54.3%, cell accuracy: 21953.9%%, board accuracy: 84.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.302303, G loss: 0.723861, D accuracy: 55.1%, cell accuracy: 21947.5%%, board accuracy: 85.1% \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3898, G loss: 0.6473\n",
      "[124/1778] D loss: 1.3843, G loss: 0.6625\n",
      "[244/1778] D loss: 1.4342, G loss: 0.5587\n",
      "[364/1778] D loss: 1.1422, G loss: 0.9455\n",
      "[484/1778] D loss: 1.4070, G loss: 0.7073\n",
      "[604/1778] D loss: 1.3890, G loss: 0.6587\n",
      "[724/1778] D loss: 1.1410, G loss: 0.9293\n",
      "[844/1778] D loss: 1.1520, G loss: 0.8718\n",
      "[964/1778] D loss: 1.3884, G loss: 0.6316\n",
      "[1084/1778] D loss: 1.3969, G loss: 0.7978\n",
      "[1204/1778] D loss: 1.1535, G loss: 0.8733\n",
      "[1324/1778] D loss: 1.2290, G loss: 0.9291\n",
      "[1444/1778] D loss: 1.3636, G loss: 0.7787\n",
      "[1564/1778] D loss: 1.3160, G loss: 0.9157\n",
      "[1684/1778] D loss: 1.1395, G loss: 1.1703\n",
      "train error: \n",
      " D loss: 1.354393, G loss: 0.783642, D accuracy: 53.0%, cell accuracy: 21944.5%%, board accuracy: 82.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.343975, G loss: 0.804969, D accuracy: 53.4%, cell accuracy: 21933.3%%, board accuracy: 81.5% \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3309, G loss: 0.8225\n",
      "[124/1778] D loss: 1.3909, G loss: 0.6387\n",
      "[244/1778] D loss: 1.5474, G loss: 0.6023\n",
      "[364/1778] D loss: 1.2468, G loss: 0.9733\n",
      "[484/1778] D loss: 1.4735, G loss: 0.6099\n",
      "[604/1778] D loss: 1.3992, G loss: 0.7030\n",
      "[724/1778] D loss: 1.3387, G loss: 0.8266\n",
      "[844/1778] D loss: 1.3991, G loss: 0.6188\n",
      "[964/1778] D loss: 1.3042, G loss: 0.8452\n",
      "[1084/1778] D loss: 1.3437, G loss: 0.8664\n",
      "[1204/1778] D loss: 1.2816, G loss: 0.9178\n",
      "[1324/1778] D loss: 1.3958, G loss: 0.6384\n",
      "[1444/1778] D loss: 1.3881, G loss: 0.6969\n",
      "[1564/1778] D loss: 1.4007, G loss: 0.7369\n",
      "[1684/1778] D loss: 1.2854, G loss: 0.7153\n",
      "train error: \n",
      " D loss: 1.296324, G loss: 0.843999, D accuracy: 57.6%, cell accuracy: 21933.6%%, board accuracy: 79.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.281854, G loss: 0.878413, D accuracy: 57.7%, cell accuracy: 21924.8%%, board accuracy: 77.5% \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3871, G loss: 0.6825\n",
      "[124/1778] D loss: 0.9819, G loss: 1.5452\n",
      "[244/1778] D loss: 1.2124, G loss: 1.0355\n",
      "[364/1778] D loss: 1.3472, G loss: 0.7264\n",
      "[484/1778] D loss: 1.3948, G loss: 0.7654\n",
      "[604/1778] D loss: 1.3947, G loss: 0.7664\n",
      "[724/1778] D loss: 1.3978, G loss: 0.7853\n",
      "[844/1778] D loss: 1.2277, G loss: 0.8122\n",
      "[964/1778] D loss: 1.5066, G loss: 0.5976\n",
      "[1084/1778] D loss: 1.0823, G loss: 1.1884\n",
      "[1204/1778] D loss: 1.3913, G loss: 0.7340\n",
      "[1324/1778] D loss: 1.1185, G loss: 0.8219\n",
      "[1444/1778] D loss: 1.2760, G loss: 0.9061\n",
      "[1564/1778] D loss: 1.3961, G loss: 0.6275\n",
      "[1684/1778] D loss: 1.3891, G loss: 0.6515\n",
      "train error: \n",
      " D loss: 1.341207, G loss: 0.653230, D accuracy: 54.3%, cell accuracy: 21948.7%%, board accuracy: 85.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.316256, G loss: 0.681262, D accuracy: 55.2%, cell accuracy: 21944.8%%, board accuracy: 84.7% \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3884, G loss: 0.6251\n",
      "[124/1778] D loss: 1.3996, G loss: 0.7403\n",
      "[244/1778] D loss: 1.1778, G loss: 0.9221\n",
      "[364/1778] D loss: 1.3729, G loss: 0.7044\n",
      "[484/1778] D loss: 1.3893, G loss: 0.6923\n",
      "[604/1778] D loss: 1.3047, G loss: 0.8737\n",
      "[724/1778] D loss: 1.4188, G loss: 0.6807\n",
      "[844/1778] D loss: 1.4830, G loss: 0.6784\n",
      "[964/1778] D loss: 1.3916, G loss: 0.6715\n",
      "[1084/1778] D loss: 1.4073, G loss: 0.8194\n",
      "[1204/1778] D loss: 1.4455, G loss: 0.8852\n",
      "[1324/1778] D loss: 1.1731, G loss: 0.8185\n",
      "[1444/1778] D loss: 1.3923, G loss: 0.7414\n",
      "[1564/1778] D loss: 1.3924, G loss: 0.6759\n",
      "[1684/1778] D loss: 1.3870, G loss: 0.7087\n",
      "train error: \n",
      " D loss: 1.321992, G loss: 0.766204, D accuracy: 54.7%, cell accuracy: 21953.0%%, board accuracy: 85.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.303607, G loss: 0.796705, D accuracy: 54.8%, cell accuracy: 21942.3%%, board accuracy: 82.0% \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3946, G loss: 0.7466\n",
      "[124/1778] D loss: 1.3766, G loss: 0.7702\n",
      "[244/1778] D loss: 1.3885, G loss: 0.6208\n",
      "[364/1778] D loss: 1.4100, G loss: 0.8239\n",
      "[484/1778] D loss: 1.3927, G loss: 0.7285\n",
      "[604/1778] D loss: 1.1579, G loss: 0.8868\n",
      "[724/1778] D loss: 1.3895, G loss: 0.6511\n",
      "[844/1778] D loss: 1.3914, G loss: 0.6677\n",
      "[964/1778] D loss: 1.3865, G loss: 0.6754\n",
      "[1084/1778] D loss: 1.3865, G loss: 0.6997\n",
      "[1204/1778] D loss: 1.0733, G loss: 1.2561\n",
      "[1324/1778] D loss: 1.1424, G loss: 0.8760\n",
      "[1444/1778] D loss: 1.3460, G loss: 0.7761\n",
      "[1564/1778] D loss: 1.3771, G loss: 0.6981\n",
      "[1684/1778] D loss: 1.3345, G loss: 0.7866\n",
      "train error: \n",
      " D loss: 1.309968, G loss: 0.830220, D accuracy: 54.2%, cell accuracy: 21959.1%%, board accuracy: 87.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.287864, G loss: 0.873424, D accuracy: 54.7%, cell accuracy: 21950.0%%, board accuracy: 84.5% \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3979, G loss: 0.7109\n",
      "[124/1778] D loss: 1.4151, G loss: 0.5946\n",
      "[244/1778] D loss: 1.3874, G loss: 0.7100\n",
      "[364/1778] D loss: 1.3868, G loss: 0.7506\n",
      "[484/1778] D loss: 1.1399, G loss: 0.8217\n",
      "[604/1778] D loss: 1.3957, G loss: 0.6752\n",
      "[724/1778] D loss: 1.3929, G loss: 0.7447\n",
      "[844/1778] D loss: 1.3869, G loss: 0.7236\n",
      "[964/1778] D loss: 1.3800, G loss: 0.6692\n",
      "[1084/1778] D loss: 1.1109, G loss: 0.8612\n",
      "[1204/1778] D loss: 1.9391, G loss: 0.4944\n",
      "[1324/1778] D loss: 1.4284, G loss: 0.6246\n",
      "[1444/1778] D loss: 1.0601, G loss: 1.2570\n",
      "[1564/1778] D loss: 1.3726, G loss: 0.6878\n",
      "[1684/1778] D loss: 1.3861, G loss: 0.7117\n",
      "train error: \n",
      " D loss: 1.308100, G loss: 0.802236, D accuracy: 54.8%, cell accuracy: 21949.1%%, board accuracy: 81.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.286326, G loss: 0.841495, D accuracy: 55.2%, cell accuracy: 21941.9%%, board accuracy: 79.3% \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3960, G loss: 0.7094\n",
      "[124/1778] D loss: 1.3882, G loss: 0.6250\n",
      "[244/1778] D loss: 1.3552, G loss: 0.7403\n",
      "[364/1778] D loss: 1.3756, G loss: 0.7298\n",
      "[484/1778] D loss: 1.3839, G loss: 0.7233\n",
      "[604/1778] D loss: 1.0844, G loss: 1.1241\n",
      "[724/1778] D loss: 1.3819, G loss: 0.6055\n",
      "[844/1778] D loss: 1.3968, G loss: 0.7891\n",
      "[964/1778] D loss: 1.3866, G loss: 0.7110\n",
      "[1084/1778] D loss: 1.3930, G loss: 0.7716\n",
      "[1204/1778] D loss: 1.7547, G loss: 0.5214\n",
      "[1324/1778] D loss: 1.3881, G loss: 0.7144\n",
      "[1444/1778] D loss: 1.0884, G loss: 1.0312\n",
      "[1564/1778] D loss: 1.1194, G loss: 0.8931\n",
      "[1684/1778] D loss: 1.0863, G loss: 1.2230\n",
      "train error: \n",
      " D loss: 1.318666, G loss: 0.823717, D accuracy: 54.3%, cell accuracy: 21957.6%%, board accuracy: 86.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.300628, G loss: 0.861246, D accuracy: 54.7%, cell accuracy: 21950.2%%, board accuracy: 85.4% \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3900, G loss: 0.6977\n",
      "[124/1778] D loss: 1.1755, G loss: 0.9210\n",
      "[244/1778] D loss: 1.4010, G loss: 0.7830\n",
      "[364/1778] D loss: 1.3980, G loss: 0.8505\n",
      "[484/1778] D loss: 1.1370, G loss: 0.8934\n",
      "[604/1778] D loss: 1.3550, G loss: 0.7319\n",
      "[724/1778] D loss: 1.3875, G loss: 0.6812\n",
      "[844/1778] D loss: 1.3619, G loss: 0.6983\n",
      "[964/1778] D loss: 1.3805, G loss: 0.6791\n",
      "[1084/1778] D loss: 1.3801, G loss: 0.6949\n",
      "[1204/1778] D loss: 1.4022, G loss: 0.7066\n",
      "[1324/1778] D loss: 1.1425, G loss: 0.8430\n",
      "[1444/1778] D loss: 1.4026, G loss: 0.5568\n",
      "[1564/1778] D loss: 1.3999, G loss: 0.6290\n",
      "[1684/1778] D loss: 1.3673, G loss: 0.6775\n",
      "train error: \n",
      " D loss: 1.323785, G loss: 0.703833, D accuracy: 57.2%, cell accuracy: 21917.2%%, board accuracy: 73.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.304484, G loss: 0.735633, D accuracy: 57.8%, cell accuracy: 21914.4%%, board accuracy: 70.0% \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3872, G loss: 0.6815\n",
      "[124/1778] D loss: 1.4018, G loss: 0.5773\n",
      "[244/1778] D loss: 1.3937, G loss: 0.6360\n",
      "[364/1778] D loss: 1.1919, G loss: 0.8544\n",
      "[484/1778] D loss: 1.2865, G loss: 0.9390\n",
      "[604/1778] D loss: 1.3015, G loss: 0.8558\n",
      "[724/1778] D loss: 1.3943, G loss: 0.6824\n",
      "[844/1778] D loss: 1.3682, G loss: 0.7612\n",
      "[964/1778] D loss: 1.3455, G loss: 0.8129\n",
      "[1084/1778] D loss: 1.2985, G loss: 0.8186\n",
      "[1204/1778] D loss: 1.2085, G loss: 0.7277\n",
      "[1324/1778] D loss: 0.9535, G loss: 1.1900\n",
      "[1444/1778] D loss: 1.4096, G loss: 0.9180\n",
      "[1564/1778] D loss: 1.4049, G loss: 0.6740\n",
      "[1684/1778] D loss: 1.3919, G loss: 0.7357\n",
      "train error: \n",
      " D loss: 1.317376, G loss: 0.820371, D accuracy: 56.2%, cell accuracy: 21926.0%%, board accuracy: 78.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.308229, G loss: 0.840444, D accuracy: 56.5%, cell accuracy: 21917.1%%, board accuracy: 76.6% \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2579, G loss: 0.8490\n",
      "[124/1778] D loss: 1.3945, G loss: 0.6712\n",
      "[244/1778] D loss: 1.3141, G loss: 0.7100\n",
      "[364/1778] D loss: 1.4032, G loss: 0.6207\n",
      "[484/1778] D loss: 1.2099, G loss: 0.7823\n",
      "[604/1778] D loss: 1.3923, G loss: 0.7238\n",
      "[724/1778] D loss: 1.4979, G loss: 0.6037\n",
      "[844/1778] D loss: 1.3946, G loss: 0.7864\n",
      "[964/1778] D loss: 1.3751, G loss: 0.7636\n",
      "[1084/1778] D loss: 1.3936, G loss: 0.8182\n",
      "[1204/1778] D loss: 1.4018, G loss: 0.7534\n",
      "[1324/1778] D loss: 1.3924, G loss: 0.7619\n",
      "[1444/1778] D loss: 1.1984, G loss: 0.9964\n",
      "[1564/1778] D loss: 1.3400, G loss: 0.6708\n",
      "[1684/1778] D loss: 1.3933, G loss: 0.8089\n",
      "train error: \n",
      " D loss: 1.320346, G loss: 0.805395, D accuracy: 53.5%, cell accuracy: 21932.1%%, board accuracy: 80.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.309212, G loss: 0.809140, D accuracy: 54.2%, cell accuracy: 21930.0%%, board accuracy: 81.1% \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3246, G loss: 0.8303\n",
      "[124/1778] D loss: 1.4348, G loss: 0.6897\n",
      "[244/1778] D loss: 1.3888, G loss: 0.6536\n",
      "[364/1778] D loss: 1.3789, G loss: 0.6257\n",
      "[484/1778] D loss: 1.3951, G loss: 0.6624\n",
      "[604/1778] D loss: 1.5563, G loss: 0.7499\n",
      "[724/1778] D loss: 1.3870, G loss: 0.7080\n",
      "[844/1778] D loss: 1.3314, G loss: 0.7208\n",
      "[964/1778] D loss: 1.3576, G loss: 0.6758\n",
      "[1084/1778] D loss: 1.3951, G loss: 0.7740\n",
      "[1204/1778] D loss: 1.3817, G loss: 0.8292\n",
      "[1324/1778] D loss: 1.3956, G loss: 0.7456\n",
      "[1444/1778] D loss: 1.3993, G loss: 0.7579\n",
      "[1564/1778] D loss: 1.3884, G loss: 0.7493\n",
      "[1684/1778] D loss: 1.3962, G loss: 0.8075\n",
      "train error: \n",
      " D loss: 1.310506, G loss: 0.759435, D accuracy: 54.4%, cell accuracy: 21961.8%%, board accuracy: 88.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.287860, G loss: 0.792684, D accuracy: 55.5%, cell accuracy: 21954.3%%, board accuracy: 86.5% \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3908, G loss: 0.6985\n",
      "[124/1778] D loss: 1.3989, G loss: 0.7685\n",
      "[244/1778] D loss: 1.3977, G loss: 0.8043\n",
      "[364/1778] D loss: 1.3842, G loss: 0.6655\n",
      "[484/1778] D loss: 1.3969, G loss: 0.7382\n",
      "[604/1778] D loss: 1.3947, G loss: 0.7162\n",
      "[724/1778] D loss: 1.1075, G loss: 0.9033\n",
      "[844/1778] D loss: 1.3723, G loss: 0.7261\n",
      "[964/1778] D loss: 1.3910, G loss: 0.7324\n",
      "[1084/1778] D loss: 1.3865, G loss: 0.6977\n",
      "[1204/1778] D loss: 1.3928, G loss: 0.6541\n",
      "[1324/1778] D loss: 1.1481, G loss: 0.9198\n",
      "[1444/1778] D loss: 1.3981, G loss: 0.6744\n",
      "[1564/1778] D loss: 1.3502, G loss: 0.7142\n",
      "[1684/1778] D loss: 1.0624, G loss: 1.2349\n",
      "train error: \n",
      " D loss: 1.321327, G loss: 0.767765, D accuracy: 55.1%, cell accuracy: 21938.1%%, board accuracy: 78.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.312609, G loss: 0.792069, D accuracy: 55.6%, cell accuracy: 21926.4%%, board accuracy: 75.0% \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3908, G loss: 0.7311\n",
      "[124/1778] D loss: 1.3969, G loss: 0.6904\n",
      "[244/1778] D loss: 1.3838, G loss: 0.6385\n",
      "[364/1778] D loss: 1.1864, G loss: 1.0157\n",
      "[484/1778] D loss: 1.3955, G loss: 0.7698\n",
      "[604/1778] D loss: 1.3907, G loss: 0.6168\n",
      "[724/1778] D loss: 1.3535, G loss: 0.6645\n",
      "[844/1778] D loss: 1.3852, G loss: 0.6914\n",
      "[964/1778] D loss: 1.1272, G loss: 0.8805\n",
      "[1084/1778] D loss: 1.0592, G loss: 0.9202\n",
      "[1204/1778] D loss: 1.2543, G loss: 1.0227\n",
      "[1324/1778] D loss: 1.3877, G loss: 0.7179\n",
      "[1444/1778] D loss: 1.4120, G loss: 0.5927\n",
      "[1564/1778] D loss: 1.4142, G loss: 0.5504\n",
      "[1684/1778] D loss: 1.0745, G loss: 1.1304\n",
      "train error: \n",
      " D loss: 1.338631, G loss: 0.687604, D accuracy: 55.3%, cell accuracy: 21946.1%%, board accuracy: 82.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.335825, G loss: 0.706686, D accuracy: 54.8%, cell accuracy: 21937.6%%, board accuracy: 81.5% \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3958, G loss: 0.5997\n",
      "[124/1778] D loss: 1.4579, G loss: 0.7499\n",
      "[244/1778] D loss: 1.3389, G loss: 0.6255\n",
      "[364/1778] D loss: 1.6167, G loss: 0.6447\n",
      "[484/1778] D loss: 1.2724, G loss: 0.8326\n",
      "[604/1778] D loss: 1.3890, G loss: 0.6889\n",
      "[724/1778] D loss: 1.2034, G loss: 0.8823\n",
      "[844/1778] D loss: 1.1633, G loss: 0.9033\n",
      "[964/1778] D loss: 1.3881, G loss: 0.6977\n",
      "[1084/1778] D loss: 1.3890, G loss: 0.7270\n",
      "[1204/1778] D loss: 1.2026, G loss: 0.8164\n",
      "[1324/1778] D loss: 1.3865, G loss: 0.6926\n",
      "[1444/1778] D loss: 1.3878, G loss: 0.7258\n",
      "[1564/1778] D loss: 1.1683, G loss: 0.8030\n",
      "[1684/1778] D loss: 1.4070, G loss: 0.6438\n",
      "train error: \n",
      " D loss: 1.337220, G loss: 0.659536, D accuracy: 53.9%, cell accuracy: 21956.7%%, board accuracy: 84.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.315017, G loss: 0.685596, D accuracy: 54.8%, cell accuracy: 21951.1%%, board accuracy: 83.3% \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3962, G loss: 0.5783\n",
      "[124/1778] D loss: 1.4045, G loss: 0.6223\n",
      "[244/1778] D loss: 1.3962, G loss: 0.6689\n",
      "[364/1778] D loss: 1.3571, G loss: 0.7149\n",
      "[484/1778] D loss: 1.3931, G loss: 0.7154\n",
      "[604/1778] D loss: 1.3993, G loss: 0.8019\n",
      "[724/1778] D loss: 1.2487, G loss: 0.8663\n",
      "[844/1778] D loss: 1.1264, G loss: 0.8523\n",
      "[964/1778] D loss: 1.5904, G loss: 0.6865\n",
      "[1084/1778] D loss: 1.3035, G loss: 0.8458\n",
      "[1204/1778] D loss: 1.1312, G loss: 0.9876\n",
      "[1324/1778] D loss: 1.3948, G loss: 0.6096\n",
      "[1444/1778] D loss: 1.3892, G loss: 0.6660\n",
      "[1564/1778] D loss: 1.1248, G loss: 1.3426\n",
      "[1684/1778] D loss: 1.3882, G loss: 0.7068\n",
      "train error: \n",
      " D loss: 1.323939, G loss: 0.747751, D accuracy: 54.0%, cell accuracy: 21960.1%%, board accuracy: 87.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.304152, G loss: 0.784300, D accuracy: 55.3%, cell accuracy: 21950.9%%, board accuracy: 86.0% \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3913, G loss: 0.7104\n",
      "[124/1778] D loss: 1.3938, G loss: 0.6342\n",
      "[244/1778] D loss: 1.3875, G loss: 0.6948\n",
      "[364/1778] D loss: 1.3958, G loss: 0.6857\n",
      "[484/1778] D loss: 1.3921, G loss: 0.6765\n",
      "[604/1778] D loss: 1.3866, G loss: 0.6712\n",
      "[724/1778] D loss: 1.3924, G loss: 0.6704\n",
      "[844/1778] D loss: 1.3966, G loss: 0.8186\n",
      "[964/1778] D loss: 1.3941, G loss: 0.7197\n",
      "[1084/1778] D loss: 1.3965, G loss: 0.6459\n",
      "[1204/1778] D loss: 1.3877, G loss: 0.7940\n",
      "[1324/1778] D loss: 1.1114, G loss: 0.8644\n",
      "[1444/1778] D loss: 1.3880, G loss: 0.6837\n",
      "[1564/1778] D loss: 1.1239, G loss: 1.0199\n",
      "[1684/1778] D loss: 1.3851, G loss: 0.7310\n",
      "train error: \n",
      " D loss: 1.317846, G loss: 0.690608, D accuracy: 54.4%, cell accuracy: 21959.2%%, board accuracy: 88.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.293097, G loss: 0.732401, D accuracy: 55.2%, cell accuracy: 21952.7%%, board accuracy: 85.8% \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1402, G loss: 0.7877\n",
      "[124/1778] D loss: 1.4094, G loss: 0.7864\n",
      "[244/1778] D loss: 1.3864, G loss: 0.7049\n",
      "[364/1778] D loss: 1.4014, G loss: 0.8027\n",
      "[484/1778] D loss: 1.3922, G loss: 0.7565\n",
      "[604/1778] D loss: 0.7871, G loss: 1.4323\n",
      "[724/1778] D loss: 1.1071, G loss: 1.0021\n",
      "[844/1778] D loss: 1.1058, G loss: 1.0838\n",
      "[964/1778] D loss: 1.0512, G loss: 1.4107\n",
      "[1084/1778] D loss: 1.3967, G loss: 0.6995\n",
      "[1204/1778] D loss: 1.1071, G loss: 1.1776\n",
      "[1324/1778] D loss: 1.3904, G loss: 0.7121\n",
      "[1444/1778] D loss: 1.1250, G loss: 0.8799\n",
      "[1564/1778] D loss: 1.4109, G loss: 0.7518\n",
      "[1684/1778] D loss: 1.1880, G loss: 0.7613\n",
      "train error: \n",
      " D loss: 1.307678, G loss: 0.739872, D accuracy: 54.2%, cell accuracy: 21963.8%%, board accuracy: 90.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.284619, G loss: 0.778599, D accuracy: 54.8%, cell accuracy: 21953.8%%, board accuracy: 86.7% \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3991, G loss: 0.6611\n",
      "[124/1778] D loss: 1.3898, G loss: 0.7500\n",
      "[244/1778] D loss: 1.3851, G loss: 0.7076\n",
      "[364/1778] D loss: 1.3944, G loss: 0.7601\n",
      "[484/1778] D loss: 1.3932, G loss: 0.7089\n",
      "[604/1778] D loss: 1.1032, G loss: 0.8992\n",
      "[724/1778] D loss: 1.3917, G loss: 0.6694\n",
      "[844/1778] D loss: 1.3751, G loss: 0.6359\n",
      "[964/1778] D loss: 1.1031, G loss: 1.0126\n",
      "[1084/1778] D loss: 1.0826, G loss: 1.1430\n",
      "[1204/1778] D loss: 1.3932, G loss: 0.7225\n",
      "[1324/1778] D loss: 1.3968, G loss: 0.6312\n",
      "[1444/1778] D loss: 1.4096, G loss: 0.5938\n",
      "[1564/1778] D loss: 1.3945, G loss: 0.8011\n",
      "[1684/1778] D loss: 1.4176, G loss: 0.8001\n",
      "train error: \n",
      " D loss: 1.306555, G loss: 0.860067, D accuracy: 53.7%, cell accuracy: 21958.6%%, board accuracy: 88.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.280913, G loss: 0.909025, D accuracy: 54.4%, cell accuracy: 21951.8%%, board accuracy: 85.6% \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1176, G loss: 1.1744\n",
      "[124/1778] D loss: 1.0644, G loss: 1.3294\n",
      "[244/1778] D loss: 1.3910, G loss: 0.6508\n",
      "[364/1778] D loss: 1.3912, G loss: 0.6857\n",
      "[484/1778] D loss: 1.3990, G loss: 0.7788\n",
      "[604/1778] D loss: 1.4061, G loss: 0.6424\n",
      "[724/1778] D loss: 1.3998, G loss: 0.5607\n",
      "[844/1778] D loss: 1.3840, G loss: 0.6139\n",
      "[964/1778] D loss: 1.0571, G loss: 1.3541\n",
      "[1084/1778] D loss: 1.3903, G loss: 0.7393\n",
      "[1204/1778] D loss: 1.0982, G loss: 0.9548\n",
      "[1324/1778] D loss: 1.4128, G loss: 0.8103\n",
      "[1444/1778] D loss: 1.3885, G loss: 0.6814\n",
      "[1564/1778] D loss: 1.0903, G loss: 1.1730\n",
      "[1684/1778] D loss: 1.3921, G loss: 0.7357\n",
      "train error: \n",
      " D loss: 1.302210, G loss: 0.819423, D accuracy: 54.3%, cell accuracy: 21951.7%%, board accuracy: 85.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.287031, G loss: 0.856204, D accuracy: 54.7%, cell accuracy: 21942.8%%, board accuracy: 82.4% \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.5535, G loss: 0.6272\n",
      "[124/1778] D loss: 1.4024, G loss: 0.7873\n",
      "[244/1778] D loss: 1.3998, G loss: 0.6340\n",
      "[364/1778] D loss: 1.3916, G loss: 0.6647\n",
      "[484/1778] D loss: 1.4020, G loss: 0.6290\n",
      "[604/1778] D loss: 1.0990, G loss: 1.1978\n",
      "[724/1778] D loss: 1.2975, G loss: 0.7768\n",
      "[844/1778] D loss: 1.3773, G loss: 0.6501\n",
      "[964/1778] D loss: 1.2666, G loss: 0.7969\n",
      "[1084/1778] D loss: 0.7903, G loss: 1.3339\n",
      "[1204/1778] D loss: 1.3398, G loss: 0.8015\n",
      "[1324/1778] D loss: 1.3890, G loss: 0.6702\n",
      "[1444/1778] D loss: 1.1112, G loss: 0.9276\n",
      "[1564/1778] D loss: 1.3994, G loss: 0.6679\n",
      "[1684/1778] D loss: 1.2486, G loss: 0.9239\n",
      "train error: \n",
      " D loss: 1.305264, G loss: 0.817173, D accuracy: 54.4%, cell accuracy: 21945.7%%, board accuracy: 82.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.284718, G loss: 0.851841, D accuracy: 55.4%, cell accuracy: 21935.1%%, board accuracy: 78.6% \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3474, G loss: 0.7187\n",
      "[124/1778] D loss: 1.3879, G loss: 0.7030\n",
      "[244/1778] D loss: 1.4550, G loss: 0.5747\n",
      "[364/1778] D loss: 1.3897, G loss: 0.7215\n",
      "[484/1778] D loss: 1.0870, G loss: 0.9265\n",
      "[604/1778] D loss: 1.4242, G loss: 0.7939\n",
      "[724/1778] D loss: 1.3242, G loss: 0.6370\n",
      "[844/1778] D loss: 1.4005, G loss: 0.8549\n",
      "[964/1778] D loss: 1.4844, G loss: 0.5858\n",
      "[1084/1778] D loss: 1.3840, G loss: 0.7599\n",
      "[1204/1778] D loss: 1.3821, G loss: 0.6841\n",
      "[1324/1778] D loss: 1.2831, G loss: 0.8288\n",
      "[1444/1778] D loss: 1.3839, G loss: 0.6145\n",
      "[1564/1778] D loss: 1.3375, G loss: 0.6925\n",
      "[1684/1778] D loss: 1.3402, G loss: 0.6137\n",
      "train error: \n",
      " D loss: 1.293619, G loss: 0.791889, D accuracy: 58.3%, cell accuracy: 21854.0%%, board accuracy: 56.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.285531, G loss: 0.810820, D accuracy: 58.3%, cell accuracy: 21844.1%%, board accuracy: 55.2% \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3672, G loss: 0.7558\n",
      "[124/1778] D loss: 1.0785, G loss: 0.7997\n",
      "[244/1778] D loss: 1.3855, G loss: 0.6367\n",
      "[364/1778] D loss: 1.0664, G loss: 1.0003\n",
      "[484/1778] D loss: 1.2452, G loss: 0.6415\n",
      "[604/1778] D loss: 1.2596, G loss: 0.7800\n",
      "[724/1778] D loss: 1.3561, G loss: 0.7269\n",
      "[844/1778] D loss: 1.0759, G loss: 0.8414\n",
      "[964/1778] D loss: 1.4104, G loss: 0.6388\n",
      "[1084/1778] D loss: 1.2356, G loss: 0.7999\n",
      "[1204/1778] D loss: 1.0872, G loss: 0.9931\n",
      "[1324/1778] D loss: 1.3909, G loss: 0.6638\n",
      "[1444/1778] D loss: 1.3812, G loss: 0.6611\n",
      "[1564/1778] D loss: 1.4023, G loss: 0.7390\n",
      "[1684/1778] D loss: 1.2511, G loss: 0.8447\n",
      "train error: \n",
      " D loss: 1.339417, G loss: 0.658335, D accuracy: 54.3%, cell accuracy: 21948.7%%, board accuracy: 84.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.324013, G loss: 0.681478, D accuracy: 55.1%, cell accuracy: 21945.3%%, board accuracy: 83.3% \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4411, G loss: 0.5121\n",
      "[124/1778] D loss: 1.0601, G loss: 1.0681\n",
      "[244/1778] D loss: 1.3942, G loss: 0.6753\n",
      "[364/1778] D loss: 1.1430, G loss: 0.8776\n",
      "[484/1778] D loss: 1.3903, G loss: 0.7835\n",
      "[604/1778] D loss: 1.1376, G loss: 0.9343\n",
      "[724/1778] D loss: 1.3954, G loss: 0.6145\n",
      "[844/1778] D loss: 1.3822, G loss: 0.7502\n",
      "[964/1778] D loss: 1.4092, G loss: 0.7033\n",
      "[1084/1778] D loss: 1.3851, G loss: 0.6030\n",
      "[1204/1778] D loss: 1.4145, G loss: 0.6415\n",
      "[1324/1778] D loss: 1.4015, G loss: 0.7827\n",
      "[1444/1778] D loss: 1.3797, G loss: 0.6759\n",
      "[1564/1778] D loss: 1.3896, G loss: 0.7446\n",
      "[1684/1778] D loss: 1.3949, G loss: 0.6468\n",
      "train error: \n",
      " D loss: 1.310461, G loss: 0.825994, D accuracy: 54.5%, cell accuracy: 21956.3%%, board accuracy: 88.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.298630, G loss: 0.855793, D accuracy: 55.0%, cell accuracy: 21946.8%%, board accuracy: 85.8% \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4013, G loss: 0.7689\n",
      "[124/1778] D loss: 1.3962, G loss: 0.6691\n",
      "[244/1778] D loss: 1.4128, G loss: 0.8366\n",
      "[364/1778] D loss: 1.4155, G loss: 0.8109\n",
      "[484/1778] D loss: 0.8198, G loss: 1.2080\n",
      "[604/1778] D loss: 1.3855, G loss: 0.7036\n",
      "[724/1778] D loss: 1.3970, G loss: 0.7482\n",
      "[844/1778] D loss: 1.1134, G loss: 0.9005\n",
      "[964/1778] D loss: 1.0894, G loss: 1.1013\n",
      "[1084/1778] D loss: 1.4305, G loss: 0.8516\n",
      "[1204/1778] D loss: 1.3914, G loss: 0.7689\n",
      "[1324/1778] D loss: 1.3923, G loss: 0.6664\n",
      "[1444/1778] D loss: 1.1469, G loss: 0.7710\n",
      "[1564/1778] D loss: 1.1310, G loss: 0.8406\n",
      "[1684/1778] D loss: 1.3912, G loss: 0.6802\n",
      "train error: \n",
      " D loss: 1.330545, G loss: 0.760121, D accuracy: 54.7%, cell accuracy: 21942.1%%, board accuracy: 82.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.317122, G loss: 0.802362, D accuracy: 55.7%, cell accuracy: 21926.6%%, board accuracy: 78.2% \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3861, G loss: 0.7215\n",
      "[124/1778] D loss: 1.3965, G loss: 0.7128\n",
      "[244/1778] D loss: 1.2351, G loss: 0.6807\n",
      "[364/1778] D loss: 1.0757, G loss: 0.8188\n",
      "[484/1778] D loss: 1.3764, G loss: 0.6894\n",
      "[604/1778] D loss: 1.3912, G loss: 0.7773\n",
      "[724/1778] D loss: 1.3838, G loss: 0.6959\n",
      "[844/1778] D loss: 1.3830, G loss: 0.7665\n",
      "[964/1778] D loss: 1.2148, G loss: 0.7949\n",
      "[1084/1778] D loss: 1.1851, G loss: 0.8060\n",
      "[1204/1778] D loss: 1.3877, G loss: 0.7188\n",
      "[1324/1778] D loss: 1.3913, G loss: 0.7124\n",
      "[1444/1778] D loss: 1.3890, G loss: 0.6748\n",
      "[1564/1778] D loss: 1.4044, G loss: 0.6985\n",
      "[1684/1778] D loss: 1.3913, G loss: 0.6459\n",
      "train error: \n",
      " D loss: 1.325476, G loss: 0.788759, D accuracy: 53.7%, cell accuracy: 21951.3%%, board accuracy: 86.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.306135, G loss: 0.822857, D accuracy: 55.1%, cell accuracy: 21947.7%%, board accuracy: 85.4% \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3958, G loss: 0.7457\n",
      "[124/1778] D loss: 1.5239, G loss: 0.7816\n",
      "[244/1778] D loss: 1.1199, G loss: 0.7740\n",
      "[364/1778] D loss: 1.4034, G loss: 0.6326\n",
      "[484/1778] D loss: 1.8553, G loss: 0.5116\n",
      "[604/1778] D loss: 1.1239, G loss: 0.9273\n",
      "[724/1778] D loss: 1.0530, G loss: 1.0319\n",
      "[844/1778] D loss: 1.4092, G loss: 0.7053\n",
      "[964/1778] D loss: 1.4074, G loss: 0.6481\n",
      "[1084/1778] D loss: 1.2398, G loss: 0.9871\n",
      "[1204/1778] D loss: 1.3531, G loss: 0.6753\n",
      "[1324/1778] D loss: 1.3894, G loss: 0.7040\n",
      "[1444/1778] D loss: 1.2231, G loss: 0.6842\n",
      "[1564/1778] D loss: 1.3337, G loss: 0.7325\n",
      "[1684/1778] D loss: 1.4221, G loss: 0.6837\n",
      "train error: \n",
      " D loss: 1.336947, G loss: 0.674115, D accuracy: 54.4%, cell accuracy: 21947.9%%, board accuracy: 86.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.313055, G loss: 0.712796, D accuracy: 55.2%, cell accuracy: 21945.3%%, board accuracy: 86.0% \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3950, G loss: 0.6387\n",
      "[124/1778] D loss: 1.3909, G loss: 0.7282\n",
      "[244/1778] D loss: 1.1741, G loss: 0.9357\n",
      "[364/1778] D loss: 1.3903, G loss: 0.6658\n",
      "[484/1778] D loss: 1.3986, G loss: 0.6677\n",
      "[604/1778] D loss: 1.3900, G loss: 0.7347\n",
      "[724/1778] D loss: 1.3890, G loss: 0.6402\n",
      "[844/1778] D loss: 1.4124, G loss: 0.7815\n",
      "[964/1778] D loss: 0.9619, G loss: 1.0588\n",
      "[1084/1778] D loss: 1.3944, G loss: 0.6130\n",
      "[1204/1778] D loss: 1.3895, G loss: 0.6972\n",
      "[1324/1778] D loss: 1.1622, G loss: 0.7575\n",
      "[1444/1778] D loss: 0.9684, G loss: 0.9951\n",
      "[1564/1778] D loss: 1.1271, G loss: 0.9167\n",
      "[1684/1778] D loss: 1.3869, G loss: 0.7974\n",
      "train error: \n",
      " D loss: 1.332519, G loss: 0.953931, D accuracy: 53.8%, cell accuracy: 21955.2%%, board accuracy: 88.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.308084, G loss: 0.996756, D accuracy: 54.7%, cell accuracy: 21943.2%%, board accuracy: 86.7% \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4194, G loss: 0.8252\n",
      "[124/1778] D loss: 1.3873, G loss: 0.6607\n",
      "[244/1778] D loss: 1.3976, G loss: 0.6130\n",
      "[364/1778] D loss: 1.3917, G loss: 0.6986\n",
      "[484/1778] D loss: 1.3915, G loss: 0.7168\n",
      "[604/1778] D loss: 1.3872, G loss: 0.6815\n",
      "[724/1778] D loss: 1.3748, G loss: 0.7165\n",
      "[844/1778] D loss: 1.1278, G loss: 0.9591\n",
      "[964/1778] D loss: 1.1441, G loss: 0.8613\n",
      "[1084/1778] D loss: 1.4511, G loss: 0.6862\n",
      "[1204/1778] D loss: 1.3916, G loss: 0.7592\n",
      "[1324/1778] D loss: 1.4137, G loss: 0.8297\n",
      "[1444/1778] D loss: 1.4137, G loss: 0.5959\n",
      "[1564/1778] D loss: 1.1228, G loss: 0.9093\n",
      "[1684/1778] D loss: 1.4055, G loss: 0.6624\n",
      "train error: \n",
      " D loss: 1.323167, G loss: 0.740049, D accuracy: 54.8%, cell accuracy: 21943.4%%, board accuracy: 86.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.298062, G loss: 0.771717, D accuracy: 55.4%, cell accuracy: 21932.2%%, board accuracy: 84.0% \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3889, G loss: 0.6770\n",
      "[124/1778] D loss: 1.3887, G loss: 0.6436\n",
      "[244/1778] D loss: 1.3885, G loss: 0.7404\n",
      "[364/1778] D loss: 1.1161, G loss: 1.0587\n",
      "[484/1778] D loss: 1.3812, G loss: 0.7256\n",
      "[604/1778] D loss: 1.0778, G loss: 1.0962\n",
      "[724/1778] D loss: 1.1228, G loss: 0.9066\n",
      "[844/1778] D loss: 1.3888, G loss: 0.6480\n",
      "[964/1778] D loss: 1.3600, G loss: 0.7315\n",
      "[1084/1778] D loss: 1.3871, G loss: 0.7267\n",
      "[1204/1778] D loss: 1.0618, G loss: 1.1726\n",
      "[1324/1778] D loss: 1.1349, G loss: 0.9084\n",
      "[1444/1778] D loss: 1.1329, G loss: 0.7721\n",
      "[1564/1778] D loss: 1.3779, G loss: 0.7180\n",
      "[1684/1778] D loss: 1.3894, G loss: 0.7269\n",
      "train error: \n",
      " D loss: 1.328818, G loss: 0.622878, D accuracy: 54.7%, cell accuracy: 21939.6%%, board accuracy: 84.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.306229, G loss: 0.658707, D accuracy: 55.6%, cell accuracy: 21931.1%%, board accuracy: 82.9% \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4230, G loss: 0.5495\n",
      "[124/1778] D loss: 1.0564, G loss: 1.4978\n",
      "[244/1778] D loss: 1.3680, G loss: 0.6154\n",
      "[364/1778] D loss: 1.3768, G loss: 0.7149\n",
      "[484/1778] D loss: 1.3790, G loss: 0.6097\n",
      "[604/1778] D loss: 0.8190, G loss: 1.6324\n",
      "[724/1778] D loss: 1.3379, G loss: 0.8469\n",
      "[844/1778] D loss: 1.4035, G loss: 0.8422\n",
      "[964/1778] D loss: 1.3936, G loss: 0.6693\n",
      "[1084/1778] D loss: 1.4797, G loss: 0.7966\n",
      "[1204/1778] D loss: 1.3989, G loss: 0.6259\n",
      "[1324/1778] D loss: 1.0858, G loss: 0.9758\n",
      "[1444/1778] D loss: 1.4006, G loss: 0.7633\n",
      "[1564/1778] D loss: 1.0880, G loss: 1.4197\n",
      "[1684/1778] D loss: 1.4022, G loss: 0.6894\n",
      "train error: \n",
      " D loss: 1.300894, G loss: 0.848787, D accuracy: 54.9%, cell accuracy: 21934.4%%, board accuracy: 84.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.285360, G loss: 0.880914, D accuracy: 56.0%, cell accuracy: 21922.5%%, board accuracy: 83.8% \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3875, G loss: 0.7213\n",
      "[124/1778] D loss: 1.3828, G loss: 0.7116\n",
      "[244/1778] D loss: 1.3900, G loss: 0.7173\n",
      "[364/1778] D loss: 1.1418, G loss: 0.8178\n",
      "[484/1778] D loss: 1.3883, G loss: 0.6759\n",
      "[604/1778] D loss: 1.3954, G loss: 0.7161\n",
      "[724/1778] D loss: 1.3928, G loss: 0.6179\n",
      "[844/1778] D loss: 1.4050, G loss: 0.7516\n",
      "[964/1778] D loss: 1.3879, G loss: 0.6955\n",
      "[1084/1778] D loss: 1.1220, G loss: 0.9258\n",
      "[1204/1778] D loss: 1.4105, G loss: 0.5255\n",
      "[1324/1778] D loss: 1.1222, G loss: 0.8396\n",
      "[1444/1778] D loss: 1.5438, G loss: 0.5844\n",
      "[1564/1778] D loss: 0.8759, G loss: 1.1271\n",
      "[1684/1778] D loss: 1.3940, G loss: 0.6991\n",
      "train error: \n",
      " D loss: 1.308330, G loss: 0.729047, D accuracy: 54.6%, cell accuracy: 21941.1%%, board accuracy: 87.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.290295, G loss: 0.763431, D accuracy: 55.6%, cell accuracy: 21928.6%%, board accuracy: 85.6% \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3879, G loss: 0.7092\n",
      "[124/1778] D loss: 1.3792, G loss: 0.7463\n",
      "[244/1778] D loss: 1.3945, G loss: 0.7208\n",
      "[364/1778] D loss: 1.3862, G loss: 0.7493\n",
      "[484/1778] D loss: 1.3389, G loss: 0.8076\n",
      "[604/1778] D loss: 0.9745, G loss: 1.2397\n",
      "[724/1778] D loss: 1.1087, G loss: 0.9319\n",
      "[844/1778] D loss: 1.3988, G loss: 0.8432\n",
      "[964/1778] D loss: 1.1264, G loss: 0.8285\n",
      "[1084/1778] D loss: 1.0936, G loss: 0.9239\n",
      "[1204/1778] D loss: 1.0995, G loss: 1.0189\n",
      "[1324/1778] D loss: 1.3909, G loss: 0.7275\n",
      "[1444/1778] D loss: 1.3902, G loss: 0.7122\n",
      "[1564/1778] D loss: 1.0142, G loss: 0.9339\n",
      "[1684/1778] D loss: 1.0083, G loss: 1.0559\n",
      "train error: \n",
      " D loss: 1.306641, G loss: 0.797935, D accuracy: 54.9%, cell accuracy: 21929.2%%, board accuracy: 82.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.286272, G loss: 0.834953, D accuracy: 55.5%, cell accuracy: 21916.7%%, board accuracy: 81.5% \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1054, G loss: 0.9173\n",
      "[124/1778] D loss: 1.4078, G loss: 0.6719\n",
      "[244/1778] D loss: 1.3822, G loss: 0.6900\n",
      "[364/1778] D loss: 1.1108, G loss: 0.8632\n",
      "[484/1778] D loss: 1.3946, G loss: 0.7073\n",
      "[604/1778] D loss: 1.4099, G loss: 0.5572\n",
      "[724/1778] D loss: 1.3568, G loss: 0.6242\n",
      "[844/1778] D loss: 1.1253, G loss: 0.8487\n",
      "[964/1778] D loss: 1.3852, G loss: 0.6691\n",
      "[1084/1778] D loss: 0.7727, G loss: 1.4517\n",
      "[1204/1778] D loss: 1.3551, G loss: 0.7507\n",
      "[1324/1778] D loss: 1.1379, G loss: 0.7558\n",
      "[1444/1778] D loss: 1.3667, G loss: 0.6218\n",
      "[1564/1778] D loss: 1.3891, G loss: 0.7693\n",
      "[1684/1778] D loss: 1.4208, G loss: 0.6907\n",
      "train error: \n",
      " D loss: 1.306209, G loss: 0.757934, D accuracy: 55.0%, cell accuracy: 21931.9%%, board accuracy: 84.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.293443, G loss: 0.778419, D accuracy: 55.3%, cell accuracy: 21919.8%%, board accuracy: 82.7% \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2230, G loss: 0.7702\n",
      "[124/1778] D loss: 1.1041, G loss: 0.8036\n",
      "[244/1778] D loss: 1.1244, G loss: 0.8026\n",
      "[364/1778] D loss: 1.3741, G loss: 0.7853\n",
      "[484/1778] D loss: 1.3788, G loss: 0.7462\n",
      "[604/1778] D loss: 1.3917, G loss: 0.6235\n",
      "[724/1778] D loss: 1.3889, G loss: 0.7352\n",
      "[844/1778] D loss: 1.3856, G loss: 0.6922\n",
      "[964/1778] D loss: 1.3641, G loss: 0.8100\n",
      "[1084/1778] D loss: 1.3236, G loss: 0.8248\n",
      "[1204/1778] D loss: 2.1431, G loss: 0.4717\n",
      "[1324/1778] D loss: 1.2495, G loss: 1.2661\n",
      "[1444/1778] D loss: 1.1498, G loss: 0.8605\n",
      "[1564/1778] D loss: 1.3889, G loss: 0.6928\n",
      "[1684/1778] D loss: 1.1225, G loss: 1.0870\n",
      "train error: \n",
      " D loss: 1.306222, G loss: 0.786747, D accuracy: 54.7%, cell accuracy: 21933.1%%, board accuracy: 85.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.291067, G loss: 0.820850, D accuracy: 55.9%, cell accuracy: 21922.3%%, board accuracy: 82.9% \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1235, G loss: 0.9191\n",
      "[124/1778] D loss: 1.3875, G loss: 0.6942\n",
      "[244/1778] D loss: 1.3960, G loss: 0.6276\n",
      "[364/1778] D loss: 1.4024, G loss: 0.7284\n",
      "[484/1778] D loss: 1.0948, G loss: 0.9600\n",
      "[604/1778] D loss: 1.3931, G loss: 0.8208\n",
      "[724/1778] D loss: 1.3635, G loss: 0.7794\n",
      "[844/1778] D loss: 1.1438, G loss: 0.7850\n",
      "[964/1778] D loss: 1.4084, G loss: 0.5920\n",
      "[1084/1778] D loss: 1.3885, G loss: 0.7067\n",
      "[1204/1778] D loss: 1.1350, G loss: 0.8870\n",
      "[1324/1778] D loss: 1.1164, G loss: 0.9103\n",
      "[1444/1778] D loss: 1.2192, G loss: 1.2359\n",
      "[1564/1778] D loss: 1.4088, G loss: 0.8209\n",
      "[1684/1778] D loss: 1.3887, G loss: 0.7285\n",
      "train error: \n",
      " D loss: 1.311642, G loss: 0.829691, D accuracy: 54.2%, cell accuracy: 21933.6%%, board accuracy: 86.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.295287, G loss: 0.862448, D accuracy: 55.6%, cell accuracy: 21921.6%%, board accuracy: 84.9% \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4788, G loss: 0.7907\n",
      "[124/1778] D loss: 1.3909, G loss: 0.7701\n",
      "[244/1778] D loss: 1.3975, G loss: 0.7799\n",
      "[364/1778] D loss: 1.1076, G loss: 1.0024\n",
      "[484/1778] D loss: 1.3868, G loss: 0.6821\n",
      "[604/1778] D loss: 1.3892, G loss: 0.6555\n",
      "[724/1778] D loss: 1.3872, G loss: 0.7276\n",
      "[844/1778] D loss: 1.3929, G loss: 0.7486\n",
      "[964/1778] D loss: 1.1069, G loss: 0.9413\n",
      "[1084/1778] D loss: 1.3536, G loss: 0.7897\n",
      "[1204/1778] D loss: 1.4707, G loss: 0.8857\n",
      "[1324/1778] D loss: 1.3881, G loss: 0.6277\n",
      "[1444/1778] D loss: 1.1284, G loss: 0.8416\n",
      "[1564/1778] D loss: 1.3792, G loss: 0.7644\n",
      "[1684/1778] D loss: 1.3863, G loss: 0.6835\n",
      "train error: \n",
      " D loss: 1.311781, G loss: 0.707362, D accuracy: 54.2%, cell accuracy: 21937.2%%, board accuracy: 86.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.292638, G loss: 0.742062, D accuracy: 55.3%, cell accuracy: 21925.5%%, board accuracy: 86.5% \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3915, G loss: 0.6284\n",
      "[124/1778] D loss: 1.3909, G loss: 0.6927\n",
      "[244/1778] D loss: 1.3903, G loss: 0.6727\n",
      "[364/1778] D loss: 1.2477, G loss: 0.9710\n",
      "[484/1778] D loss: 1.0950, G loss: 0.9168\n",
      "[604/1778] D loss: 1.1188, G loss: 0.8213\n",
      "[724/1778] D loss: 1.0899, G loss: 1.1094\n",
      "[844/1778] D loss: 1.3912, G loss: 0.6999\n",
      "[964/1778] D loss: 1.0562, G loss: 1.2457\n",
      "[1084/1778] D loss: 1.4096, G loss: 0.7064\n",
      "[1204/1778] D loss: 1.1114, G loss: 0.9333\n",
      "[1324/1778] D loss: 1.4054, G loss: 0.7942\n",
      "[1444/1778] D loss: 1.3803, G loss: 0.6798\n",
      "[1564/1778] D loss: 1.3872, G loss: 0.7484\n",
      "[1684/1778] D loss: 1.0854, G loss: 1.0315\n",
      "train error: \n",
      " D loss: 1.306867, G loss: 0.751939, D accuracy: 54.2%, cell accuracy: 21938.2%%, board accuracy: 87.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.289564, G loss: 0.780492, D accuracy: 55.3%, cell accuracy: 21924.3%%, board accuracy: 85.6% \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3938, G loss: 0.7323\n",
      "[124/1778] D loss: 0.8032, G loss: 1.2044\n",
      "[244/1778] D loss: 1.3972, G loss: 0.7829\n",
      "[364/1778] D loss: 1.4505, G loss: 0.8624\n",
      "[484/1778] D loss: 1.3891, G loss: 0.7091\n",
      "[604/1778] D loss: 1.3901, G loss: 0.6406\n",
      "[724/1778] D loss: 1.1503, G loss: 0.7990\n",
      "[844/1778] D loss: 1.3252, G loss: 0.7282\n",
      "[964/1778] D loss: 1.3970, G loss: 0.8254\n",
      "[1084/1778] D loss: 1.3906, G loss: 0.7099\n",
      "[1204/1778] D loss: 1.0385, G loss: 1.7103\n",
      "[1324/1778] D loss: 1.0825, G loss: 1.0141\n",
      "[1444/1778] D loss: 1.2915, G loss: 0.6253\n",
      "[1564/1778] D loss: 1.4071, G loss: 0.8052\n",
      "[1684/1778] D loss: 1.1016, G loss: 0.9481\n",
      "train error: \n",
      " D loss: 1.299726, G loss: 0.811946, D accuracy: 54.9%, cell accuracy: 21925.8%%, board accuracy: 86.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.287187, G loss: 0.840817, D accuracy: 55.1%, cell accuracy: 21914.0%%, board accuracy: 84.9% \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3903, G loss: 0.7606\n",
      "[124/1778] D loss: 1.3969, G loss: 0.6784\n",
      "[244/1778] D loss: 1.3979, G loss: 0.7410\n",
      "[364/1778] D loss: 1.4053, G loss: 0.6274\n",
      "[484/1778] D loss: 1.0933, G loss: 0.9812\n",
      "[604/1778] D loss: 1.3998, G loss: 0.7257\n",
      "[724/1778] D loss: 1.3937, G loss: 0.6754\n",
      "[844/1778] D loss: 1.0906, G loss: 1.0464\n",
      "[964/1778] D loss: 1.0782, G loss: 0.9415\n",
      "[1084/1778] D loss: 1.3922, G loss: 0.7372\n",
      "[1204/1778] D loss: 1.3689, G loss: 0.7276\n",
      "[1324/1778] D loss: 1.3930, G loss: 0.7320\n",
      "[1444/1778] D loss: 1.0821, G loss: 1.0813\n",
      "[1564/1778] D loss: 1.0572, G loss: 1.1557\n",
      "[1684/1778] D loss: 1.4806, G loss: 0.8552\n",
      "train error: \n",
      " D loss: 1.314634, G loss: 0.709533, D accuracy: 54.1%, cell accuracy: 21933.5%%, board accuracy: 86.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.291454, G loss: 0.744907, D accuracy: 55.4%, cell accuracy: 21921.6%%, board accuracy: 85.1% \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3913, G loss: 0.6156\n",
      "[124/1778] D loss: 1.3854, G loss: 0.7169\n",
      "[244/1778] D loss: 1.3979, G loss: 0.6863\n",
      "[364/1778] D loss: 1.4171, G loss: 0.8565\n",
      "[484/1778] D loss: 1.3982, G loss: 0.5852\n",
      "[604/1778] D loss: 1.3877, G loss: 0.7049\n",
      "[724/1778] D loss: 1.4267, G loss: 0.7231\n",
      "[844/1778] D loss: 1.3894, G loss: 0.7016\n",
      "[964/1778] D loss: 1.3937, G loss: 0.6226\n",
      "[1084/1778] D loss: 1.4231, G loss: 0.8746\n",
      "[1204/1778] D loss: 1.0836, G loss: 1.0473\n",
      "[1324/1778] D loss: 1.3581, G loss: 0.8540\n",
      "[1444/1778] D loss: 1.0894, G loss: 1.0919\n",
      "[1564/1778] D loss: 1.4057, G loss: 0.7074\n",
      "[1684/1778] D loss: 1.1104, G loss: 0.9438\n",
      "train error: \n",
      " D loss: 1.306728, G loss: 0.772051, D accuracy: 54.3%, cell accuracy: 21934.3%%, board accuracy: 88.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.286409, G loss: 0.809142, D accuracy: 55.5%, cell accuracy: 21919.8%%, board accuracy: 84.0% \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3917, G loss: 0.6930\n",
      "[124/1778] D loss: 1.1216, G loss: 0.9282\n",
      "[244/1778] D loss: 1.3896, G loss: 0.6628\n",
      "[364/1778] D loss: 1.1386, G loss: 0.8275\n",
      "[484/1778] D loss: 1.4657, G loss: 0.5981\n",
      "[604/1778] D loss: 1.3726, G loss: 0.6947\n",
      "[724/1778] D loss: 1.3942, G loss: 0.6446\n",
      "[844/1778] D loss: 1.0875, G loss: 1.0220\n",
      "[964/1778] D loss: 1.0979, G loss: 0.9062\n",
      "[1084/1778] D loss: 1.1413, G loss: 0.7455\n",
      "[1204/1778] D loss: 1.3918, G loss: 0.7725\n",
      "[1324/1778] D loss: 1.1070, G loss: 0.9887\n",
      "[1444/1778] D loss: 1.4172, G loss: 0.5707\n",
      "[1564/1778] D loss: 1.3930, G loss: 0.6892\n",
      "[1684/1778] D loss: 1.1492, G loss: 0.7548\n",
      "train error: \n",
      " D loss: 1.310565, G loss: 0.716765, D accuracy: 53.9%, cell accuracy: 21941.2%%, board accuracy: 87.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.298463, G loss: 0.750353, D accuracy: 54.5%, cell accuracy: 21928.6%%, board accuracy: 84.9% \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3595, G loss: 0.6822\n",
      "[124/1778] D loss: 1.4016, G loss: 0.7023\n",
      "[244/1778] D loss: 1.4051, G loss: 0.7483\n",
      "[364/1778] D loss: 1.4169, G loss: 0.7956\n",
      "[484/1778] D loss: 1.1038, G loss: 0.8480\n",
      "[604/1778] D loss: 1.3880, G loss: 0.7008\n",
      "[724/1778] D loss: 1.3899, G loss: 0.6815\n",
      "[844/1778] D loss: 1.3889, G loss: 0.6614\n",
      "[964/1778] D loss: 1.3774, G loss: 0.6661\n",
      "[1084/1778] D loss: 1.4043, G loss: 0.6883\n",
      "[1204/1778] D loss: 1.3935, G loss: 0.7230\n",
      "[1324/1778] D loss: 1.3885, G loss: 0.7492\n",
      "[1444/1778] D loss: 1.0836, G loss: 1.0982\n",
      "[1564/1778] D loss: 1.4409, G loss: 0.8641\n",
      "[1684/1778] D loss: 1.3941, G loss: 0.7200\n",
      "train error: \n",
      " D loss: 1.306899, G loss: 0.852770, D accuracy: 53.9%, cell accuracy: 21930.4%%, board accuracy: 85.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.292047, G loss: 0.891769, D accuracy: 55.1%, cell accuracy: 21917.3%%, board accuracy: 84.9% \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0782, G loss: 1.1298\n",
      "[124/1778] D loss: 1.3476, G loss: 0.8172\n",
      "[244/1778] D loss: 1.3877, G loss: 0.6704\n",
      "[364/1778] D loss: 1.3980, G loss: 0.6092\n",
      "[484/1778] D loss: 1.3890, G loss: 0.7154\n",
      "[604/1778] D loss: 1.3983, G loss: 0.6584\n",
      "[724/1778] D loss: 1.4262, G loss: 0.8381\n",
      "[844/1778] D loss: 1.4674, G loss: 1.0389\n",
      "[964/1778] D loss: 1.3993, G loss: 0.7881\n",
      "[1084/1778] D loss: 1.3954, G loss: 0.6926\n",
      "[1204/1778] D loss: 1.0600, G loss: 1.2478\n",
      "[1324/1778] D loss: 1.3914, G loss: 0.6827\n",
      "[1444/1778] D loss: 1.0992, G loss: 0.9496\n",
      "[1564/1778] D loss: 1.3896, G loss: 0.6934\n",
      "[1684/1778] D loss: 1.3704, G loss: 0.7857\n",
      "train error: \n",
      " D loss: 1.301027, G loss: 0.832476, D accuracy: 54.4%, cell accuracy: 21923.4%%, board accuracy: 85.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.285370, G loss: 0.866797, D accuracy: 55.3%, cell accuracy: 21905.2%%, board accuracy: 82.2% \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3812, G loss: 0.6649\n",
      "[124/1778] D loss: 1.1312, G loss: 0.8954\n",
      "[244/1778] D loss: 1.3676, G loss: 0.7237\n",
      "[364/1778] D loss: 0.7928, G loss: 1.2852\n",
      "[484/1778] D loss: 1.0676, G loss: 1.2255\n",
      "[604/1778] D loss: 1.3923, G loss: 0.6819\n",
      "[724/1778] D loss: 0.8248, G loss: 1.1828\n",
      "[844/1778] D loss: 1.4530, G loss: 0.7188\n",
      "[964/1778] D loss: 1.4063, G loss: 0.6579\n",
      "[1084/1778] D loss: 1.3920, G loss: 0.6616\n",
      "[1204/1778] D loss: 1.1543, G loss: 0.7029\n",
      "[1324/1778] D loss: 1.3599, G loss: 0.7303\n",
      "[1444/1778] D loss: 1.3902, G loss: 0.6435\n",
      "[1564/1778] D loss: 1.1370, G loss: 0.8265\n",
      "[1684/1778] D loss: 1.3910, G loss: 0.6808\n",
      "train error: \n",
      " D loss: 1.309950, G loss: 0.712504, D accuracy: 54.0%, cell accuracy: 21949.5%%, board accuracy: 88.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.291022, G loss: 0.753357, D accuracy: 55.0%, cell accuracy: 21938.7%%, board accuracy: 87.6% \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4713, G loss: 0.7695\n",
      "[124/1778] D loss: 1.4155, G loss: 0.5697\n",
      "[244/1778] D loss: 1.3882, G loss: 0.7224\n",
      "[364/1778] D loss: 1.4029, G loss: 0.7304\n",
      "[484/1778] D loss: 1.3879, G loss: 0.7267\n",
      "[604/1778] D loss: 0.7482, G loss: 1.7589\n",
      "[724/1778] D loss: 1.3932, G loss: 0.8530\n",
      "[844/1778] D loss: 0.7513, G loss: 1.3990\n",
      "[964/1778] D loss: 1.3464, G loss: 0.7458\n",
      "[1084/1778] D loss: 1.3323, G loss: 0.7881\n",
      "[1204/1778] D loss: 1.4047, G loss: 0.6120\n",
      "[1324/1778] D loss: 1.0504, G loss: 1.4232\n",
      "[1444/1778] D loss: 1.0870, G loss: 1.0324\n",
      "[1564/1778] D loss: 1.2404, G loss: 1.1908\n",
      "[1684/1778] D loss: 1.3402, G loss: 0.8239\n",
      "train error: \n",
      " D loss: 1.267278, G loss: 0.914599, D accuracy: 57.2%, cell accuracy: 21842.5%%, board accuracy: 61.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.251779, G loss: 0.950860, D accuracy: 59.2%, cell accuracy: 21810.4%%, board accuracy: 59.7% \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1213, G loss: 0.9842\n",
      "[124/1778] D loss: 0.7373, G loss: 1.1412\n",
      "[244/1778] D loss: 1.1420, G loss: 0.8765\n",
      "[364/1778] D loss: 1.2746, G loss: 0.8173\n",
      "[484/1778] D loss: 1.1935, G loss: 0.6605\n",
      "[604/1778] D loss: 1.2975, G loss: 0.6526\n",
      "[724/1778] D loss: 1.3550, G loss: 0.7463\n",
      "[844/1778] D loss: 1.4007, G loss: 0.7907\n",
      "[964/1778] D loss: 1.3992, G loss: 0.6156\n",
      "[1084/1778] D loss: 1.4094, G loss: 0.8099\n",
      "[1204/1778] D loss: 1.3970, G loss: 0.7305\n",
      "[1324/1778] D loss: 1.4874, G loss: 0.9146\n",
      "[1444/1778] D loss: 1.0895, G loss: 0.9596\n",
      "[1564/1778] D loss: 1.1510, G loss: 0.7170\n",
      "[1684/1778] D loss: 1.4023, G loss: 0.6047\n",
      "train error: \n",
      " D loss: 1.310709, G loss: 0.729139, D accuracy: 54.2%, cell accuracy: 21937.9%%, board accuracy: 83.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.291407, G loss: 0.765768, D accuracy: 54.8%, cell accuracy: 21926.4%%, board accuracy: 82.2% \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0599, G loss: 1.1333\n",
      "[124/1778] D loss: 1.3906, G loss: 0.6604\n",
      "[244/1778] D loss: 1.4186, G loss: 0.8069\n",
      "[364/1778] D loss: 1.0971, G loss: 1.0368\n",
      "[484/1778] D loss: 1.4024, G loss: 0.7009\n",
      "[604/1778] D loss: 1.3867, G loss: 0.6757\n",
      "[724/1778] D loss: 1.0962, G loss: 1.0602\n",
      "[844/1778] D loss: 1.4126, G loss: 0.6480\n",
      "[964/1778] D loss: 0.7659, G loss: 1.4810\n",
      "[1084/1778] D loss: 1.1154, G loss: 0.7791\n",
      "[1204/1778] D loss: 1.3823, G loss: 0.6600\n",
      "[1324/1778] D loss: 1.3875, G loss: 0.6773\n",
      "[1444/1778] D loss: 1.3822, G loss: 0.9108\n",
      "[1564/1778] D loss: 1.0742, G loss: 0.9785\n",
      "[1684/1778] D loss: 1.4076, G loss: 0.6746\n",
      "train error: \n",
      " D loss: 1.304148, G loss: 0.794987, D accuracy: 54.3%, cell accuracy: 21924.5%%, board accuracy: 83.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.288610, G loss: 0.823744, D accuracy: 55.4%, cell accuracy: 21909.5%%, board accuracy: 82.0% \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "[4/1778] D loss: 0.8277, G loss: 1.1657\n",
      "[124/1778] D loss: 1.4407, G loss: 0.9244\n",
      "[244/1778] D loss: 1.1491, G loss: 0.7871\n",
      "[364/1778] D loss: 1.1134, G loss: 0.8612\n",
      "[484/1778] D loss: 1.3267, G loss: 0.6372\n",
      "[604/1778] D loss: 1.0980, G loss: 1.0355\n",
      "[724/1778] D loss: 1.1575, G loss: 0.9797\n",
      "[844/1778] D loss: 1.0950, G loss: 0.9323\n",
      "[964/1778] D loss: 1.3807, G loss: 0.6069\n",
      "[1084/1778] D loss: 1.4138, G loss: 0.7754\n",
      "[1204/1778] D loss: 1.3937, G loss: 0.7078\n",
      "[1324/1778] D loss: 1.3764, G loss: 0.6580\n",
      "[1444/1778] D loss: 1.3889, G loss: 0.7730\n",
      "[1564/1778] D loss: 1.3905, G loss: 0.7394\n",
      "[1684/1778] D loss: 1.4170, G loss: 0.6636\n",
      "train error: \n",
      " D loss: 1.304956, G loss: 0.832030, D accuracy: 53.9%, cell accuracy: 21946.7%%, board accuracy: 89.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.291188, G loss: 0.870911, D accuracy: 54.6%, cell accuracy: 21933.8%%, board accuracy: 86.5% \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3897, G loss: 0.6438\n",
      "[124/1778] D loss: 1.3628, G loss: 0.6625\n",
      "[244/1778] D loss: 1.3985, G loss: 0.5997\n",
      "[364/1778] D loss: 1.3875, G loss: 0.6509\n",
      "[484/1778] D loss: 1.3947, G loss: 0.7732\n",
      "[604/1778] D loss: 1.4022, G loss: 0.8270\n",
      "[724/1778] D loss: 1.3915, G loss: 0.6711\n",
      "[844/1778] D loss: 1.4033, G loss: 0.6171\n",
      "[964/1778] D loss: 0.7821, G loss: 1.3031\n",
      "[1084/1778] D loss: 1.3828, G loss: 0.7366\n",
      "[1204/1778] D loss: 1.3958, G loss: 0.6421\n",
      "[1324/1778] D loss: 1.1346, G loss: 1.1010\n",
      "[1444/1778] D loss: 1.0877, G loss: 0.9940\n",
      "[1564/1778] D loss: 1.3651, G loss: 0.6345\n",
      "[1684/1778] D loss: 1.3959, G loss: 0.6161\n",
      "train error: \n",
      " D loss: 1.319432, G loss: 0.647476, D accuracy: 55.4%, cell accuracy: 21920.0%%, board accuracy: 85.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.296642, G loss: 0.689422, D accuracy: 56.0%, cell accuracy: 21905.0%%, board accuracy: 83.6% \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4149, G loss: 0.5722\n",
      "[124/1778] D loss: 1.3949, G loss: 0.7242\n",
      "[244/1778] D loss: 1.3819, G loss: 0.6871\n",
      "[364/1778] D loss: 1.3902, G loss: 0.6251\n",
      "[484/1778] D loss: 1.3945, G loss: 0.7386\n",
      "[604/1778] D loss: 1.3913, G loss: 0.7706\n",
      "[724/1778] D loss: 1.3902, G loss: 0.6886\n",
      "[844/1778] D loss: 1.3948, G loss: 0.6821\n",
      "[964/1778] D loss: 1.3978, G loss: 0.6176\n",
      "[1084/1778] D loss: 1.3886, G loss: 0.7380\n",
      "[1204/1778] D loss: 1.3902, G loss: 0.6268\n",
      "[1324/1778] D loss: 1.4140, G loss: 0.5783\n",
      "[1444/1778] D loss: 1.0872, G loss: 1.0676\n",
      "[1564/1778] D loss: 1.3809, G loss: 0.7099\n",
      "[1684/1778] D loss: 1.4062, G loss: 0.6199\n",
      "train error: \n",
      " D loss: 1.304917, G loss: 0.730483, D accuracy: 54.3%, cell accuracy: 21933.2%%, board accuracy: 87.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.288014, G loss: 0.763852, D accuracy: 55.2%, cell accuracy: 21917.3%%, board accuracy: 85.1% \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4162, G loss: 0.6359\n",
      "[124/1778] D loss: 1.0997, G loss: 0.9225\n",
      "[244/1778] D loss: 1.0960, G loss: 1.0163\n",
      "[364/1778] D loss: 1.1022, G loss: 1.0009\n",
      "[484/1778] D loss: 1.3555, G loss: 0.8911\n",
      "[604/1778] D loss: 1.4724, G loss: 0.8713\n",
      "[724/1778] D loss: 1.4064, G loss: 0.8537\n",
      "[844/1778] D loss: 1.3934, G loss: 0.7776\n",
      "[964/1778] D loss: 1.4008, G loss: 0.7249\n",
      "[1084/1778] D loss: 1.0784, G loss: 1.1144\n",
      "[1204/1778] D loss: 1.3887, G loss: 0.7313\n",
      "[1324/1778] D loss: 1.3968, G loss: 0.7202\n",
      "[1444/1778] D loss: 1.3897, G loss: 0.6387\n",
      "[1564/1778] D loss: 1.1047, G loss: 1.2754\n",
      "[1684/1778] D loss: 0.7311, G loss: 1.6418\n",
      "train error: \n",
      " D loss: 1.306333, G loss: 0.751542, D accuracy: 54.1%, cell accuracy: 21939.1%%, board accuracy: 87.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.288650, G loss: 0.787745, D accuracy: 54.7%, cell accuracy: 21927.5%%, board accuracy: 86.5% \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0996, G loss: 0.9509\n",
      "[124/1778] D loss: 1.3874, G loss: 0.6978\n",
      "[244/1778] D loss: 1.4067, G loss: 0.5779\n",
      "[364/1778] D loss: 1.4066, G loss: 0.5816\n",
      "[484/1778] D loss: 1.1468, G loss: 0.7584\n",
      "[604/1778] D loss: 1.0948, G loss: 0.9029\n",
      "[724/1778] D loss: 1.4018, G loss: 0.8196\n",
      "[844/1778] D loss: 1.4143, G loss: 0.7430\n",
      "[964/1778] D loss: 1.0977, G loss: 0.9494\n",
      "[1084/1778] D loss: 1.4022, G loss: 0.8202\n",
      "[1204/1778] D loss: 1.1127, G loss: 0.8948\n",
      "[1324/1778] D loss: 1.0949, G loss: 0.9832\n",
      "[1444/1778] D loss: 1.1121, G loss: 1.0230\n",
      "[1564/1778] D loss: 1.3435, G loss: 1.0244\n",
      "[1684/1778] D loss: 0.8795, G loss: 1.0584\n",
      "train error: \n",
      " D loss: 1.308812, G loss: 0.848113, D accuracy: 54.1%, cell accuracy: 21933.1%%, board accuracy: 84.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.299247, G loss: 0.878989, D accuracy: 54.6%, cell accuracy: 21919.4%%, board accuracy: 83.3% \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1051, G loss: 0.9134\n",
      "[124/1778] D loss: 1.3981, G loss: 0.7491\n",
      "[244/1778] D loss: 1.1307, G loss: 1.0239\n",
      "[364/1778] D loss: 1.0924, G loss: 1.0734\n",
      "[484/1778] D loss: 1.0848, G loss: 1.0879\n",
      "[604/1778] D loss: 1.3982, G loss: 0.5661\n",
      "[724/1778] D loss: 1.3668, G loss: 0.6590\n",
      "[844/1778] D loss: 1.2975, G loss: 0.9174\n",
      "[964/1778] D loss: 1.0783, G loss: 1.0696\n",
      "[1084/1778] D loss: 1.0795, G loss: 0.9567\n",
      "[1204/1778] D loss: 1.3945, G loss: 0.6725\n",
      "[1324/1778] D loss: 1.3869, G loss: 0.7186\n",
      "[1444/1778] D loss: 1.4079, G loss: 0.8257\n",
      "[1564/1778] D loss: 1.3764, G loss: 0.8386\n",
      "[1684/1778] D loss: 1.3902, G loss: 0.6898\n",
      "train error: \n",
      " D loss: 1.300320, G loss: 0.825330, D accuracy: 54.1%, cell accuracy: 21925.8%%, board accuracy: 86.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.284375, G loss: 0.865547, D accuracy: 55.1%, cell accuracy: 21911.5%%, board accuracy: 85.6% \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4040, G loss: 0.7464\n",
      "[124/1778] D loss: 1.3930, G loss: 0.6639\n",
      "[244/1778] D loss: 1.3923, G loss: 0.6406\n",
      "[364/1778] D loss: 0.8007, G loss: 1.1448\n",
      "[484/1778] D loss: 1.3925, G loss: 0.7113\n",
      "[604/1778] D loss: 1.4142, G loss: 0.8270\n",
      "[724/1778] D loss: 1.3592, G loss: 0.7103\n",
      "[844/1778] D loss: 1.4059, G loss: 0.7998\n",
      "[964/1778] D loss: 1.4065, G loss: 0.8691\n",
      "[1084/1778] D loss: 1.3994, G loss: 0.7630\n",
      "[1204/1778] D loss: 1.3903, G loss: 0.7183\n",
      "[1324/1778] D loss: 1.1251, G loss: 0.7993\n",
      "[1444/1778] D loss: 1.3921, G loss: 0.6683\n",
      "[1564/1778] D loss: 1.0528, G loss: 1.2826\n",
      "[1684/1778] D loss: 1.0814, G loss: 0.9980\n",
      "train error: \n",
      " D loss: 1.313102, G loss: 0.701710, D accuracy: 54.0%, cell accuracy: 21935.1%%, board accuracy: 85.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.294187, G loss: 0.738266, D accuracy: 54.6%, cell accuracy: 21918.7%%, board accuracy: 82.2% \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0750, G loss: 1.0647\n",
      "[124/1778] D loss: 1.0743, G loss: 1.1302\n",
      "[244/1778] D loss: 1.3875, G loss: 0.6674\n",
      "[364/1778] D loss: 1.4038, G loss: 0.6833\n",
      "[484/1778] D loss: 1.3478, G loss: 0.8371\n",
      "[604/1778] D loss: 1.3841, G loss: 0.6293\n",
      "[724/1778] D loss: 1.3929, G loss: 0.6043\n",
      "[844/1778] D loss: 1.4073, G loss: 0.5899\n",
      "[964/1778] D loss: 1.4215, G loss: 0.6549\n",
      "[1084/1778] D loss: 1.3917, G loss: 0.6676\n",
      "[1204/1778] D loss: 1.3946, G loss: 0.6795\n",
      "[1324/1778] D loss: 1.3455, G loss: 0.8138\n",
      "[1444/1778] D loss: 1.3967, G loss: 0.7072\n",
      "[1564/1778] D loss: 1.3907, G loss: 0.7275\n",
      "[1684/1778] D loss: 1.1016, G loss: 1.0732\n",
      "train error: \n",
      " D loss: 1.312352, G loss: 0.912173, D accuracy: 54.1%, cell accuracy: 21933.7%%, board accuracy: 87.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.296083, G loss: 0.959694, D accuracy: 55.2%, cell accuracy: 21918.2%%, board accuracy: 84.9% \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3967, G loss: 0.7720\n",
      "[124/1778] D loss: 1.3882, G loss: 0.6863\n",
      "[244/1778] D loss: 1.1485, G loss: 0.8066\n",
      "[364/1778] D loss: 1.3902, G loss: 0.7327\n",
      "[484/1778] D loss: 1.0872, G loss: 1.0480\n",
      "[604/1778] D loss: 1.3981, G loss: 0.8036\n",
      "[724/1778] D loss: 1.0728, G loss: 1.1631\n",
      "[844/1778] D loss: 1.3885, G loss: 0.7216\n",
      "[964/1778] D loss: 1.3972, G loss: 0.6452\n",
      "[1084/1778] D loss: 1.4091, G loss: 0.7281\n",
      "[1204/1778] D loss: 1.3880, G loss: 0.7414\n",
      "[1324/1778] D loss: 1.3910, G loss: 0.7658\n",
      "[1444/1778] D loss: 1.3897, G loss: 0.7044\n",
      "[1564/1778] D loss: 1.0918, G loss: 1.0611\n",
      "[1684/1778] D loss: 1.4100, G loss: 0.7804\n",
      "train error: \n",
      " D loss: 1.300707, G loss: 0.808419, D accuracy: 54.1%, cell accuracy: 21936.3%%, board accuracy: 88.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.283934, G loss: 0.849498, D accuracy: 55.0%, cell accuracy: 21919.4%%, board accuracy: 85.8% \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3981, G loss: 0.6738\n",
      "[124/1778] D loss: 1.3513, G loss: 0.7004\n",
      "[244/1778] D loss: 1.1186, G loss: 0.8542\n",
      "[364/1778] D loss: 1.1203, G loss: 0.9400\n",
      "[484/1778] D loss: 0.9976, G loss: 1.1173\n",
      "[604/1778] D loss: 1.3985, G loss: 0.6707\n",
      "[724/1778] D loss: 1.3983, G loss: 0.6369\n",
      "[844/1778] D loss: 1.4094, G loss: 0.7447\n",
      "[964/1778] D loss: 1.4293, G loss: 0.8725\n",
      "[1084/1778] D loss: 1.4073, G loss: 0.8084\n",
      "[1204/1778] D loss: 1.0580, G loss: 1.0432\n",
      "[1324/1778] D loss: 1.0922, G loss: 0.9779\n",
      "[1444/1778] D loss: 1.3952, G loss: 0.7582\n",
      "[1564/1778] D loss: 1.3872, G loss: 0.6563\n",
      "[1684/1778] D loss: 1.3917, G loss: 0.6279\n",
      "train error: \n",
      " D loss: 1.305104, G loss: 0.836908, D accuracy: 53.8%, cell accuracy: 21937.9%%, board accuracy: 87.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.292564, G loss: 0.869482, D accuracy: 54.8%, cell accuracy: 21924.8%%, board accuracy: 85.8% \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1032, G loss: 1.1222\n",
      "[124/1778] D loss: 1.4006, G loss: 0.6093\n",
      "[244/1778] D loss: 1.3938, G loss: 0.6536\n",
      "[364/1778] D loss: 1.3916, G loss: 0.7480\n",
      "[484/1778] D loss: 1.1029, G loss: 0.9264\n",
      "[604/1778] D loss: 0.8046, G loss: 1.3233\n",
      "[724/1778] D loss: 1.3513, G loss: 0.8152\n",
      "[844/1778] D loss: 1.0709, G loss: 0.9979\n",
      "[964/1778] D loss: 1.3839, G loss: 0.6688\n",
      "[1084/1778] D loss: 1.3874, G loss: 0.6827\n",
      "[1204/1778] D loss: 1.4036, G loss: 0.8160\n",
      "[1324/1778] D loss: 1.1276, G loss: 1.0505\n",
      "[1444/1778] D loss: 1.4042, G loss: 0.7414\n",
      "[1564/1778] D loss: 1.1011, G loss: 1.0216\n",
      "[1684/1778] D loss: 1.5082, G loss: 0.8657\n",
      "train error: \n",
      " D loss: 1.300587, G loss: 0.818881, D accuracy: 54.2%, cell accuracy: 21931.4%%, board accuracy: 86.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.287172, G loss: 0.855516, D accuracy: 55.1%, cell accuracy: 21916.2%%, board accuracy: 82.7% \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3262, G loss: 0.8373\n",
      "[124/1778] D loss: 1.0817, G loss: 1.1146\n",
      "[244/1778] D loss: 1.3970, G loss: 0.6779\n",
      "[364/1778] D loss: 1.0862, G loss: 0.9529\n",
      "[484/1778] D loss: 1.4110, G loss: 0.6473\n",
      "[604/1778] D loss: 1.3959, G loss: 0.7532\n",
      "[724/1778] D loss: 0.8385, G loss: 1.1312\n",
      "[844/1778] D loss: 1.3898, G loss: 0.6424\n",
      "[964/1778] D loss: 1.0639, G loss: 1.0789\n",
      "[1084/1778] D loss: 1.3909, G loss: 0.7068\n",
      "[1204/1778] D loss: 1.1101, G loss: 0.9212\n",
      "[1324/1778] D loss: 1.3821, G loss: 0.8044\n",
      "[1444/1778] D loss: 1.3934, G loss: 0.7427\n",
      "[1564/1778] D loss: 1.3949, G loss: 0.7383\n",
      "[1684/1778] D loss: 1.4010, G loss: 0.6955\n",
      "train error: \n",
      " D loss: 1.303227, G loss: 0.769474, D accuracy: 54.0%, cell accuracy: 21930.9%%, board accuracy: 85.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.287106, G loss: 0.808444, D accuracy: 54.6%, cell accuracy: 21918.5%%, board accuracy: 83.8% \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3915, G loss: 0.6947\n",
      "[124/1778] D loss: 1.0628, G loss: 1.1573\n",
      "[244/1778] D loss: 1.3870, G loss: 0.6782\n",
      "[364/1778] D loss: 1.3955, G loss: 0.7256\n",
      "[484/1778] D loss: 1.1312, G loss: 0.7916\n",
      "[604/1778] D loss: 1.3702, G loss: 0.7825\n",
      "[724/1778] D loss: 1.0943, G loss: 1.0893\n",
      "[844/1778] D loss: 1.3952, G loss: 0.8085\n",
      "[964/1778] D loss: 1.3980, G loss: 0.7229\n",
      "[1084/1778] D loss: 1.3094, G loss: 0.7815\n",
      "[1204/1778] D loss: 1.3935, G loss: 0.7032\n",
      "[1324/1778] D loss: 1.0838, G loss: 1.0392\n",
      "[1444/1778] D loss: 1.3916, G loss: 0.6682\n",
      "[1564/1778] D loss: 1.3950, G loss: 0.6847\n",
      "[1684/1778] D loss: 1.3972, G loss: 0.6563\n",
      "train error: \n",
      " D loss: 1.302651, G loss: 0.780024, D accuracy: 54.0%, cell accuracy: 21932.4%%, board accuracy: 86.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.289147, G loss: 0.813958, D accuracy: 55.0%, cell accuracy: 21921.4%%, board accuracy: 85.8% \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3883, G loss: 0.6929\n",
      "[124/1778] D loss: 1.3935, G loss: 0.7892\n",
      "[244/1778] D loss: 1.3776, G loss: 0.7107\n",
      "[364/1778] D loss: 1.3939, G loss: 0.6103\n",
      "[484/1778] D loss: 1.3929, G loss: 0.6112\n",
      "[604/1778] D loss: 1.4146, G loss: 0.6131\n",
      "[724/1778] D loss: 1.3942, G loss: 0.6633\n",
      "[844/1778] D loss: 1.4040, G loss: 0.7869\n",
      "[964/1778] D loss: 1.0765, G loss: 1.0875\n",
      "[1084/1778] D loss: 1.4202, G loss: 0.8314\n",
      "[1204/1778] D loss: 1.3952, G loss: 0.6519\n",
      "[1324/1778] D loss: 1.3929, G loss: 0.6538\n",
      "[1444/1778] D loss: 1.3943, G loss: 0.6191\n",
      "[1564/1778] D loss: 1.3243, G loss: 0.7698\n",
      "[1684/1778] D loss: 1.1189, G loss: 0.8404\n",
      "train error: \n",
      " D loss: 1.295107, G loss: 0.778774, D accuracy: 54.9%, cell accuracy: 21941.7%%, board accuracy: 85.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.279388, G loss: 0.808541, D accuracy: 55.4%, cell accuracy: 21934.2%%, board accuracy: 83.8% \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "[4/1778] D loss: 0.7608, G loss: 1.5814\n",
      "[124/1778] D loss: 1.0873, G loss: 0.9663\n",
      "[244/1778] D loss: 1.1383, G loss: 0.8677\n",
      "[364/1778] D loss: 1.0979, G loss: 0.9010\n",
      "[484/1778] D loss: 1.4058, G loss: 0.7733\n",
      "[604/1778] D loss: 1.3896, G loss: 0.7143\n",
      "[724/1778] D loss: 1.3947, G loss: 0.6190\n",
      "[844/1778] D loss: 1.1316, G loss: 0.7622\n",
      "[964/1778] D loss: 1.3894, G loss: 0.6900\n",
      "[1084/1778] D loss: 1.4019, G loss: 0.7362\n",
      "[1204/1778] D loss: 1.4028, G loss: 0.6341\n",
      "[1324/1778] D loss: 1.3946, G loss: 0.7220\n",
      "[1444/1778] D loss: 1.3313, G loss: 0.7571\n",
      "[1564/1778] D loss: 1.0594, G loss: 1.2204\n",
      "[1684/1778] D loss: 1.0841, G loss: 0.9785\n",
      "train error: \n",
      " D loss: 1.302397, G loss: 0.891068, D accuracy: 53.7%, cell accuracy: 21945.8%%, board accuracy: 87.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.287630, G loss: 0.941622, D accuracy: 54.7%, cell accuracy: 21936.9%%, board accuracy: 85.1% \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0724, G loss: 1.2507\n",
      "[124/1778] D loss: 1.4016, G loss: 0.7812\n",
      "[244/1778] D loss: 1.0710, G loss: 1.0643\n",
      "[364/1778] D loss: 1.3725, G loss: 0.8980\n",
      "[484/1778] D loss: 1.4022, G loss: 0.6114\n",
      "[604/1778] D loss: 1.4035, G loss: 0.7003\n",
      "[724/1778] D loss: 1.3980, G loss: 0.7795\n",
      "[844/1778] D loss: 1.3888, G loss: 0.6637\n",
      "[964/1778] D loss: 1.0928, G loss: 0.9752\n",
      "[1084/1778] D loss: 1.3912, G loss: 0.6618\n",
      "[1204/1778] D loss: 1.3950, G loss: 0.6479\n",
      "[1324/1778] D loss: 1.3920, G loss: 0.7502\n",
      "[1444/1778] D loss: 1.4177, G loss: 0.7911\n",
      "[1564/1778] D loss: 1.3872, G loss: 0.7308\n",
      "[1684/1778] D loss: 1.3935, G loss: 0.6643\n",
      "train error: \n",
      " D loss: 1.297764, G loss: 0.811977, D accuracy: 54.1%, cell accuracy: 21952.7%%, board accuracy: 88.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.281233, G loss: 0.856073, D accuracy: 55.1%, cell accuracy: 21948.0%%, board accuracy: 87.6% \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3878, G loss: 0.6462\n",
      "[124/1778] D loss: 1.3911, G loss: 0.6783\n",
      "[244/1778] D loss: 1.4298, G loss: 0.5901\n",
      "[364/1778] D loss: 1.3885, G loss: 0.7241\n",
      "[484/1778] D loss: 1.4335, G loss: 0.7080\n",
      "[604/1778] D loss: 1.3770, G loss: 0.8346\n",
      "[724/1778] D loss: 1.0692, G loss: 1.1212\n",
      "[844/1778] D loss: 1.4113, G loss: 0.6596\n",
      "[964/1778] D loss: 1.0729, G loss: 1.0467\n",
      "[1084/1778] D loss: 1.3928, G loss: 0.6252\n",
      "[1204/1778] D loss: 1.3919, G loss: 0.6837\n",
      "[1324/1778] D loss: 1.3976, G loss: 0.7059\n",
      "[1444/1778] D loss: 1.4017, G loss: 0.6608\n",
      "[1564/1778] D loss: 1.0682, G loss: 1.1464\n",
      "[1684/1778] D loss: 1.0734, G loss: 1.1346\n",
      "train error: \n",
      " D loss: 1.304070, G loss: 0.746300, D accuracy: 54.1%, cell accuracy: 21947.4%%, board accuracy: 87.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.286673, G loss: 0.790157, D accuracy: 54.8%, cell accuracy: 21940.3%%, board accuracy: 87.4% \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.5980, G loss: 0.4904\n",
      "[124/1778] D loss: 1.3881, G loss: 0.7191\n",
      "[244/1778] D loss: 1.3923, G loss: 0.7557\n",
      "[364/1778] D loss: 1.4289, G loss: 0.7976\n",
      "[484/1778] D loss: 1.3945, G loss: 0.7526\n",
      "[604/1778] D loss: 1.3578, G loss: 0.6400\n",
      "[724/1778] D loss: 1.3846, G loss: 0.8085\n",
      "[844/1778] D loss: 1.4086, G loss: 0.7555\n",
      "[964/1778] D loss: 1.1785, G loss: 0.7870\n",
      "[1084/1778] D loss: 1.4060, G loss: 0.5999\n",
      "[1204/1778] D loss: 1.4341, G loss: 0.7102\n",
      "[1324/1778] D loss: 1.3863, G loss: 0.6913\n",
      "[1444/1778] D loss: 1.3636, G loss: 0.6297\n",
      "[1564/1778] D loss: 1.1221, G loss: 0.8227\n",
      "[1684/1778] D loss: 1.4024, G loss: 0.7889\n",
      "train error: \n",
      " D loss: 1.298912, G loss: 0.816966, D accuracy: 54.1%, cell accuracy: 21953.2%%, board accuracy: 89.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.285713, G loss: 0.865557, D accuracy: 54.8%, cell accuracy: 21947.7%%, board accuracy: 88.1% \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4627, G loss: 0.8132\n",
      "[124/1778] D loss: 1.3919, G loss: 0.7131\n",
      "[244/1778] D loss: 1.3346, G loss: 0.6658\n",
      "[364/1778] D loss: 0.7400, G loss: 1.6946\n",
      "[484/1778] D loss: 1.3902, G loss: 0.6574\n",
      "[604/1778] D loss: 1.3969, G loss: 0.7321\n",
      "[724/1778] D loss: 1.1585, G loss: 0.8399\n",
      "[844/1778] D loss: 1.3662, G loss: 0.6507\n",
      "[964/1778] D loss: 1.3934, G loss: 0.6941\n",
      "[1084/1778] D loss: 1.3883, G loss: 0.9309\n",
      "[1204/1778] D loss: 1.3935, G loss: 0.7178\n",
      "[1324/1778] D loss: 1.0950, G loss: 0.9830\n",
      "[1444/1778] D loss: 1.0846, G loss: 1.0683\n",
      "[1564/1778] D loss: 1.3924, G loss: 0.6836\n",
      "[1684/1778] D loss: 0.7827, G loss: 1.2864\n",
      "train error: \n",
      " D loss: 1.289227, G loss: 0.816033, D accuracy: 55.4%, cell accuracy: 21915.9%%, board accuracy: 82.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.274928, G loss: 0.854914, D accuracy: 57.0%, cell accuracy: 21900.0%%, board accuracy: 81.5% \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4074, G loss: 0.6455\n",
      "[124/1778] D loss: 1.3514, G loss: 0.7482\n",
      "[244/1778] D loss: 1.4090, G loss: 0.6499\n",
      "[364/1778] D loss: 1.1056, G loss: 0.9743\n",
      "[484/1778] D loss: 1.5582, G loss: 0.7103\n",
      "[604/1778] D loss: 1.3899, G loss: 0.6724\n",
      "[724/1778] D loss: 1.3942, G loss: 0.7065\n",
      "[844/1778] D loss: 1.3940, G loss: 0.7719\n",
      "[964/1778] D loss: 1.3791, G loss: 0.6685\n",
      "[1084/1778] D loss: 1.3954, G loss: 0.7383\n",
      "[1204/1778] D loss: 1.3884, G loss: 0.6803\n",
      "[1324/1778] D loss: 1.4038, G loss: 0.6781\n",
      "[1444/1778] D loss: 1.3352, G loss: 0.8492\n",
      "[1564/1778] D loss: 1.0915, G loss: 1.0964\n",
      "[1684/1778] D loss: 1.3947, G loss: 0.7053\n",
      "train error: \n",
      " D loss: 1.299529, G loss: 0.778981, D accuracy: 54.2%, cell accuracy: 21933.2%%, board accuracy: 87.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.289168, G loss: 0.816536, D accuracy: 54.7%, cell accuracy: 21918.9%%, board accuracy: 86.9% \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0836, G loss: 0.9679\n",
      "[124/1778] D loss: 1.4084, G loss: 0.7798\n",
      "[244/1778] D loss: 1.0921, G loss: 0.9973\n",
      "[364/1778] D loss: 1.3842, G loss: 0.7627\n",
      "[484/1778] D loss: 1.0619, G loss: 0.9632\n",
      "[604/1778] D loss: 1.3969, G loss: 0.6694\n",
      "[724/1778] D loss: 1.2181, G loss: 0.8603\n",
      "[844/1778] D loss: 1.4080, G loss: 0.8356\n",
      "[964/1778] D loss: 1.4159, G loss: 0.5731\n",
      "[1084/1778] D loss: 1.3947, G loss: 0.7113\n",
      "[1204/1778] D loss: 1.0664, G loss: 1.1128\n",
      "[1324/1778] D loss: 1.3861, G loss: 0.6042\n",
      "[1444/1778] D loss: 1.3926, G loss: 0.6337\n",
      "[1564/1778] D loss: 0.7513, G loss: 1.6289\n",
      "[1684/1778] D loss: 1.4018, G loss: 0.7494\n",
      "train error: \n",
      " D loss: 1.301729, G loss: 0.764941, D accuracy: 54.2%, cell accuracy: 21931.1%%, board accuracy: 88.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.289032, G loss: 0.802762, D accuracy: 55.1%, cell accuracy: 21916.2%%, board accuracy: 86.5% \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1591, G loss: 0.7820\n",
      "[124/1778] D loss: 1.3963, G loss: 0.7919\n",
      "[244/1778] D loss: 1.3894, G loss: 0.6317\n",
      "[364/1778] D loss: 1.4080, G loss: 0.5799\n",
      "[484/1778] D loss: 1.3870, G loss: 0.6725\n",
      "[604/1778] D loss: 1.3949, G loss: 0.6835\n",
      "[724/1778] D loss: 1.4273, G loss: 0.5571\n",
      "[844/1778] D loss: 1.3908, G loss: 0.6657\n",
      "[964/1778] D loss: 1.3885, G loss: 0.7096\n",
      "[1084/1778] D loss: 1.3337, G loss: 0.8469\n",
      "[1204/1778] D loss: 1.3872, G loss: 0.6175\n",
      "[1324/1778] D loss: 1.0998, G loss: 0.9190\n",
      "[1444/1778] D loss: 1.4112, G loss: 0.8396\n",
      "[1564/1778] D loss: 1.3886, G loss: 0.7298\n",
      "[1684/1778] D loss: 1.2824, G loss: 0.8268\n",
      "train error: \n",
      " D loss: 1.300637, G loss: 0.854411, D accuracy: 54.0%, cell accuracy: 21931.4%%, board accuracy: 85.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.288019, G loss: 0.912116, D accuracy: 55.6%, cell accuracy: 21915.3%%, board accuracy: 84.0% \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3936, G loss: 0.7750\n",
      "[124/1778] D loss: 1.4036, G loss: 0.5878\n",
      "[244/1778] D loss: 1.4208, G loss: 0.6207\n",
      "[364/1778] D loss: 1.3897, G loss: 0.6126\n",
      "[484/1778] D loss: 1.3681, G loss: 0.6595\n",
      "[604/1778] D loss: 1.3868, G loss: 0.6545\n",
      "[724/1778] D loss: 1.3872, G loss: 0.7156\n",
      "[844/1778] D loss: 1.3880, G loss: 0.7595\n",
      "[964/1778] D loss: 1.3870, G loss: 0.7358\n",
      "[1084/1778] D loss: 1.1437, G loss: 0.8284\n",
      "[1204/1778] D loss: 0.7704, G loss: 1.3585\n",
      "[1324/1778] D loss: 1.0858, G loss: 1.1003\n",
      "[1444/1778] D loss: 1.3887, G loss: 0.6646\n",
      "[1564/1778] D loss: 1.4156, G loss: 0.8224\n",
      "[1684/1778] D loss: 1.3610, G loss: 0.6619\n",
      "train error: \n",
      " D loss: 1.299158, G loss: 0.816102, D accuracy: 54.0%, cell accuracy: 21936.8%%, board accuracy: 87.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.283285, G loss: 0.861819, D accuracy: 55.4%, cell accuracy: 21921.6%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0965, G loss: 0.9910\n",
      "[124/1778] D loss: 0.8259, G loss: 1.3163\n",
      "[244/1778] D loss: 1.3409, G loss: 0.8244\n",
      "[364/1778] D loss: 1.0849, G loss: 0.9938\n",
      "[484/1778] D loss: 1.3909, G loss: 0.6614\n",
      "[604/1778] D loss: 1.4367, G loss: 0.7937\n",
      "[724/1778] D loss: 1.0863, G loss: 0.9509\n",
      "[844/1778] D loss: 1.0803, G loss: 1.1701\n",
      "[964/1778] D loss: 1.4022, G loss: 0.6339\n",
      "[1084/1778] D loss: 1.4114, G loss: 0.7725\n",
      "[1204/1778] D loss: 1.2767, G loss: 1.3184\n",
      "[1324/1778] D loss: 1.3937, G loss: 0.7066\n",
      "[1444/1778] D loss: 1.0745, G loss: 1.4007\n",
      "[1564/1778] D loss: 1.1046, G loss: 1.0656\n",
      "[1684/1778] D loss: 1.4042, G loss: 0.6030\n",
      "train error: \n",
      " D loss: 1.304855, G loss: 0.727297, D accuracy: 54.2%, cell accuracy: 21937.1%%, board accuracy: 88.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.287949, G loss: 0.767772, D accuracy: 55.2%, cell accuracy: 21922.1%%, board accuracy: 86.5% \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4082, G loss: 0.5947\n",
      "[124/1778] D loss: 1.0592, G loss: 1.1471\n",
      "[244/1778] D loss: 1.4294, G loss: 0.8019\n",
      "[364/1778] D loss: 1.3955, G loss: 0.7710\n",
      "[484/1778] D loss: 1.3001, G loss: 0.7501\n",
      "[604/1778] D loss: 1.3809, G loss: 0.7075\n",
      "[724/1778] D loss: 1.0856, G loss: 0.9586\n",
      "[844/1778] D loss: 1.0884, G loss: 1.1376\n",
      "[964/1778] D loss: 1.1030, G loss: 1.0597\n",
      "[1084/1778] D loss: 1.3963, G loss: 0.6400\n",
      "[1204/1778] D loss: 1.3989, G loss: 0.6217\n",
      "[1324/1778] D loss: 1.3906, G loss: 0.6990\n",
      "[1444/1778] D loss: 1.4139, G loss: 0.7303\n",
      "[1564/1778] D loss: 1.0937, G loss: 1.1935\n",
      "[1684/1778] D loss: 1.3851, G loss: 0.7020\n",
      "train error: \n",
      " D loss: 1.303208, G loss: 0.733508, D accuracy: 54.2%, cell accuracy: 21937.1%%, board accuracy: 88.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.287586, G loss: 0.783406, D accuracy: 55.2%, cell accuracy: 21921.2%%, board accuracy: 86.7% \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4003, G loss: 0.6302\n",
      "[124/1778] D loss: 1.1191, G loss: 0.8354\n",
      "[244/1778] D loss: 1.4013, G loss: 0.6017\n",
      "[364/1778] D loss: 1.3949, G loss: 0.7460\n",
      "[484/1778] D loss: 1.3937, G loss: 0.7761\n",
      "[604/1778] D loss: 1.3941, G loss: 0.7868\n",
      "[724/1778] D loss: 1.3968, G loss: 0.6524\n",
      "[844/1778] D loss: 1.3984, G loss: 0.6953\n",
      "[964/1778] D loss: 1.3897, G loss: 0.7254\n",
      "[1084/1778] D loss: 1.3889, G loss: 0.7437\n",
      "[1204/1778] D loss: 1.4251, G loss: 0.5682\n",
      "[1324/1778] D loss: 1.4310, G loss: 0.5450\n",
      "[1444/1778] D loss: 1.4099, G loss: 0.8167\n",
      "[1564/1778] D loss: 1.3938, G loss: 0.6137\n",
      "[1684/1778] D loss: 1.0878, G loss: 0.9461\n",
      "train error: \n",
      " D loss: 1.301128, G loss: 0.748665, D accuracy: 54.4%, cell accuracy: 21933.5%%, board accuracy: 87.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.283702, G loss: 0.789580, D accuracy: 55.9%, cell accuracy: 21920.9%%, board accuracy: 86.0% \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0812, G loss: 1.0413\n",
      "[124/1778] D loss: 1.3649, G loss: 0.9548\n",
      "[244/1778] D loss: 1.4068, G loss: 0.6738\n",
      "[364/1778] D loss: 1.4044, G loss: 0.6628\n",
      "[484/1778] D loss: 1.0770, G loss: 1.0712\n",
      "[604/1778] D loss: 1.3631, G loss: 0.7609\n",
      "[724/1778] D loss: 1.3910, G loss: 0.7253\n",
      "[844/1778] D loss: 1.0974, G loss: 1.0413\n",
      "[964/1778] D loss: 1.3872, G loss: 0.6836\n",
      "[1084/1778] D loss: 1.3874, G loss: 0.6954\n",
      "[1204/1778] D loss: 1.0686, G loss: 1.1101\n",
      "[1324/1778] D loss: 1.3912, G loss: 0.6593\n",
      "[1444/1778] D loss: 1.3954, G loss: 0.7414\n",
      "[1564/1778] D loss: 1.3945, G loss: 0.7451\n",
      "[1684/1778] D loss: 1.3925, G loss: 0.6514\n",
      "train error: \n",
      " D loss: 1.324370, G loss: 0.655637, D accuracy: 54.0%, cell accuracy: 21937.0%%, board accuracy: 89.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.313707, G loss: 0.687031, D accuracy: 55.0%, cell accuracy: 21923.0%%, board accuracy: 87.2% \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4019, G loss: 0.6101\n",
      "[124/1778] D loss: 1.3959, G loss: 0.7390\n",
      "[244/1778] D loss: 1.3932, G loss: 0.7413\n",
      "[364/1778] D loss: 1.0526, G loss: 1.4059\n",
      "[484/1778] D loss: 1.3769, G loss: 0.7130\n",
      "[604/1778] D loss: 1.0751, G loss: 0.9925\n",
      "[724/1778] D loss: 1.1137, G loss: 1.2976\n",
      "[844/1778] D loss: 1.4205, G loss: 0.8296\n",
      "[964/1778] D loss: 1.0749, G loss: 1.0316\n",
      "[1084/1778] D loss: 1.4069, G loss: 0.6749\n",
      "[1204/1778] D loss: 1.3756, G loss: 0.7733\n",
      "[1324/1778] D loss: 1.0861, G loss: 1.1933\n",
      "[1444/1778] D loss: 1.0717, G loss: 1.1073\n",
      "[1564/1778] D loss: 1.3991, G loss: 0.7235\n",
      "[1684/1778] D loss: 1.3974, G loss: 0.7186\n",
      "train error: \n",
      " D loss: 1.299268, G loss: 0.765543, D accuracy: 54.0%, cell accuracy: 21935.6%%, board accuracy: 89.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.287168, G loss: 0.799814, D accuracy: 54.7%, cell accuracy: 21923.6%%, board accuracy: 88.3% \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0973, G loss: 0.9492\n",
      "[124/1778] D loss: 1.3922, G loss: 0.7146\n",
      "[244/1778] D loss: 1.3972, G loss: 0.7399\n",
      "[364/1778] D loss: 1.3872, G loss: 0.6681\n",
      "[484/1778] D loss: 1.3872, G loss: 0.7230\n",
      "[604/1778] D loss: 1.4021, G loss: 0.7490\n",
      "[724/1778] D loss: 1.3914, G loss: 0.6736\n",
      "[844/1778] D loss: 1.3900, G loss: 0.6458\n",
      "[964/1778] D loss: 1.3910, G loss: 0.6331\n",
      "[1084/1778] D loss: 1.0741, G loss: 1.0716\n",
      "[1204/1778] D loss: 1.3982, G loss: 0.5941\n",
      "[1324/1778] D loss: 1.1005, G loss: 1.1218\n",
      "[1444/1778] D loss: 1.0639, G loss: 1.1676\n",
      "[1564/1778] D loss: 1.0698, G loss: 1.1482\n",
      "[1684/1778] D loss: 1.3576, G loss: 0.7566\n",
      "train error: \n",
      " D loss: 1.292333, G loss: 0.833980, D accuracy: 54.1%, cell accuracy: 21925.1%%, board accuracy: 87.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.275216, G loss: 0.884591, D accuracy: 55.9%, cell accuracy: 21907.9%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3884, G loss: 0.7019\n",
      "[124/1778] D loss: 1.0924, G loss: 1.1291\n",
      "[244/1778] D loss: 1.4065, G loss: 0.7034\n",
      "[364/1778] D loss: 1.3884, G loss: 0.6972\n",
      "[484/1778] D loss: 1.0772, G loss: 1.0716\n",
      "[604/1778] D loss: 1.3809, G loss: 0.7484\n",
      "[724/1778] D loss: 1.3977, G loss: 0.6449\n",
      "[844/1778] D loss: 1.3877, G loss: 0.6642\n",
      "[964/1778] D loss: 1.3919, G loss: 0.7134\n",
      "[1084/1778] D loss: 1.3942, G loss: 0.6409\n",
      "[1204/1778] D loss: 1.3354, G loss: 0.7855\n",
      "[1324/1778] D loss: 1.3799, G loss: 0.7257\n",
      "[1444/1778] D loss: 1.0680, G loss: 1.1196\n",
      "[1564/1778] D loss: 1.1169, G loss: 1.1686\n",
      "[1684/1778] D loss: 1.1181, G loss: 0.8477\n",
      "train error: \n",
      " D loss: 1.313429, G loss: 0.682847, D accuracy: 54.6%, cell accuracy: 21930.7%%, board accuracy: 88.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.310633, G loss: 0.709503, D accuracy: 55.1%, cell accuracy: 21920.7%%, board accuracy: 88.7% \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4054, G loss: 0.6698\n",
      "[124/1778] D loss: 1.3843, G loss: 0.6938\n",
      "[244/1778] D loss: 1.3913, G loss: 0.7164\n",
      "[364/1778] D loss: 1.3941, G loss: 0.7021\n",
      "[484/1778] D loss: 1.0676, G loss: 1.1369\n",
      "[604/1778] D loss: 1.3947, G loss: 0.6100\n",
      "[724/1778] D loss: 1.5067, G loss: 0.5705\n",
      "[844/1778] D loss: 1.3758, G loss: 0.7654\n",
      "[964/1778] D loss: 0.8143, G loss: 1.2746\n",
      "[1084/1778] D loss: 1.3735, G loss: 0.6625\n",
      "[1204/1778] D loss: 1.2009, G loss: 1.3544\n",
      "[1324/1778] D loss: 1.3985, G loss: 0.6760\n",
      "[1444/1778] D loss: 1.1068, G loss: 1.1032\n",
      "[1564/1778] D loss: 1.3965, G loss: 0.6246\n",
      "[1684/1778] D loss: 1.3908, G loss: 0.6918\n",
      "train error: \n",
      " D loss: 1.305456, G loss: 0.763298, D accuracy: 55.4%, cell accuracy: 21919.2%%, board accuracy: 86.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.293911, G loss: 0.808212, D accuracy: 55.9%, cell accuracy: 21901.4%%, board accuracy: 84.0% \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4044, G loss: 0.6095\n",
      "[124/1778] D loss: 1.4205, G loss: 0.7981\n",
      "[244/1778] D loss: 1.4000, G loss: 0.6596\n",
      "[364/1778] D loss: 1.3896, G loss: 0.6622\n",
      "[484/1778] D loss: 1.3904, G loss: 0.6210\n",
      "[604/1778] D loss: 1.1081, G loss: 0.9072\n",
      "[724/1778] D loss: 1.2941, G loss: 0.9910\n",
      "[844/1778] D loss: 1.3921, G loss: 0.7197\n",
      "[964/1778] D loss: 1.4067, G loss: 0.7038\n",
      "[1084/1778] D loss: 1.3859, G loss: 0.6180\n",
      "[1204/1778] D loss: 1.3919, G loss: 0.7463\n",
      "[1324/1778] D loss: 1.3885, G loss: 0.6705\n",
      "[1444/1778] D loss: 1.3877, G loss: 0.6881\n",
      "[1564/1778] D loss: 1.3723, G loss: 0.7450\n",
      "[1684/1778] D loss: 1.3927, G loss: 0.7983\n",
      "train error: \n",
      " D loss: 1.302318, G loss: 0.768434, D accuracy: 54.0%, cell accuracy: 21936.8%%, board accuracy: 89.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.287547, G loss: 0.821300, D accuracy: 54.8%, cell accuracy: 21920.0%%, board accuracy: 88.5% \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1051, G loss: 0.9037\n",
      "[124/1778] D loss: 1.4003, G loss: 0.7644\n",
      "[244/1778] D loss: 1.3966, G loss: 0.6653\n",
      "[364/1778] D loss: 1.3901, G loss: 0.6552\n",
      "[484/1778] D loss: 1.0393, G loss: 1.0112\n",
      "[604/1778] D loss: 1.3955, G loss: 0.6169\n",
      "[724/1778] D loss: 1.3901, G loss: 0.7251\n",
      "[844/1778] D loss: 1.3877, G loss: 0.6993\n",
      "[964/1778] D loss: 1.4050, G loss: 0.6284\n",
      "[1084/1778] D loss: 1.0841, G loss: 1.0947\n",
      "[1204/1778] D loss: 1.0727, G loss: 1.3057\n",
      "[1324/1778] D loss: 1.0680, G loss: 1.2657\n",
      "[1444/1778] D loss: 1.0896, G loss: 1.0142\n",
      "[1564/1778] D loss: 1.1156, G loss: 0.7932\n",
      "[1684/1778] D loss: 1.4230, G loss: 0.5912\n",
      "train error: \n",
      " D loss: 1.309696, G loss: 0.748903, D accuracy: 54.1%, cell accuracy: 21928.5%%, board accuracy: 88.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.298022, G loss: 0.790375, D accuracy: 55.1%, cell accuracy: 21909.5%%, board accuracy: 86.7% \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0846, G loss: 0.9527\n",
      "[124/1778] D loss: 1.3942, G loss: 0.7584\n",
      "[244/1778] D loss: 0.5652, G loss: 1.5537\n",
      "[364/1778] D loss: 1.3938, G loss: 0.5920\n",
      "[484/1778] D loss: 1.8423, G loss: 0.5978\n",
      "[604/1778] D loss: 1.3956, G loss: 0.7220\n",
      "[724/1778] D loss: 1.3893, G loss: 0.6673\n",
      "[844/1778] D loss: 1.4048, G loss: 0.7616\n",
      "[964/1778] D loss: 1.3974, G loss: 0.7539\n",
      "[1084/1778] D loss: 1.4004, G loss: 0.7288\n",
      "[1204/1778] D loss: 1.2734, G loss: 0.8127\n",
      "[1324/1778] D loss: 1.3907, G loss: 0.6929\n",
      "[1444/1778] D loss: 1.4122, G loss: 0.8532\n",
      "[1564/1778] D loss: 1.3934, G loss: 0.6903\n",
      "[1684/1778] D loss: 1.3914, G loss: 0.7006\n",
      "train error: \n",
      " D loss: 1.308008, G loss: 0.902543, D accuracy: 54.0%, cell accuracy: 21924.8%%, board accuracy: 87.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.306363, G loss: 0.941536, D accuracy: 54.4%, cell accuracy: 21910.4%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3182, G loss: 0.8479\n",
      "[124/1778] D loss: 1.3876, G loss: 0.6339\n",
      "[244/1778] D loss: 1.3425, G loss: 0.7083\n",
      "[364/1778] D loss: 1.4092, G loss: 0.5414\n",
      "[484/1778] D loss: 1.3868, G loss: 0.7603\n",
      "[604/1778] D loss: 1.3880, G loss: 0.6693\n",
      "[724/1778] D loss: 1.3889, G loss: 0.6756\n",
      "[844/1778] D loss: 1.3874, G loss: 0.7034\n",
      "[964/1778] D loss: 1.4143, G loss: 0.6924\n",
      "[1084/1778] D loss: 1.3738, G loss: 0.7979\n",
      "[1204/1778] D loss: 1.3617, G loss: 0.7600\n",
      "[1324/1778] D loss: 1.0760, G loss: 1.0590\n",
      "[1444/1778] D loss: 1.4066, G loss: 0.8184\n",
      "[1564/1778] D loss: 1.0806, G loss: 1.0890\n",
      "[1684/1778] D loss: 1.3942, G loss: 0.6236\n",
      "train error: \n",
      " D loss: 1.303181, G loss: 0.767760, D accuracy: 54.1%, cell accuracy: 21932.7%%, board accuracy: 88.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.292550, G loss: 0.800840, D accuracy: 54.7%, cell accuracy: 21915.1%%, board accuracy: 86.7% \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1009, G loss: 0.9277\n",
      "[124/1778] D loss: 1.0732, G loss: 1.1314\n",
      "[244/1778] D loss: 1.0690, G loss: 1.3499\n",
      "[364/1778] D loss: 1.3920, G loss: 0.7519\n",
      "[484/1778] D loss: 1.0571, G loss: 1.3272\n",
      "[604/1778] D loss: 1.1144, G loss: 0.9082\n",
      "[724/1778] D loss: 1.0678, G loss: 1.2513\n",
      "[844/1778] D loss: 1.4487, G loss: 0.7618\n",
      "[964/1778] D loss: 1.3875, G loss: 0.7121\n",
      "[1084/1778] D loss: 1.3884, G loss: 0.6808\n",
      "[1204/1778] D loss: 1.4122, G loss: 0.7701\n",
      "[1324/1778] D loss: 1.0861, G loss: 1.2132\n",
      "[1444/1778] D loss: 1.3960, G loss: 0.7023\n",
      "[1564/1778] D loss: 1.3904, G loss: 0.5872\n",
      "[1684/1778] D loss: 1.3950, G loss: 0.7477\n",
      "train error: \n",
      " D loss: 1.289900, G loss: 0.814523, D accuracy: 56.5%, cell accuracy: 21914.1%%, board accuracy: 83.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.289284, G loss: 0.850802, D accuracy: 56.8%, cell accuracy: 21898.0%%, board accuracy: 81.5% \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4210, G loss: 0.5805\n",
      "[124/1778] D loss: 1.3977, G loss: 0.7779\n",
      "[244/1778] D loss: 1.0699, G loss: 1.1112\n",
      "[364/1778] D loss: 1.4170, G loss: 0.7078\n",
      "[484/1778] D loss: 1.4053, G loss: 0.7437\n",
      "[604/1778] D loss: 1.4025, G loss: 0.7118\n",
      "[724/1778] D loss: 1.3520, G loss: 0.8444\n",
      "[844/1778] D loss: 0.8382, G loss: 1.1928\n",
      "[964/1778] D loss: 1.3923, G loss: 0.6738\n",
      "[1084/1778] D loss: 1.3027, G loss: 0.7356\n",
      "[1204/1778] D loss: 1.1195, G loss: 0.8850\n",
      "[1324/1778] D loss: 1.1784, G loss: 0.7817\n",
      "[1444/1778] D loss: 1.3890, G loss: 0.6973\n",
      "[1564/1778] D loss: 1.4369, G loss: 0.4835\n",
      "[1684/1778] D loss: 1.3283, G loss: 0.9765\n",
      "train error: \n",
      " D loss: 1.307346, G loss: 0.876569, D accuracy: 53.8%, cell accuracy: 21925.1%%, board accuracy: 86.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.298650, G loss: 0.936143, D accuracy: 54.6%, cell accuracy: 21905.2%%, board accuracy: 84.5% \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3938, G loss: 0.7712\n",
      "[124/1778] D loss: 1.3875, G loss: 0.7284\n",
      "[244/1778] D loss: 1.3866, G loss: 0.7047\n",
      "[364/1778] D loss: 1.1190, G loss: 0.8711\n",
      "[484/1778] D loss: 1.3723, G loss: 0.8705\n",
      "[604/1778] D loss: 1.2661, G loss: 1.0048\n",
      "[724/1778] D loss: 0.7867, G loss: 1.2602\n",
      "[844/1778] D loss: 1.1524, G loss: 0.8187\n",
      "[964/1778] D loss: 1.3842, G loss: 0.7307\n",
      "[1084/1778] D loss: 1.3894, G loss: 0.7031\n",
      "[1204/1778] D loss: 1.3952, G loss: 0.7042\n",
      "[1324/1778] D loss: 1.4067, G loss: 0.6651\n",
      "[1444/1778] D loss: 1.4136, G loss: 0.7987\n",
      "[1564/1778] D loss: 1.1030, G loss: 1.2159\n",
      "[1684/1778] D loss: 1.3948, G loss: 0.7650\n",
      "train error: \n",
      " D loss: 1.332693, G loss: 0.652359, D accuracy: 54.0%, cell accuracy: 21934.5%%, board accuracy: 88.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.322529, G loss: 0.683926, D accuracy: 54.7%, cell accuracy: 21916.2%%, board accuracy: 87.8% \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4017, G loss: 0.5913\n",
      "[124/1778] D loss: 1.1078, G loss: 1.0839\n",
      "[244/1778] D loss: 1.0670, G loss: 1.0780\n",
      "[364/1778] D loss: 1.0825, G loss: 1.0448\n",
      "[484/1778] D loss: 1.3925, G loss: 0.7365\n",
      "[604/1778] D loss: 1.4031, G loss: 0.8208\n",
      "[724/1778] D loss: 1.3558, G loss: 0.7125\n",
      "[844/1778] D loss: 1.3872, G loss: 0.7270\n",
      "[964/1778] D loss: 1.0570, G loss: 1.3492\n",
      "[1084/1778] D loss: 1.4082, G loss: 0.6996\n",
      "[1204/1778] D loss: 1.3899, G loss: 0.7413\n",
      "[1324/1778] D loss: 1.3924, G loss: 0.6280\n",
      "[1444/1778] D loss: 1.1117, G loss: 0.8738\n",
      "[1564/1778] D loss: 1.3955, G loss: 0.6195\n",
      "[1684/1778] D loss: 1.1140, G loss: 0.8601\n",
      "train error: \n",
      " D loss: 1.301985, G loss: 0.733243, D accuracy: 54.3%, cell accuracy: 21933.5%%, board accuracy: 88.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.282804, G loss: 0.772486, D accuracy: 55.4%, cell accuracy: 21915.8%%, board accuracy: 86.5% \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4038, G loss: 0.6007\n",
      "[124/1778] D loss: 1.0845, G loss: 1.0290\n",
      "[244/1778] D loss: 1.0780, G loss: 1.1486\n",
      "[364/1778] D loss: 1.3879, G loss: 0.7266\n",
      "[484/1778] D loss: 1.1067, G loss: 0.9333\n",
      "[604/1778] D loss: 1.3909, G loss: 0.6731\n",
      "[724/1778] D loss: 1.3894, G loss: 0.7199\n",
      "[844/1778] D loss: 1.0970, G loss: 0.8926\n",
      "[964/1778] D loss: 1.1172, G loss: 0.8107\n",
      "[1084/1778] D loss: 1.0911, G loss: 0.9585\n",
      "[1204/1778] D loss: 1.3951, G loss: 0.6604\n",
      "[1324/1778] D loss: 1.3941, G loss: 0.6690\n",
      "[1444/1778] D loss: 1.3883, G loss: 0.6454\n",
      "[1564/1778] D loss: 1.3875, G loss: 0.6895\n",
      "[1684/1778] D loss: 1.4100, G loss: 0.6816\n",
      "train error: \n",
      " D loss: 1.297958, G loss: 0.788153, D accuracy: 54.0%, cell accuracy: 21933.1%%, board accuracy: 89.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.279401, G loss: 0.829655, D accuracy: 55.4%, cell accuracy: 21915.5%%, board accuracy: 87.6% \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3883, G loss: 0.6958\n",
      "[124/1778] D loss: 1.3759, G loss: 0.6606\n",
      "[244/1778] D loss: 1.0704, G loss: 1.0300\n",
      "[364/1778] D loss: 1.0645, G loss: 1.1966\n",
      "[484/1778] D loss: 1.3892, G loss: 0.7038\n",
      "[604/1778] D loss: 1.4284, G loss: 0.8196\n",
      "[724/1778] D loss: 1.0781, G loss: 1.0252\n",
      "[844/1778] D loss: 1.3870, G loss: 0.6934\n",
      "[964/1778] D loss: 1.3923, G loss: 0.7250\n",
      "[1084/1778] D loss: 1.3988, G loss: 0.6715\n",
      "[1204/1778] D loss: 1.3886, G loss: 0.7113\n",
      "[1324/1778] D loss: 1.3903, G loss: 0.7728\n",
      "[1444/1778] D loss: 1.3904, G loss: 0.6921\n",
      "[1564/1778] D loss: 1.3941, G loss: 0.6477\n",
      "[1684/1778] D loss: 1.0710, G loss: 1.0756\n",
      "train error: \n",
      " D loss: 1.289118, G loss: 0.839338, D accuracy: 54.4%, cell accuracy: 21927.1%%, board accuracy: 86.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.272159, G loss: 0.891032, D accuracy: 55.2%, cell accuracy: 21905.9%%, board accuracy: 84.0% \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0777, G loss: 1.0100\n",
      "[124/1778] D loss: 1.4000, G loss: 0.6460\n",
      "[244/1778] D loss: 1.3886, G loss: 0.6709\n",
      "[364/1778] D loss: 1.0740, G loss: 1.0767\n",
      "[484/1778] D loss: 1.3907, G loss: 0.6541\n",
      "[604/1778] D loss: 1.3909, G loss: 0.6209\n",
      "[724/1778] D loss: 1.3915, G loss: 0.7377\n",
      "[844/1778] D loss: 1.3873, G loss: 0.7032\n",
      "[964/1778] D loss: 1.0289, G loss: 1.4077\n",
      "[1084/1778] D loss: 1.3892, G loss: 0.6778\n",
      "[1204/1778] D loss: 1.3903, G loss: 0.7589\n",
      "[1324/1778] D loss: 1.3797, G loss: 0.7020\n",
      "[1444/1778] D loss: 1.3895, G loss: 0.6696\n",
      "[1564/1778] D loss: 1.4038, G loss: 0.8324\n",
      "[1684/1778] D loss: 1.3894, G loss: 0.6359\n",
      "train error: \n",
      " D loss: 1.298151, G loss: 0.872752, D accuracy: 53.9%, cell accuracy: 21937.9%%, board accuracy: 89.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.282997, G loss: 0.914370, D accuracy: 55.0%, cell accuracy: 21921.6%%, board accuracy: 87.2% \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4043, G loss: 0.7823\n",
      "[124/1778] D loss: 1.3917, G loss: 0.6328\n",
      "[244/1778] D loss: 1.4098, G loss: 0.8401\n",
      "[364/1778] D loss: 1.3914, G loss: 0.7411\n",
      "[484/1778] D loss: 1.0886, G loss: 0.9442\n",
      "[604/1778] D loss: 1.3894, G loss: 0.6642\n",
      "[724/1778] D loss: 1.0942, G loss: 1.0343\n",
      "[844/1778] D loss: 1.3909, G loss: 0.6569\n",
      "[964/1778] D loss: 1.3906, G loss: 0.7465\n",
      "[1084/1778] D loss: 1.3899, G loss: 0.6753\n",
      "[1204/1778] D loss: 1.3939, G loss: 0.7439\n",
      "[1324/1778] D loss: 1.3834, G loss: 0.7531\n",
      "[1444/1778] D loss: 1.0961, G loss: 0.9981\n",
      "[1564/1778] D loss: 1.4041, G loss: 0.8426\n",
      "[1684/1778] D loss: 1.3873, G loss: 0.6840\n",
      "train error: \n",
      " D loss: 1.298632, G loss: 0.825399, D accuracy: 54.0%, cell accuracy: 21953.5%%, board accuracy: 89.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.280370, G loss: 0.864099, D accuracy: 54.8%, cell accuracy: 21944.8%%, board accuracy: 89.2% \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0715, G loss: 1.0387\n",
      "[124/1778] D loss: 1.3881, G loss: 0.7131\n",
      "[244/1778] D loss: 1.3991, G loss: 0.7407\n",
      "[364/1778] D loss: 1.3889, G loss: 0.6803\n",
      "[484/1778] D loss: 1.3883, G loss: 0.7085\n",
      "[604/1778] D loss: 1.3913, G loss: 0.7098\n",
      "[724/1778] D loss: 1.0738, G loss: 1.0292\n",
      "[844/1778] D loss: 1.3893, G loss: 0.6635\n",
      "[964/1778] D loss: 1.3948, G loss: 0.6257\n",
      "[1084/1778] D loss: 1.3922, G loss: 0.6829\n",
      "[1204/1778] D loss: 1.3887, G loss: 0.6912\n",
      "[1324/1778] D loss: 1.3982, G loss: 0.7323\n",
      "[1444/1778] D loss: 0.7828, G loss: 1.3134\n",
      "[1564/1778] D loss: 1.3885, G loss: 0.6555\n",
      "[1684/1778] D loss: 1.3924, G loss: 0.7488\n",
      "train error: \n",
      " D loss: 1.296154, G loss: 0.781114, D accuracy: 53.9%, cell accuracy: 21963.2%%, board accuracy: 90.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.285590, G loss: 0.816941, D accuracy: 54.7%, cell accuracy: 21953.4%%, board accuracy: 88.7% \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3976, G loss: 0.6159\n",
      "[124/1778] D loss: 1.3952, G loss: 0.6825\n",
      "[244/1778] D loss: 1.3947, G loss: 0.6866\n",
      "[364/1778] D loss: 1.3962, G loss: 0.7391\n",
      "[484/1778] D loss: 1.4258, G loss: 0.8145\n",
      "[604/1778] D loss: 1.3874, G loss: 0.6448\n",
      "[724/1778] D loss: 1.0626, G loss: 1.1453\n",
      "[844/1778] D loss: 1.0562, G loss: 1.3670\n",
      "[964/1778] D loss: 1.0783, G loss: 0.9929\n",
      "[1084/1778] D loss: 1.3907, G loss: 0.6242\n",
      "[1204/1778] D loss: 1.3890, G loss: 0.6951\n",
      "[1324/1778] D loss: 1.3879, G loss: 0.7187\n",
      "[1444/1778] D loss: 1.3915, G loss: 0.7097\n",
      "[1564/1778] D loss: 1.3871, G loss: 0.7060\n",
      "[1684/1778] D loss: 1.0646, G loss: 1.3966\n",
      "train error: \n",
      " D loss: 1.298203, G loss: 0.760736, D accuracy: 54.6%, cell accuracy: 21934.5%%, board accuracy: 86.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.283893, G loss: 0.802633, D accuracy: 55.5%, cell accuracy: 21917.8%%, board accuracy: 83.3% \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4022, G loss: 0.6138\n",
      "[124/1778] D loss: 1.3475, G loss: 0.7540\n",
      "[244/1778] D loss: 1.3901, G loss: 0.7272\n",
      "[364/1778] D loss: 1.3878, G loss: 0.6847\n",
      "[484/1778] D loss: 1.3966, G loss: 0.7091\n",
      "[604/1778] D loss: 0.7482, G loss: 1.5638\n",
      "[724/1778] D loss: 1.1083, G loss: 0.9908\n",
      "[844/1778] D loss: 1.0964, G loss: 0.9329\n",
      "[964/1778] D loss: 1.4013, G loss: 0.6023\n",
      "[1084/1778] D loss: 1.4027, G loss: 0.5999\n",
      "[1204/1778] D loss: 1.3802, G loss: 0.7761\n",
      "[1324/1778] D loss: 1.3895, G loss: 0.7652\n",
      "[1444/1778] D loss: 1.3885, G loss: 0.6901\n",
      "[1564/1778] D loss: 1.4127, G loss: 0.5960\n",
      "[1684/1778] D loss: 1.3907, G loss: 0.6483\n",
      "train error: \n",
      " D loss: 1.288689, G loss: 0.818232, D accuracy: 54.8%, cell accuracy: 21927.8%%, board accuracy: 88.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.270222, G loss: 0.865427, D accuracy: 55.3%, cell accuracy: 21909.7%%, board accuracy: 84.9% \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0690, G loss: 1.0883\n",
      "[124/1778] D loss: 1.3930, G loss: 0.6659\n",
      "[244/1778] D loss: 1.3926, G loss: 0.6996\n",
      "[364/1778] D loss: 1.3987, G loss: 0.6026\n",
      "[484/1778] D loss: 1.4086, G loss: 0.6495\n",
      "[604/1778] D loss: 1.3912, G loss: 0.7137\n",
      "[724/1778] D loss: 1.3887, G loss: 0.6234\n",
      "[844/1778] D loss: 1.3871, G loss: 0.7031\n",
      "[964/1778] D loss: 1.4051, G loss: 0.7949\n",
      "[1084/1778] D loss: 1.3891, G loss: 0.6565\n",
      "[1204/1778] D loss: 1.3933, G loss: 0.6648\n",
      "[1324/1778] D loss: 1.3879, G loss: 0.7136\n",
      "[1444/1778] D loss: 1.0614, G loss: 1.1777\n",
      "[1564/1778] D loss: 1.3870, G loss: 0.7032\n",
      "[1684/1778] D loss: 1.3910, G loss: 0.7143\n",
      "train error: \n",
      " D loss: 1.299685, G loss: 0.808917, D accuracy: 54.0%, cell accuracy: 21943.1%%, board accuracy: 90.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.285037, G loss: 0.848183, D accuracy: 55.0%, cell accuracy: 21929.5%%, board accuracy: 88.1% \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4060, G loss: 0.6868\n",
      "[124/1778] D loss: 1.3883, G loss: 0.6616\n",
      "[244/1778] D loss: 1.3934, G loss: 0.5897\n",
      "[364/1778] D loss: 1.0666, G loss: 1.1564\n",
      "[484/1778] D loss: 1.3877, G loss: 0.6737\n",
      "[604/1778] D loss: 1.3889, G loss: 0.6599\n",
      "[724/1778] D loss: 1.2866, G loss: 0.7878\n",
      "[844/1778] D loss: 0.7424, G loss: 1.6690\n",
      "[964/1778] D loss: 1.3922, G loss: 0.6333\n",
      "[1084/1778] D loss: 1.0837, G loss: 1.2418\n",
      "[1204/1778] D loss: 1.0701, G loss: 1.0493\n",
      "[1324/1778] D loss: 1.3921, G loss: 0.6560\n",
      "[1444/1778] D loss: 1.0760, G loss: 1.0204\n",
      "[1564/1778] D loss: 1.0831, G loss: 1.0664\n",
      "[1684/1778] D loss: 1.3881, G loss: 0.7229\n",
      "train error: \n",
      " D loss: 1.292785, G loss: 0.808399, D accuracy: 54.2%, cell accuracy: 21935.3%%, board accuracy: 90.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.278014, G loss: 0.849838, D accuracy: 55.0%, cell accuracy: 21916.9%%, board accuracy: 88.3% \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3966, G loss: 0.6994\n",
      "[124/1778] D loss: 1.0806, G loss: 1.0377\n",
      "[244/1778] D loss: 1.4304, G loss: 0.8389\n",
      "[364/1778] D loss: 1.0590, G loss: 1.1950\n",
      "[484/1778] D loss: 1.3893, G loss: 0.7175\n",
      "[604/1778] D loss: 1.3871, G loss: 0.7088\n",
      "[724/1778] D loss: 1.3884, G loss: 0.6663\n",
      "[844/1778] D loss: 1.3820, G loss: 0.6223\n",
      "[964/1778] D loss: 1.3933, G loss: 0.6861\n",
      "[1084/1778] D loss: 1.0880, G loss: 1.0097\n",
      "[1204/1778] D loss: 1.4847, G loss: 0.8281\n",
      "[1324/1778] D loss: 1.0787, G loss: 1.1841\n",
      "[1444/1778] D loss: 1.4001, G loss: 0.7388\n",
      "[1564/1778] D loss: 1.4066, G loss: 0.9455\n",
      "[1684/1778] D loss: 1.0587, G loss: 1.2698\n",
      "train error: \n",
      " D loss: 1.291065, G loss: 0.844329, D accuracy: 54.3%, cell accuracy: 21931.9%%, board accuracy: 89.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.276347, G loss: 0.892897, D accuracy: 55.0%, cell accuracy: 21912.6%%, board accuracy: 87.4% \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1228, G loss: 0.9327\n",
      "[124/1778] D loss: 0.7598, G loss: 1.8002\n",
      "[244/1778] D loss: 1.3908, G loss: 0.7205\n",
      "[364/1778] D loss: 0.7499, G loss: 1.5044\n",
      "[484/1778] D loss: 1.3906, G loss: 0.6853\n",
      "[604/1778] D loss: 1.3875, G loss: 0.6160\n",
      "[724/1778] D loss: 1.3982, G loss: 0.6630\n",
      "[844/1778] D loss: 1.3866, G loss: 0.6814\n",
      "[964/1778] D loss: 1.3941, G loss: 0.7044\n",
      "[1084/1778] D loss: 1.3926, G loss: 0.7544\n",
      "[1204/1778] D loss: 1.0807, G loss: 1.0023\n",
      "[1324/1778] D loss: 1.1153, G loss: 0.9757\n",
      "[1444/1778] D loss: 1.3952, G loss: 0.7034\n",
      "[1564/1778] D loss: 1.3529, G loss: 0.6471\n",
      "[1684/1778] D loss: 1.3894, G loss: 0.7218\n",
      "train error: \n",
      " D loss: 1.295169, G loss: 0.880636, D accuracy: 54.0%, cell accuracy: 21936.4%%, board accuracy: 88.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.281330, G loss: 0.930263, D accuracy: 55.1%, cell accuracy: 21918.5%%, board accuracy: 87.6% \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0772, G loss: 1.0832\n",
      "[124/1778] D loss: 1.3946, G loss: 0.6095\n",
      "[244/1778] D loss: 1.4157, G loss: 0.7910\n",
      "[364/1778] D loss: 1.3959, G loss: 0.7071\n",
      "[484/1778] D loss: 1.0536, G loss: 1.2846\n",
      "[604/1778] D loss: 1.3886, G loss: 0.7233\n",
      "[724/1778] D loss: 1.4310, G loss: 0.8194\n",
      "[844/1778] D loss: 1.0644, G loss: 1.2273\n",
      "[964/1778] D loss: 1.3866, G loss: 0.7164\n",
      "[1084/1778] D loss: 1.4007, G loss: 0.7500\n",
      "[1204/1778] D loss: 1.4068, G loss: 0.7733\n",
      "[1324/1778] D loss: 1.3864, G loss: 0.7073\n",
      "[1444/1778] D loss: 1.3904, G loss: 0.6884\n",
      "[1564/1778] D loss: 1.3890, G loss: 0.6751\n",
      "[1684/1778] D loss: 1.4013, G loss: 0.7954\n",
      "train error: \n",
      " D loss: 1.299634, G loss: 0.904120, D accuracy: 53.8%, cell accuracy: 21941.1%%, board accuracy: 88.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.290813, G loss: 0.936831, D accuracy: 54.4%, cell accuracy: 21932.9%%, board accuracy: 87.4% \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3991, G loss: 0.7428\n",
      "[124/1778] D loss: 1.3942, G loss: 0.6236\n",
      "[244/1778] D loss: 1.3892, G loss: 0.7556\n",
      "[364/1778] D loss: 1.0645, G loss: 1.1383\n",
      "[484/1778] D loss: 1.3890, G loss: 0.7230\n",
      "[604/1778] D loss: 1.3967, G loss: 0.6449\n",
      "[724/1778] D loss: 1.3877, G loss: 0.6881\n",
      "[844/1778] D loss: 0.7788, G loss: 1.3661\n",
      "[964/1778] D loss: 1.0559, G loss: 1.3555\n",
      "[1084/1778] D loss: 1.3868, G loss: 0.7293\n",
      "[1204/1778] D loss: 1.3882, G loss: 0.7285\n",
      "[1324/1778] D loss: 1.0479, G loss: 1.2525\n",
      "[1444/1778] D loss: 1.0540, G loss: 1.4425\n",
      "[1564/1778] D loss: 1.3985, G loss: 0.7128\n",
      "[1684/1778] D loss: 1.3874, G loss: 0.7039\n",
      "train error: \n",
      " D loss: 1.296490, G loss: 0.812953, D accuracy: 54.1%, cell accuracy: 21941.1%%, board accuracy: 89.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.276373, G loss: 0.862752, D accuracy: 55.2%, cell accuracy: 21929.1%%, board accuracy: 86.5% \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.5177, G loss: 0.8375\n",
      "[124/1778] D loss: 1.0574, G loss: 1.2094\n",
      "[244/1778] D loss: 1.1194, G loss: 0.8743\n",
      "[364/1778] D loss: 1.3891, G loss: 0.6906\n",
      "[484/1778] D loss: 1.0636, G loss: 1.1737\n",
      "[604/1778] D loss: 1.3934, G loss: 0.7259\n",
      "[724/1778] D loss: 1.3890, G loss: 0.6947\n",
      "[844/1778] D loss: 1.3888, G loss: 0.7104\n",
      "[964/1778] D loss: 1.3929, G loss: 0.7188\n",
      "[1084/1778] D loss: 1.3752, G loss: 0.7098\n",
      "[1204/1778] D loss: 1.4001, G loss: 0.7783\n",
      "[1324/1778] D loss: 1.3877, G loss: 0.7186\n",
      "[1444/1778] D loss: 1.0752, G loss: 1.2633\n",
      "[1564/1778] D loss: 1.0527, G loss: 1.3687\n",
      "[1684/1778] D loss: 1.3894, G loss: 0.7354\n",
      "train error: \n",
      " D loss: 1.293129, G loss: 0.839234, D accuracy: 53.9%, cell accuracy: 21942.2%%, board accuracy: 89.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.278572, G loss: 0.879114, D accuracy: 55.2%, cell accuracy: 21930.9%%, board accuracy: 87.4% \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4009, G loss: 0.6589\n",
      "[124/1778] D loss: 1.3965, G loss: 0.6456\n",
      "[244/1778] D loss: 1.3735, G loss: 0.7415\n",
      "[364/1778] D loss: 1.3956, G loss: 0.6083\n",
      "[484/1778] D loss: 1.3982, G loss: 0.6558\n",
      "[604/1778] D loss: 1.3948, G loss: 0.7223\n",
      "[724/1778] D loss: 1.0592, G loss: 1.2309\n",
      "[844/1778] D loss: 0.7278, G loss: 1.7748\n",
      "[964/1778] D loss: 1.3937, G loss: 0.7268\n",
      "[1084/1778] D loss: 1.3485, G loss: 0.7288\n",
      "[1204/1778] D loss: 1.4041, G loss: 0.6895\n",
      "[1324/1778] D loss: 1.3913, G loss: 0.7195\n",
      "[1444/1778] D loss: 1.0628, G loss: 1.1281\n",
      "[1564/1778] D loss: 1.0511, G loss: 1.2386\n",
      "[1684/1778] D loss: 1.4466, G loss: 0.8559\n",
      "train error: \n",
      " D loss: 1.306373, G loss: 0.980258, D accuracy: 53.9%, cell accuracy: 21947.6%%, board accuracy: 90.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.291843, G loss: 1.030864, D accuracy: 54.6%, cell accuracy: 21939.4%%, board accuracy: 89.0% \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0716, G loss: 1.3313\n",
      "[124/1778] D loss: 1.0853, G loss: 1.0054\n",
      "[244/1778] D loss: 1.3884, G loss: 0.6587\n",
      "[364/1778] D loss: 1.3905, G loss: 0.7102\n",
      "[484/1778] D loss: 1.0464, G loss: 1.4345\n",
      "[604/1778] D loss: 1.0768, G loss: 1.1609\n",
      "[724/1778] D loss: 1.4034, G loss: 0.5974\n",
      "[844/1778] D loss: 1.4685, G loss: 0.8935\n",
      "[964/1778] D loss: 1.3812, G loss: 0.6780\n",
      "[1084/1778] D loss: 1.3872, G loss: 0.7122\n",
      "[1204/1778] D loss: 1.3611, G loss: 0.7312\n",
      "[1324/1778] D loss: 1.3933, G loss: 0.7175\n",
      "[1444/1778] D loss: 1.0613, G loss: 1.2270\n",
      "[1564/1778] D loss: 1.0686, G loss: 1.0848\n",
      "[1684/1778] D loss: 1.0795, G loss: 0.9894\n",
      "train error: \n",
      " D loss: 1.293750, G loss: 0.833473, D accuracy: 54.3%, cell accuracy: 21934.9%%, board accuracy: 87.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.281841, G loss: 0.876238, D accuracy: 55.4%, cell accuracy: 21921.4%%, board accuracy: 85.1% \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3909, G loss: 0.7015\n",
      "[124/1778] D loss: 1.0669, G loss: 1.1403\n",
      "[244/1778] D loss: 1.3686, G loss: 0.6722\n",
      "[364/1778] D loss: 1.3904, G loss: 0.7145\n",
      "[484/1778] D loss: 1.3954, G loss: 0.7324\n",
      "[604/1778] D loss: 1.0738, G loss: 1.0631\n",
      "[724/1778] D loss: 1.3968, G loss: 0.7958\n",
      "[844/1778] D loss: 1.3903, G loss: 0.7014\n",
      "[964/1778] D loss: 1.3847, G loss: 0.6729\n",
      "[1084/1778] D loss: 1.3990, G loss: 0.7484\n",
      "[1204/1778] D loss: 1.3872, G loss: 0.6925\n",
      "[1324/1778] D loss: 1.3875, G loss: 0.7005\n",
      "[1444/1778] D loss: 1.1145, G loss: 0.9270\n",
      "[1564/1778] D loss: 1.0766, G loss: 1.2729\n",
      "[1684/1778] D loss: 1.3891, G loss: 0.6209\n",
      "train error: \n",
      " D loss: 1.309758, G loss: 0.771690, D accuracy: 54.3%, cell accuracy: 21940.9%%, board accuracy: 87.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.295412, G loss: 0.806992, D accuracy: 54.7%, cell accuracy: 21928.6%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3932, G loss: 0.6716\n",
      "[124/1778] D loss: 1.3905, G loss: 0.6734\n",
      "[244/1778] D loss: 1.3885, G loss: 0.7283\n",
      "[364/1778] D loss: 1.0622, G loss: 1.2173\n",
      "[484/1778] D loss: 1.0642, G loss: 1.1285\n",
      "[604/1778] D loss: 1.3897, G loss: 0.7206\n",
      "[724/1778] D loss: 1.3961, G loss: 0.6780\n",
      "[844/1778] D loss: 1.3906, G loss: 0.7068\n",
      "[964/1778] D loss: 1.3881, G loss: 0.7031\n",
      "[1084/1778] D loss: 1.3791, G loss: 0.6473\n",
      "[1204/1778] D loss: 1.3887, G loss: 0.7095\n",
      "[1324/1778] D loss: 1.0466, G loss: 1.1240\n",
      "[1444/1778] D loss: 1.3918, G loss: 0.6671\n",
      "[1564/1778] D loss: 1.3884, G loss: 0.6854\n",
      "[1684/1778] D loss: 1.3947, G loss: 0.7916\n",
      "train error: \n",
      " D loss: 1.293753, G loss: 0.794596, D accuracy: 54.6%, cell accuracy: 21934.8%%, board accuracy: 86.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.302609, G loss: 0.807150, D accuracy: 55.0%, cell accuracy: 21920.3%%, board accuracy: 84.2% \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3764, G loss: 0.6656\n",
      "[124/1778] D loss: 1.3895, G loss: 0.7585\n",
      "[244/1778] D loss: 1.4975, G loss: 0.9175\n",
      "[364/1778] D loss: 1.3882, G loss: 0.6672\n",
      "[484/1778] D loss: 1.0930, G loss: 1.1756\n",
      "[604/1778] D loss: 1.3922, G loss: 0.7145\n",
      "[724/1778] D loss: 1.0870, G loss: 1.1167\n",
      "[844/1778] D loss: 1.3618, G loss: 0.6813\n",
      "[964/1778] D loss: 1.3914, G loss: 0.6546\n",
      "[1084/1778] D loss: 1.3956, G loss: 0.6653\n",
      "[1204/1778] D loss: 1.3866, G loss: 0.6882\n",
      "[1324/1778] D loss: 1.4074, G loss: 0.8241\n",
      "[1444/1778] D loss: 1.1024, G loss: 0.9070\n",
      "[1564/1778] D loss: 1.3938, G loss: 0.7385\n",
      "[1684/1778] D loss: 1.4845, G loss: 0.8088\n",
      "train error: \n",
      " D loss: 1.295409, G loss: 0.883325, D accuracy: 54.1%, cell accuracy: 21947.7%%, board accuracy: 86.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.275161, G loss: 0.927826, D accuracy: 55.6%, cell accuracy: 21940.3%%, board accuracy: 85.1% \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "[4/1778] D loss: 0.7466, G loss: 1.6400\n",
      "[124/1778] D loss: 1.3788, G loss: 0.6588\n",
      "[244/1778] D loss: 1.0740, G loss: 1.0673\n",
      "[364/1778] D loss: 1.4050, G loss: 0.6072\n",
      "[484/1778] D loss: 1.0290, G loss: 1.4209\n",
      "[604/1778] D loss: 1.4005, G loss: 0.6427\n",
      "[724/1778] D loss: 1.3888, G loss: 0.6708\n",
      "[844/1778] D loss: 1.3889, G loss: 0.7658\n",
      "[964/1778] D loss: 1.3908, G loss: 0.6963\n",
      "[1084/1778] D loss: 0.7282, G loss: 1.8781\n",
      "[1204/1778] D loss: 1.3946, G loss: 0.7252\n",
      "[1324/1778] D loss: 0.7506, G loss: 1.6940\n",
      "[1444/1778] D loss: 1.3887, G loss: 0.6548\n",
      "[1564/1778] D loss: 1.3930, G loss: 0.7334\n",
      "[1684/1778] D loss: 1.4066, G loss: 0.6127\n",
      "train error: \n",
      " D loss: 1.298995, G loss: 0.824013, D accuracy: 54.2%, cell accuracy: 21943.6%%, board accuracy: 88.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.284039, G loss: 0.858419, D accuracy: 55.5%, cell accuracy: 21936.5%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3730, G loss: 0.6428\n",
      "[124/1778] D loss: 1.4323, G loss: 0.6092\n",
      "[244/1778] D loss: 1.3926, G loss: 0.7152\n",
      "[364/1778] D loss: 1.3870, G loss: 0.6975\n",
      "[484/1778] D loss: 1.3982, G loss: 0.7695\n",
      "[604/1778] D loss: 1.3981, G loss: 0.8005\n",
      "[724/1778] D loss: 1.3843, G loss: 0.7440\n",
      "[844/1778] D loss: 1.4046, G loss: 0.5794\n",
      "[964/1778] D loss: 1.3988, G loss: 0.6815\n",
      "[1084/1778] D loss: 1.0847, G loss: 1.1036\n",
      "[1204/1778] D loss: 1.1690, G loss: 0.8165\n",
      "[1324/1778] D loss: 1.3879, G loss: 0.6841\n",
      "[1444/1778] D loss: 1.3755, G loss: 0.6712\n",
      "[1564/1778] D loss: 1.0680, G loss: 1.1546\n",
      "[1684/1778] D loss: 1.0679, G loss: 1.0465\n",
      "train error: \n",
      " D loss: 1.291085, G loss: 0.875052, D accuracy: 54.9%, cell accuracy: 21943.4%%, board accuracy: 85.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.288114, G loss: 0.917722, D accuracy: 55.6%, cell accuracy: 21935.8%%, board accuracy: 84.7% \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2626, G loss: 1.0295\n",
      "[124/1778] D loss: 1.3519, G loss: 0.8513\n",
      "[244/1778] D loss: 0.8091, G loss: 1.1862\n",
      "[364/1778] D loss: 1.3932, G loss: 0.6596\n",
      "[484/1778] D loss: 1.3891, G loss: 0.7221\n",
      "[604/1778] D loss: 1.4080, G loss: 0.8062\n",
      "[724/1778] D loss: 1.3948, G loss: 0.7652\n",
      "[844/1778] D loss: 1.0760, G loss: 1.0117\n",
      "[964/1778] D loss: 1.3888, G loss: 0.7334\n",
      "[1084/1778] D loss: 1.3665, G loss: 0.7220\n",
      "[1204/1778] D loss: 1.3851, G loss: 0.6588\n",
      "[1324/1778] D loss: 1.3962, G loss: 0.6991\n",
      "[1444/1778] D loss: 1.3882, G loss: 0.6609\n",
      "[1564/1778] D loss: 1.3864, G loss: 0.6792\n",
      "[1684/1778] D loss: 1.3949, G loss: 0.7636\n",
      "train error: \n",
      " D loss: 1.312772, G loss: 1.018754, D accuracy: 53.8%, cell accuracy: 21930.2%%, board accuracy: 83.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.301963, G loss: 1.062694, D accuracy: 54.7%, cell accuracy: 21917.8%%, board accuracy: 82.0% \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4219, G loss: 0.8228\n",
      "[124/1778] D loss: 1.3833, G loss: 0.6838\n",
      "[244/1778] D loss: 1.3894, G loss: 0.7083\n",
      "[364/1778] D loss: 1.0629, G loss: 1.0617\n",
      "[484/1778] D loss: 1.4429, G loss: 0.9498\n",
      "[604/1778] D loss: 1.3982, G loss: 0.7688\n",
      "[724/1778] D loss: 1.3970, G loss: 0.6366\n",
      "[844/1778] D loss: 1.3716, G loss: 0.6594\n",
      "[964/1778] D loss: 1.3724, G loss: 0.7704\n",
      "[1084/1778] D loss: 1.3930, G loss: 0.6981\n",
      "[1204/1778] D loss: 1.3775, G loss: 0.6571\n",
      "[1324/1778] D loss: 1.0535, G loss: 1.5034\n",
      "[1444/1778] D loss: 1.3159, G loss: 0.7700\n",
      "[1564/1778] D loss: 1.3896, G loss: 0.7110\n",
      "[1684/1778] D loss: 1.3906, G loss: 0.6669\n",
      "train error: \n",
      " D loss: 1.294201, G loss: 0.828263, D accuracy: 54.3%, cell accuracy: 21942.1%%, board accuracy: 88.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.278788, G loss: 0.871178, D accuracy: 55.4%, cell accuracy: 21932.9%%, board accuracy: 88.1% \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0457, G loss: 1.3344\n",
      "[124/1778] D loss: 1.3868, G loss: 0.7296\n",
      "[244/1778] D loss: 1.3792, G loss: 0.8503\n",
      "[364/1778] D loss: 1.3925, G loss: 0.7430\n",
      "[484/1778] D loss: 1.3877, G loss: 0.6743\n",
      "[604/1778] D loss: 1.0655, G loss: 1.0817\n",
      "[724/1778] D loss: 1.3900, G loss: 0.6594\n",
      "[844/1778] D loss: 1.0654, G loss: 1.1434\n",
      "[964/1778] D loss: 1.3872, G loss: 0.6934\n",
      "[1084/1778] D loss: 1.0502, G loss: 1.3947\n",
      "[1204/1778] D loss: 1.3908, G loss: 0.7370\n",
      "[1324/1778] D loss: 1.0388, G loss: 1.2307\n",
      "[1444/1778] D loss: 1.3892, G loss: 0.7343\n",
      "[1564/1778] D loss: 1.4158, G loss: 0.7632\n",
      "[1684/1778] D loss: 1.3707, G loss: 0.7521\n",
      "train error: \n",
      " D loss: 1.291911, G loss: 0.833946, D accuracy: 54.2%, cell accuracy: 21940.6%%, board accuracy: 89.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.276090, G loss: 0.877716, D accuracy: 55.6%, cell accuracy: 21927.5%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3906, G loss: 0.6627\n",
      "[124/1778] D loss: 1.4155, G loss: 0.5896\n",
      "[244/1778] D loss: 1.4224, G loss: 0.8540\n",
      "[364/1778] D loss: 1.3871, G loss: 0.7202\n",
      "[484/1778] D loss: 1.3894, G loss: 0.7123\n",
      "[604/1778] D loss: 1.3883, G loss: 0.6153\n",
      "[724/1778] D loss: 1.4200, G loss: 0.7442\n",
      "[844/1778] D loss: 1.3890, G loss: 0.7419\n",
      "[964/1778] D loss: 1.3893, G loss: 0.6693\n",
      "[1084/1778] D loss: 1.0632, G loss: 1.2688\n",
      "[1204/1778] D loss: 1.3871, G loss: 0.6960\n",
      "[1324/1778] D loss: 1.0762, G loss: 1.1617\n",
      "[1444/1778] D loss: 0.7618, G loss: 1.5441\n",
      "[1564/1778] D loss: 1.3753, G loss: 0.7437\n",
      "[1684/1778] D loss: 1.3873, G loss: 0.7161\n",
      "train error: \n",
      " D loss: 1.300992, G loss: 0.754936, D accuracy: 54.4%, cell accuracy: 21944.5%%, board accuracy: 89.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.293833, G loss: 0.787527, D accuracy: 55.3%, cell accuracy: 21930.9%%, board accuracy: 87.2% \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3926, G loss: 0.6308\n",
      "[124/1778] D loss: 1.0687, G loss: 1.2255\n",
      "[244/1778] D loss: 1.0583, G loss: 1.2269\n",
      "[364/1778] D loss: 1.3977, G loss: 0.6238\n",
      "[484/1778] D loss: 1.3911, G loss: 0.7184\n",
      "[604/1778] D loss: 1.3953, G loss: 0.7197\n",
      "[724/1778] D loss: 1.3835, G loss: 0.6783\n",
      "[844/1778] D loss: 1.3855, G loss: 0.6715\n",
      "[964/1778] D loss: 1.3879, G loss: 0.6419\n",
      "[1084/1778] D loss: 1.4379, G loss: 0.5133\n",
      "[1204/1778] D loss: 1.2959, G loss: 1.0724\n",
      "[1324/1778] D loss: 1.0765, G loss: 1.2265\n",
      "[1444/1778] D loss: 1.3725, G loss: 0.7287\n",
      "[1564/1778] D loss: 1.0857, G loss: 0.9639\n",
      "[1684/1778] D loss: 1.3989, G loss: 0.7105\n",
      "train error: \n",
      " D loss: 1.291493, G loss: 0.860399, D accuracy: 54.3%, cell accuracy: 21944.0%%, board accuracy: 88.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.286063, G loss: 0.880158, D accuracy: 55.0%, cell accuracy: 21930.0%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3927, G loss: 0.7145\n",
      "[124/1778] D loss: 1.0461, G loss: 1.1832\n",
      "[244/1778] D loss: 1.3769, G loss: 0.6888\n",
      "[364/1778] D loss: 1.3876, G loss: 0.7031\n",
      "[484/1778] D loss: 1.4521, G loss: 0.8121\n",
      "[604/1778] D loss: 1.0603, G loss: 1.1931\n",
      "[724/1778] D loss: 1.3683, G loss: 0.7221\n",
      "[844/1778] D loss: 0.4089, G loss: 2.2662\n",
      "[964/1778] D loss: 1.3780, G loss: 0.6622\n",
      "[1084/1778] D loss: 1.3965, G loss: 0.6435\n",
      "[1204/1778] D loss: 1.3878, G loss: 0.6866\n",
      "[1324/1778] D loss: 0.7267, G loss: 1.9398\n",
      "[1444/1778] D loss: 1.3950, G loss: 0.8168\n",
      "[1564/1778] D loss: 1.3886, G loss: 0.6439\n",
      "[1684/1778] D loss: 1.3983, G loss: 0.6448\n",
      "train error: \n",
      " D loss: 1.296756, G loss: 0.816908, D accuracy: 54.6%, cell accuracy: 21945.3%%, board accuracy: 87.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.274208, G loss: 0.857886, D accuracy: 55.7%, cell accuracy: 21936.3%%, board accuracy: 85.8% \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3935, G loss: 0.6484\n",
      "[124/1778] D loss: 1.3925, G loss: 0.6733\n",
      "[244/1778] D loss: 1.0583, G loss: 1.2948\n",
      "[364/1778] D loss: 1.4092, G loss: 0.7638\n",
      "[484/1778] D loss: 1.0771, G loss: 0.9808\n",
      "[604/1778] D loss: 1.0655, G loss: 1.1181\n",
      "[724/1778] D loss: 1.3883, G loss: 0.6591\n",
      "[844/1778] D loss: 1.4935, G loss: 0.7906\n",
      "[964/1778] D loss: 1.4709, G loss: 0.5384\n",
      "[1084/1778] D loss: 1.3876, G loss: 0.6788\n",
      "[1204/1778] D loss: 1.3902, G loss: 0.7309\n",
      "[1324/1778] D loss: 1.3919, G loss: 0.7442\n",
      "[1444/1778] D loss: 1.3958, G loss: 0.6717\n",
      "[1564/1778] D loss: 1.3884, G loss: 0.6273\n",
      "[1684/1778] D loss: 1.3893, G loss: 0.6602\n",
      "train error: \n",
      " D loss: 1.291005, G loss: 0.824642, D accuracy: 54.7%, cell accuracy: 21948.0%%, board accuracy: 87.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.288002, G loss: 0.859589, D accuracy: 56.0%, cell accuracy: 21936.9%%, board accuracy: 85.8% \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3876, G loss: 0.7391\n",
      "[124/1778] D loss: 1.3978, G loss: 0.7263\n",
      "[244/1778] D loss: 1.1409, G loss: 1.3718\n",
      "[364/1778] D loss: 1.1084, G loss: 0.8779\n",
      "[484/1778] D loss: 1.2607, G loss: 0.9644\n",
      "[604/1778] D loss: 1.3880, G loss: 0.7445\n",
      "[724/1778] D loss: 1.3923, G loss: 0.6712\n",
      "[844/1778] D loss: 1.0263, G loss: 1.7380\n",
      "[964/1778] D loss: 1.3950, G loss: 0.6198\n",
      "[1084/1778] D loss: 1.2739, G loss: 0.7469\n",
      "[1204/1778] D loss: 1.4115, G loss: 0.6221\n",
      "[1324/1778] D loss: 1.3813, G loss: 0.7384\n",
      "[1444/1778] D loss: 1.3937, G loss: 0.6049\n",
      "[1564/1778] D loss: 1.4967, G loss: 0.7986\n",
      "[1684/1778] D loss: 0.7820, G loss: 1.5264\n",
      "train error: \n",
      " D loss: 1.297793, G loss: 0.904007, D accuracy: 54.3%, cell accuracy: 21939.4%%, board accuracy: 89.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.283828, G loss: 0.946065, D accuracy: 55.2%, cell accuracy: 21923.9%%, board accuracy: 85.6% \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3929, G loss: 0.7014\n",
      "[124/1778] D loss: 1.0608, G loss: 1.1879\n",
      "[244/1778] D loss: 1.3931, G loss: 0.6335\n",
      "[364/1778] D loss: 1.0841, G loss: 0.9757\n",
      "[484/1778] D loss: 1.4006, G loss: 0.6255\n",
      "[604/1778] D loss: 1.0620, G loss: 1.1348\n",
      "[724/1778] D loss: 1.3805, G loss: 0.6651\n",
      "[844/1778] D loss: 1.3569, G loss: 0.6838\n",
      "[964/1778] D loss: 1.4030, G loss: 0.8060\n",
      "[1084/1778] D loss: 1.3879, G loss: 0.6864\n",
      "[1204/1778] D loss: 1.4019, G loss: 0.6086\n",
      "[1324/1778] D loss: 1.0581, G loss: 1.2159\n",
      "[1444/1778] D loss: 1.3937, G loss: 0.7271\n",
      "[1564/1778] D loss: 1.3945, G loss: 0.7626\n",
      "[1684/1778] D loss: 1.3880, G loss: 0.7522\n",
      "train error: \n",
      " D loss: 1.294167, G loss: 0.857518, D accuracy: 54.1%, cell accuracy: 21941.1%%, board accuracy: 89.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.282393, G loss: 0.892365, D accuracy: 54.7%, cell accuracy: 21924.8%%, board accuracy: 88.5% \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3866, G loss: 0.7196\n",
      "[124/1778] D loss: 1.0626, G loss: 1.4409\n",
      "[244/1778] D loss: 1.3933, G loss: 0.7394\n",
      "[364/1778] D loss: 1.4002, G loss: 0.7943\n",
      "[484/1778] D loss: 1.3916, G loss: 0.6302\n",
      "[604/1778] D loss: 1.3887, G loss: 0.6662\n",
      "[724/1778] D loss: 1.0683, G loss: 1.0582\n",
      "[844/1778] D loss: 1.3926, G loss: 0.6737\n",
      "[964/1778] D loss: 1.4007, G loss: 0.6373\n",
      "[1084/1778] D loss: 0.7941, G loss: 1.5059\n",
      "[1204/1778] D loss: 1.0630, G loss: 1.1805\n",
      "[1324/1778] D loss: 1.3992, G loss: 0.7919\n",
      "[1444/1778] D loss: 1.3901, G loss: 0.7405\n",
      "[1564/1778] D loss: 1.3916, G loss: 0.7454\n",
      "[1684/1778] D loss: 1.3921, G loss: 0.7385\n",
      "train error: \n",
      " D loss: 1.293010, G loss: 0.896493, D accuracy: 54.0%, cell accuracy: 21937.5%%, board accuracy: 89.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.275065, G loss: 0.967713, D accuracy: 55.1%, cell accuracy: 21915.3%%, board accuracy: 87.2% \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0799, G loss: 1.1035\n",
      "[124/1778] D loss: 1.3990, G loss: 0.6259\n",
      "[244/1778] D loss: 1.3889, G loss: 0.6866\n",
      "[364/1778] D loss: 1.3916, G loss: 0.6517\n",
      "[484/1778] D loss: 1.0495, G loss: 1.4031\n",
      "[604/1778] D loss: 1.0997, G loss: 1.2320\n",
      "[724/1778] D loss: 1.3918, G loss: 0.7546\n",
      "[844/1778] D loss: 1.3868, G loss: 0.6948\n",
      "[964/1778] D loss: 1.3884, G loss: 0.6671\n",
      "[1084/1778] D loss: 1.3948, G loss: 0.6423\n",
      "[1204/1778] D loss: 1.3980, G loss: 0.7757\n",
      "[1324/1778] D loss: 1.2847, G loss: 0.8855\n",
      "[1444/1778] D loss: 1.3869, G loss: 0.6851\n",
      "[1564/1778] D loss: 1.0632, G loss: 1.6103\n",
      "[1684/1778] D loss: 1.3728, G loss: 0.6600\n",
      "train error: \n",
      " D loss: 1.290110, G loss: 0.851554, D accuracy: 54.3%, cell accuracy: 21936.7%%, board accuracy: 88.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.271268, G loss: 0.906593, D accuracy: 55.6%, cell accuracy: 21918.2%%, board accuracy: 86.0% \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0758, G loss: 1.0171\n",
      "[124/1778] D loss: 1.0669, G loss: 1.1681\n",
      "[244/1778] D loss: 1.3889, G loss: 0.7063\n",
      "[364/1778] D loss: 1.3971, G loss: 0.7675\n",
      "[484/1778] D loss: 1.3959, G loss: 0.7087\n",
      "[604/1778] D loss: 1.3872, G loss: 0.6952\n",
      "[724/1778] D loss: 1.3869, G loss: 0.6840\n",
      "[844/1778] D loss: 1.3894, G loss: 0.7037\n",
      "[964/1778] D loss: 1.3898, G loss: 0.6766\n",
      "[1084/1778] D loss: 1.3940, G loss: 0.6503\n",
      "[1204/1778] D loss: 1.3871, G loss: 0.7318\n",
      "[1324/1778] D loss: 1.3925, G loss: 0.7265\n",
      "[1444/1778] D loss: 1.3933, G loss: 0.6651\n",
      "[1564/1778] D loss: 1.3931, G loss: 0.7280\n",
      "[1684/1778] D loss: 1.3939, G loss: 0.7217\n",
      "train error: \n",
      " D loss: 1.296939, G loss: 0.804708, D accuracy: 54.2%, cell accuracy: 21942.2%%, board accuracy: 89.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.277219, G loss: 0.865255, D accuracy: 55.2%, cell accuracy: 21926.6%%, board accuracy: 84.7% \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0660, G loss: 1.1042\n",
      "[124/1778] D loss: 1.4672, G loss: 0.6456\n",
      "[244/1778] D loss: 1.0793, G loss: 1.0337\n",
      "[364/1778] D loss: 1.3743, G loss: 0.7161\n",
      "[484/1778] D loss: 1.3904, G loss: 0.7319\n",
      "[604/1778] D loss: 1.0790, G loss: 1.2778\n",
      "[724/1778] D loss: 1.3890, G loss: 0.6572\n",
      "[844/1778] D loss: 1.3884, G loss: 0.7685\n",
      "[964/1778] D loss: 1.3773, G loss: 0.6943\n",
      "[1084/1778] D loss: 1.3926, G loss: 0.6217\n",
      "[1204/1778] D loss: 1.2498, G loss: 0.9087\n",
      "[1324/1778] D loss: 1.3900, G loss: 0.6969\n",
      "[1444/1778] D loss: 1.0652, G loss: 1.2981\n",
      "[1564/1778] D loss: 1.2902, G loss: 0.8189\n",
      "[1684/1778] D loss: 1.3946, G loss: 0.6378\n",
      "train error: \n",
      " D loss: 1.300206, G loss: 0.791044, D accuracy: 54.0%, cell accuracy: 21939.1%%, board accuracy: 90.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.274227, G loss: 0.862972, D accuracy: 55.4%, cell accuracy: 21918.5%%, board accuracy: 87.8% \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3950, G loss: 0.6338\n",
      "[124/1778] D loss: 1.0589, G loss: 1.4255\n",
      "[244/1778] D loss: 1.3885, G loss: 0.6874\n",
      "[364/1778] D loss: 1.0472, G loss: 1.2081\n",
      "[484/1778] D loss: 1.0492, G loss: 1.5637\n",
      "[604/1778] D loss: 1.3930, G loss: 0.6843\n",
      "[724/1778] D loss: 0.7261, G loss: 1.9312\n",
      "[844/1778] D loss: 1.3904, G loss: 0.7311\n",
      "[964/1778] D loss: 1.3897, G loss: 0.7447\n",
      "[1084/1778] D loss: 1.0594, G loss: 1.2093\n",
      "[1204/1778] D loss: 1.3936, G loss: 0.7285\n",
      "[1324/1778] D loss: 1.0692, G loss: 1.0965\n",
      "[1444/1778] D loss: 1.3930, G loss: 0.6742\n",
      "[1564/1778] D loss: 1.3912, G loss: 0.7340\n",
      "[1684/1778] D loss: 1.3894, G loss: 0.6538\n",
      "train error: \n",
      " D loss: 1.296662, G loss: 0.886753, D accuracy: 53.8%, cell accuracy: 21940.0%%, board accuracy: 91.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.276517, G loss: 0.965129, D accuracy: 54.8%, cell accuracy: 21920.5%%, board accuracy: 89.4% \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0523, G loss: 1.4633\n",
      "[124/1778] D loss: 1.0605, G loss: 1.1547\n",
      "[244/1778] D loss: 1.4138, G loss: 0.5800\n",
      "[364/1778] D loss: 1.3989, G loss: 0.7356\n",
      "[484/1778] D loss: 1.3872, G loss: 0.6900\n",
      "[604/1778] D loss: 1.3913, G loss: 0.7428\n",
      "[724/1778] D loss: 1.0396, G loss: 1.3719\n",
      "[844/1778] D loss: 1.3877, G loss: 0.6764\n",
      "[964/1778] D loss: 1.0780, G loss: 1.2236\n",
      "[1084/1778] D loss: 1.3892, G loss: 0.6682\n",
      "[1204/1778] D loss: 1.3878, G loss: 0.6869\n",
      "[1324/1778] D loss: 1.3791, G loss: 0.7325\n",
      "[1444/1778] D loss: 1.3865, G loss: 0.7110\n",
      "[1564/1778] D loss: 1.3893, G loss: 0.7325\n",
      "[1684/1778] D loss: 1.3997, G loss: 0.7658\n",
      "train error: \n",
      " D loss: 1.296301, G loss: 0.870498, D accuracy: 54.0%, cell accuracy: 21942.4%%, board accuracy: 91.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.284754, G loss: 0.925098, D accuracy: 54.8%, cell accuracy: 21930.9%%, board accuracy: 89.9% \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3890, G loss: 0.7053\n",
      "[124/1778] D loss: 0.3918, G loss: 2.7507\n",
      "[244/1778] D loss: 1.2903, G loss: 0.9173\n",
      "[364/1778] D loss: 1.3913, G loss: 0.7568\n",
      "[484/1778] D loss: 1.3995, G loss: 0.6315\n",
      "[604/1778] D loss: 1.3957, G loss: 0.6332\n",
      "[724/1778] D loss: 1.3914, G loss: 0.6278\n",
      "[844/1778] D loss: 1.3901, G loss: 0.6391\n",
      "[964/1778] D loss: 1.3997, G loss: 0.6140\n",
      "[1084/1778] D loss: 1.3900, G loss: 0.7542\n",
      "[1204/1778] D loss: 1.3907, G loss: 0.6758\n",
      "[1324/1778] D loss: 1.0558, G loss: 1.2549\n",
      "[1444/1778] D loss: 1.3986, G loss: 0.6460\n",
      "[1564/1778] D loss: 1.1102, G loss: 1.4170\n",
      "[1684/1778] D loss: 1.4196, G loss: 0.7925\n",
      "train error: \n",
      " D loss: 1.301932, G loss: 0.896772, D accuracy: 54.0%, cell accuracy: 21941.5%%, board accuracy: 91.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.281326, G loss: 0.960799, D accuracy: 55.1%, cell accuracy: 21920.3%%, board accuracy: 88.7% \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3982, G loss: 0.7922\n",
      "[124/1778] D loss: 1.0622, G loss: 1.1842\n",
      "[244/1778] D loss: 1.4568, G loss: 1.0145\n",
      "[364/1778] D loss: 1.4316, G loss: 0.8173\n",
      "[484/1778] D loss: 1.0743, G loss: 1.0089\n",
      "[604/1778] D loss: 1.0644, G loss: 1.1259\n",
      "[724/1778] D loss: 1.3883, G loss: 0.7191\n",
      "[844/1778] D loss: 1.3862, G loss: 0.7414\n",
      "[964/1778] D loss: 1.3869, G loss: 0.6718\n",
      "[1084/1778] D loss: 1.3902, G loss: 0.6827\n",
      "[1204/1778] D loss: 1.3914, G loss: 0.7411\n",
      "[1324/1778] D loss: 1.3916, G loss: 0.7292\n",
      "[1444/1778] D loss: 1.0589, G loss: 1.1994\n",
      "[1564/1778] D loss: 1.3929, G loss: 0.6100\n",
      "[1684/1778] D loss: 1.0529, G loss: 1.3796\n",
      "train error: \n",
      " D loss: 1.294656, G loss: 0.842131, D accuracy: 53.9%, cell accuracy: 21952.5%%, board accuracy: 91.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.283930, G loss: 0.879228, D accuracy: 54.7%, cell accuracy: 21938.3%%, board accuracy: 89.4% \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3931, G loss: 0.6321\n",
      "[124/1778] D loss: 1.3919, G loss: 0.7821\n",
      "[244/1778] D loss: 1.3933, G loss: 0.7280\n",
      "[364/1778] D loss: 1.0738, G loss: 1.1373\n",
      "[484/1778] D loss: 1.4064, G loss: 0.7781\n",
      "[604/1778] D loss: 1.3882, G loss: 0.6870\n",
      "[724/1778] D loss: 1.3954, G loss: 0.7696\n",
      "[844/1778] D loss: 1.3906, G loss: 0.6570\n",
      "[964/1778] D loss: 1.3953, G loss: 0.6939\n",
      "[1084/1778] D loss: 1.3861, G loss: 0.6999\n",
      "[1204/1778] D loss: 1.3934, G loss: 0.7522\n",
      "[1324/1778] D loss: 1.3964, G loss: 0.6601\n",
      "[1444/1778] D loss: 1.3938, G loss: 0.6025\n",
      "[1564/1778] D loss: 1.3962, G loss: 0.7447\n",
      "[1684/1778] D loss: 0.7328, G loss: 1.6891\n",
      "train error: \n",
      " D loss: 1.283599, G loss: 0.861046, D accuracy: 55.3%, cell accuracy: 21927.9%%, board accuracy: 86.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.267617, G loss: 0.910003, D accuracy: 56.5%, cell accuracy: 21901.8%%, board accuracy: 82.7% \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3870, G loss: 0.7108\n",
      "[124/1778] D loss: 1.0257, G loss: 1.3461\n",
      "[244/1778] D loss: 1.3930, G loss: 0.6652\n",
      "[364/1778] D loss: 1.3883, G loss: 0.7372\n",
      "[484/1778] D loss: 1.3897, G loss: 0.7057\n",
      "[604/1778] D loss: 1.3786, G loss: 0.6970\n",
      "[724/1778] D loss: 1.3817, G loss: 0.7462\n",
      "[844/1778] D loss: 1.0134, G loss: 1.2169\n",
      "[964/1778] D loss: 1.3884, G loss: 0.6640\n",
      "[1084/1778] D loss: 1.3908, G loss: 0.7539\n",
      "[1204/1778] D loss: 1.3874, G loss: 0.6738\n",
      "[1324/1778] D loss: 1.3867, G loss: 0.7206\n",
      "[1444/1778] D loss: 1.0648, G loss: 1.0960\n",
      "[1564/1778] D loss: 1.3878, G loss: 0.6641\n",
      "[1684/1778] D loss: 1.3803, G loss: 0.7617\n",
      "train error: \n",
      " D loss: 1.280769, G loss: 0.937264, D accuracy: 54.6%, cell accuracy: 21921.3%%, board accuracy: 84.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.263281, G loss: 1.000180, D accuracy: 56.2%, cell accuracy: 21890.8%%, board accuracy: 80.2% \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0522, G loss: 1.3703\n",
      "[124/1778] D loss: 1.0591, G loss: 1.4505\n",
      "[244/1778] D loss: 1.0608, G loss: 1.3075\n",
      "[364/1778] D loss: 1.3897, G loss: 0.6903\n",
      "[484/1778] D loss: 1.3025, G loss: 0.7574\n",
      "[604/1778] D loss: 1.4027, G loss: 0.5899\n",
      "[724/1778] D loss: 1.3916, G loss: 0.7405\n",
      "[844/1778] D loss: 1.3935, G loss: 0.7830\n",
      "[964/1778] D loss: 1.5022, G loss: 0.8883\n",
      "[1084/1778] D loss: 1.3932, G loss: 0.6171\n",
      "[1204/1778] D loss: 1.3873, G loss: 0.6735\n",
      "[1324/1778] D loss: 1.4003, G loss: 0.7806\n",
      "[1444/1778] D loss: 1.3942, G loss: 0.7500\n",
      "[1564/1778] D loss: 1.0613, G loss: 1.3041\n",
      "[1684/1778] D loss: 1.3883, G loss: 0.7410\n",
      "train error: \n",
      " D loss: 1.293730, G loss: 0.851864, D accuracy: 53.8%, cell accuracy: 21953.1%%, board accuracy: 91.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.284695, G loss: 0.881313, D accuracy: 54.6%, cell accuracy: 21939.9%%, board accuracy: 88.7% \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0570, G loss: 1.2367\n",
      "[124/1778] D loss: 1.0894, G loss: 0.9648\n",
      "[244/1778] D loss: 1.3902, G loss: 0.6776\n",
      "[364/1778] D loss: 1.3935, G loss: 0.6084\n",
      "[484/1778] D loss: 1.3875, G loss: 0.6863\n",
      "[604/1778] D loss: 1.3915, G loss: 0.6474\n",
      "[724/1778] D loss: 1.3788, G loss: 0.6769\n",
      "[844/1778] D loss: 1.4024, G loss: 0.7117\n",
      "[964/1778] D loss: 1.3911, G loss: 0.7463\n",
      "[1084/1778] D loss: 0.7422, G loss: 1.5885\n",
      "[1204/1778] D loss: 1.3879, G loss: 0.7086\n",
      "[1324/1778] D loss: 1.3971, G loss: 0.5847\n",
      "[1444/1778] D loss: 1.3967, G loss: 0.6165\n",
      "[1564/1778] D loss: 1.3884, G loss: 0.6553\n",
      "[1684/1778] D loss: 1.3870, G loss: 0.6603\n",
      "train error: \n",
      " D loss: 1.294119, G loss: 0.816186, D accuracy: 54.7%, cell accuracy: 21932.1%%, board accuracy: 87.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.276336, G loss: 0.864191, D accuracy: 55.9%, cell accuracy: 21911.0%%, board accuracy: 85.4% \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3879, G loss: 0.6868\n",
      "[124/1778] D loss: 1.3926, G loss: 0.6222\n",
      "[244/1778] D loss: 1.3956, G loss: 0.8071\n",
      "[364/1778] D loss: 1.3785, G loss: 0.6483\n",
      "[484/1778] D loss: 1.0559, G loss: 1.3948\n",
      "[604/1778] D loss: 1.3884, G loss: 0.6756\n",
      "[724/1778] D loss: 1.0477, G loss: 1.5830\n",
      "[844/1778] D loss: 1.0725, G loss: 1.0361\n",
      "[964/1778] D loss: 1.3870, G loss: 0.7034\n",
      "[1084/1778] D loss: 1.3889, G loss: 0.6893\n",
      "[1204/1778] D loss: 1.3857, G loss: 0.8182\n",
      "[1324/1778] D loss: 1.4091, G loss: 0.8304\n",
      "[1444/1778] D loss: 1.0659, G loss: 1.5240\n",
      "[1564/1778] D loss: 1.3890, G loss: 0.7173\n",
      "[1684/1778] D loss: 1.3910, G loss: 0.7490\n",
      "train error: \n",
      " D loss: 1.297657, G loss: 0.914428, D accuracy: 53.7%, cell accuracy: 21942.2%%, board accuracy: 90.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.289194, G loss: 0.958274, D accuracy: 55.2%, cell accuracy: 21930.9%%, board accuracy: 88.7% \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3838, G loss: 0.7543\n",
      "[124/1778] D loss: 1.3942, G loss: 0.6730\n",
      "[244/1778] D loss: 1.2198, G loss: 0.8331\n",
      "[364/1778] D loss: 1.0653, G loss: 1.2411\n",
      "[484/1778] D loss: 1.3869, G loss: 0.7387\n",
      "[604/1778] D loss: 1.3888, G loss: 0.7099\n",
      "[724/1778] D loss: 1.3950, G loss: 0.6849\n",
      "[844/1778] D loss: 1.0520, G loss: 1.3866\n",
      "[964/1778] D loss: 1.3898, G loss: 0.7375\n",
      "[1084/1778] D loss: 1.3915, G loss: 0.6633\n",
      "[1204/1778] D loss: 1.3937, G loss: 0.7262\n",
      "[1324/1778] D loss: 1.3908, G loss: 0.7694\n",
      "[1444/1778] D loss: 1.3900, G loss: 0.7074\n",
      "[1564/1778] D loss: 1.3978, G loss: 0.6864\n",
      "[1684/1778] D loss: 1.3890, G loss: 0.7231\n",
      "train error: \n",
      " D loss: 1.314930, G loss: 1.041989, D accuracy: 53.8%, cell accuracy: 21943.3%%, board accuracy: 89.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.298239, G loss: 1.081510, D accuracy: 54.5%, cell accuracy: 21928.2%%, board accuracy: 87.2% \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0789, G loss: 1.7546\n",
      "[124/1778] D loss: 1.3998, G loss: 0.6705\n",
      "[244/1778] D loss: 1.3917, G loss: 0.6160\n",
      "[364/1778] D loss: 1.3910, G loss: 0.7975\n",
      "[484/1778] D loss: 1.3881, G loss: 0.7202\n",
      "[604/1778] D loss: 1.0583, G loss: 1.2326\n",
      "[724/1778] D loss: 1.3921, G loss: 0.7568\n",
      "[844/1778] D loss: 1.0493, G loss: 1.5272\n",
      "[964/1778] D loss: 1.3923, G loss: 0.7730\n",
      "[1084/1778] D loss: 1.0485, G loss: 1.4252\n",
      "[1204/1778] D loss: 1.3897, G loss: 0.7338\n",
      "[1324/1778] D loss: 1.0476, G loss: 1.4441\n",
      "[1444/1778] D loss: 1.0818, G loss: 1.6723\n",
      "[1564/1778] D loss: 1.3908, G loss: 0.6323\n",
      "[1684/1778] D loss: 1.3817, G loss: 0.7187\n",
      "train error: \n",
      " D loss: 1.285105, G loss: 0.869742, D accuracy: 54.9%, cell accuracy: 21923.4%%, board accuracy: 87.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.273190, G loss: 0.912881, D accuracy: 56.3%, cell accuracy: 21894.4%%, board accuracy: 82.9% \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0468, G loss: 1.4019\n",
      "[124/1778] D loss: 1.4003, G loss: 0.5659\n",
      "[244/1778] D loss: 1.0793, G loss: 1.0870\n",
      "[364/1778] D loss: 1.0575, G loss: 1.1181\n",
      "[484/1778] D loss: 1.4002, G loss: 0.6227\n",
      "[604/1778] D loss: 1.3853, G loss: 1.0706\n",
      "[724/1778] D loss: 1.3888, G loss: 0.6968\n",
      "[844/1778] D loss: 1.3365, G loss: 0.7311\n",
      "[964/1778] D loss: 0.8050, G loss: 1.4300\n",
      "[1084/1778] D loss: 1.3908, G loss: 0.6475\n",
      "[1204/1778] D loss: 1.2765, G loss: 0.8997\n",
      "[1324/1778] D loss: 1.3889, G loss: 0.6746\n",
      "[1444/1778] D loss: 1.3915, G loss: 0.6979\n",
      "[1564/1778] D loss: 1.0842, G loss: 1.1230\n",
      "[1684/1778] D loss: 1.3916, G loss: 0.6827\n",
      "train error: \n",
      " D loss: 1.301758, G loss: 0.893390, D accuracy: 53.8%, cell accuracy: 21936.2%%, board accuracy: 88.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.291535, G loss: 0.920194, D accuracy: 54.6%, cell accuracy: 21921.4%%, board accuracy: 86.0% \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3844, G loss: 0.7356\n",
      "[124/1778] D loss: 1.0839, G loss: 1.1558\n",
      "[244/1778] D loss: 1.3882, G loss: 0.7561\n",
      "[364/1778] D loss: 1.3957, G loss: 0.6760\n",
      "[484/1778] D loss: 1.3913, G loss: 0.5958\n",
      "[604/1778] D loss: 1.4095, G loss: 0.8055\n",
      "[724/1778] D loss: 1.0805, G loss: 0.9776\n",
      "[844/1778] D loss: 1.3948, G loss: 0.6125\n",
      "[964/1778] D loss: 1.3069, G loss: 0.8587\n",
      "[1084/1778] D loss: 1.1010, G loss: 0.9116\n",
      "[1204/1778] D loss: 1.0458, G loss: 1.4406\n",
      "[1324/1778] D loss: 1.0742, G loss: 1.0645\n",
      "[1444/1778] D loss: 1.3903, G loss: 0.7011\n",
      "[1564/1778] D loss: 1.3873, G loss: 0.6917\n",
      "[1684/1778] D loss: 1.3881, G loss: 0.7034\n",
      "train error: \n",
      " D loss: 1.295186, G loss: 0.817884, D accuracy: 54.0%, cell accuracy: 21951.7%%, board accuracy: 89.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.285803, G loss: 0.841021, D accuracy: 55.4%, cell accuracy: 21938.1%%, board accuracy: 85.8% \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3885, G loss: 0.6530\n",
      "[124/1778] D loss: 1.3881, G loss: 0.6973\n",
      "[244/1778] D loss: 1.3284, G loss: 0.7108\n",
      "[364/1778] D loss: 1.0985, G loss: 1.2113\n",
      "[484/1778] D loss: 1.3935, G loss: 0.5752\n",
      "[604/1778] D loss: 1.0651, G loss: 1.2736\n",
      "[724/1778] D loss: 1.0625, G loss: 1.1784\n",
      "[844/1778] D loss: 1.3885, G loss: 0.7541\n",
      "[964/1778] D loss: 1.3901, G loss: 0.7607\n",
      "[1084/1778] D loss: 1.2088, G loss: 1.1372\n",
      "[1204/1778] D loss: 1.4123, G loss: 0.5740\n",
      "[1324/1778] D loss: 1.3877, G loss: 0.7077\n",
      "[1444/1778] D loss: 1.3871, G loss: 0.7009\n",
      "[1564/1778] D loss: 1.3876, G loss: 0.6971\n",
      "[1684/1778] D loss: 1.3922, G loss: 0.7438\n",
      "train error: \n",
      " D loss: 1.296687, G loss: 0.788992, D accuracy: 53.9%, cell accuracy: 21947.0%%, board accuracy: 90.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.279218, G loss: 0.815352, D accuracy: 54.8%, cell accuracy: 21936.9%%, board accuracy: 89.0% \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3902, G loss: 0.6614\n",
      "[124/1778] D loss: 1.3881, G loss: 0.6596\n",
      "[244/1778] D loss: 1.0566, G loss: 1.2102\n",
      "[364/1778] D loss: 1.0745, G loss: 1.0181\n",
      "[484/1778] D loss: 1.3793, G loss: 0.6681\n",
      "[604/1778] D loss: 1.3881, G loss: 0.7379\n",
      "[724/1778] D loss: 1.3936, G loss: 0.6432\n",
      "[844/1778] D loss: 1.4055, G loss: 0.7270\n",
      "[964/1778] D loss: 1.4442, G loss: 0.8368\n",
      "[1084/1778] D loss: 1.3936, G loss: 0.7473\n",
      "[1204/1778] D loss: 0.7237, G loss: 1.8558\n",
      "[1324/1778] D loss: 1.3959, G loss: 0.7096\n",
      "[1444/1778] D loss: 1.3881, G loss: 0.6427\n",
      "[1564/1778] D loss: 1.3973, G loss: 0.6177\n",
      "[1684/1778] D loss: 1.1427, G loss: 0.9418\n",
      "train error: \n",
      " D loss: 1.292554, G loss: 0.860661, D accuracy: 54.0%, cell accuracy: 21950.1%%, board accuracy: 90.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.277226, G loss: 0.899527, D accuracy: 54.8%, cell accuracy: 21938.1%%, board accuracy: 89.0% \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3866, G loss: 0.7251\n",
      "[124/1778] D loss: 1.3870, G loss: 0.7094\n",
      "[244/1778] D loss: 1.3880, G loss: 0.6767\n",
      "[364/1778] D loss: 1.3922, G loss: 0.6254\n",
      "[484/1778] D loss: 1.3916, G loss: 0.6921\n",
      "[604/1778] D loss: 1.3944, G loss: 0.7461\n",
      "[724/1778] D loss: 1.3933, G loss: 0.6675\n",
      "[844/1778] D loss: 1.0652, G loss: 1.1260\n",
      "[964/1778] D loss: 1.0415, G loss: 1.9255\n",
      "[1084/1778] D loss: 1.3913, G loss: 0.7489\n",
      "[1204/1778] D loss: 1.0660, G loss: 1.1677\n",
      "[1324/1778] D loss: 1.4306, G loss: 0.5390\n",
      "[1444/1778] D loss: 1.3929, G loss: 0.7598\n",
      "[1564/1778] D loss: 1.3894, G loss: 0.7182\n",
      "[1684/1778] D loss: 1.3915, G loss: 0.7359\n",
      "train error: \n",
      " D loss: 1.296203, G loss: 0.829843, D accuracy: 54.3%, cell accuracy: 21944.4%%, board accuracy: 89.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.281859, G loss: 0.847275, D accuracy: 55.2%, cell accuracy: 21931.3%%, board accuracy: 88.1% \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3894, G loss: 0.7535\n",
      "[124/1778] D loss: 1.3884, G loss: 0.6761\n",
      "[244/1778] D loss: 1.0570, G loss: 1.3136\n",
      "[364/1778] D loss: 1.3887, G loss: 0.7209\n",
      "[484/1778] D loss: 1.1100, G loss: 1.1843\n",
      "[604/1778] D loss: 1.3884, G loss: 0.7685\n",
      "[724/1778] D loss: 1.3842, G loss: 0.6177\n",
      "[844/1778] D loss: 0.7260, G loss: 1.7470\n",
      "[964/1778] D loss: 1.0455, G loss: 1.1764\n",
      "[1084/1778] D loss: 1.0759, G loss: 1.4961\n",
      "[1204/1778] D loss: 1.3918, G loss: 0.7478\n",
      "[1324/1778] D loss: 1.4015, G loss: 0.8000\n",
      "[1444/1778] D loss: 1.3908, G loss: 0.6274\n",
      "[1564/1778] D loss: 1.3914, G loss: 0.7226\n",
      "[1684/1778] D loss: 1.0526, G loss: 1.4989\n",
      "train error: \n",
      " D loss: 1.292208, G loss: 0.803822, D accuracy: 54.6%, cell accuracy: 21957.7%%, board accuracy: 89.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.274720, G loss: 0.849986, D accuracy: 55.3%, cell accuracy: 21947.1%%, board accuracy: 88.3% \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3922, G loss: 0.6256\n",
      "[124/1778] D loss: 1.3913, G loss: 0.7284\n",
      "[244/1778] D loss: 1.1672, G loss: 0.9853\n",
      "[364/1778] D loss: 1.3911, G loss: 0.6406\n",
      "[484/1778] D loss: 1.0566, G loss: 1.1708\n",
      "[604/1778] D loss: 0.7945, G loss: 1.5159\n",
      "[724/1778] D loss: 1.0780, G loss: 1.1674\n",
      "[844/1778] D loss: 1.4040, G loss: 0.5817\n",
      "[964/1778] D loss: 1.3936, G loss: 0.7483\n",
      "[1084/1778] D loss: 1.1035, G loss: 0.9192\n",
      "[1204/1778] D loss: 1.1538, G loss: 0.9791\n",
      "[1324/1778] D loss: 1.3955, G loss: 0.7619\n",
      "[1444/1778] D loss: 1.3912, G loss: 0.7738\n",
      "[1564/1778] D loss: 1.3917, G loss: 0.7573\n",
      "[1684/1778] D loss: 1.0707, G loss: 1.2542\n",
      "train error: \n",
      " D loss: 1.313181, G loss: 0.740496, D accuracy: 54.0%, cell accuracy: 21953.4%%, board accuracy: 89.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.311795, G loss: 0.772515, D accuracy: 54.2%, cell accuracy: 21945.3%%, board accuracy: 87.4% \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4129, G loss: 0.5575\n",
      "[124/1778] D loss: 1.3873, G loss: 0.7046\n",
      "[244/1778] D loss: 1.3841, G loss: 0.6671\n",
      "[364/1778] D loss: 1.3927, G loss: 0.7424\n",
      "[484/1778] D loss: 1.1641, G loss: 0.7073\n",
      "[604/1778] D loss: 1.3881, G loss: 0.6933\n",
      "[724/1778] D loss: 1.0670, G loss: 1.2242\n",
      "[844/1778] D loss: 1.3898, G loss: 0.7178\n",
      "[964/1778] D loss: 1.0579, G loss: 1.5044\n",
      "[1084/1778] D loss: 1.2593, G loss: 0.8657\n",
      "[1204/1778] D loss: 1.3970, G loss: 0.6999\n",
      "[1324/1778] D loss: 1.2775, G loss: 1.4938\n",
      "[1444/1778] D loss: 1.3891, G loss: 0.6706\n",
      "[1564/1778] D loss: 1.0544, G loss: 1.4230\n",
      "[1684/1778] D loss: 1.3908, G loss: 0.7063\n",
      "train error: \n",
      " D loss: 1.305035, G loss: 0.883021, D accuracy: 53.6%, cell accuracy: 21952.2%%, board accuracy: 89.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.295967, G loss: 0.919474, D accuracy: 54.6%, cell accuracy: 21944.8%%, board accuracy: 88.1% \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0967, G loss: 1.0143\n",
      "[124/1778] D loss: 1.3884, G loss: 0.6885\n",
      "[244/1778] D loss: 1.3902, G loss: 0.7340\n",
      "[364/1778] D loss: 1.3776, G loss: 0.6843\n",
      "[484/1778] D loss: 1.0447, G loss: 1.3136\n",
      "[604/1778] D loss: 1.3348, G loss: 0.8110\n",
      "[724/1778] D loss: 1.0539, G loss: 1.4207\n",
      "[844/1778] D loss: 1.3893, G loss: 0.6571\n",
      "[964/1778] D loss: 1.3894, G loss: 0.7262\n",
      "[1084/1778] D loss: 1.3845, G loss: 0.7112\n",
      "[1204/1778] D loss: 1.3869, G loss: 0.7119\n",
      "[1324/1778] D loss: 1.3934, G loss: 0.7405\n",
      "[1444/1778] D loss: 1.4062, G loss: 0.8350\n",
      "[1564/1778] D loss: 1.3899, G loss: 0.7558\n",
      "[1684/1778] D loss: 1.4009, G loss: 0.6494\n",
      "train error: \n",
      " D loss: 1.311093, G loss: 0.780336, D accuracy: 53.8%, cell accuracy: 21960.0%%, board accuracy: 89.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.317874, G loss: 0.793269, D accuracy: 54.2%, cell accuracy: 21952.9%%, board accuracy: 88.7% \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3974, G loss: 0.6327\n",
      "[124/1778] D loss: 1.3927, G loss: 0.7236\n",
      "[244/1778] D loss: 1.4086, G loss: 0.5852\n",
      "[364/1778] D loss: 1.1518, G loss: 0.9488\n",
      "[484/1778] D loss: 1.4101, G loss: 0.5594\n",
      "[604/1778] D loss: 1.3486, G loss: 0.7586\n",
      "[724/1778] D loss: 1.3734, G loss: 0.7051\n",
      "[844/1778] D loss: 1.3953, G loss: 0.6274\n",
      "[964/1778] D loss: 1.3893, G loss: 0.6670\n",
      "[1084/1778] D loss: 1.1291, G loss: 1.2440\n",
      "[1204/1778] D loss: 1.3878, G loss: 0.7049\n",
      "[1324/1778] D loss: 1.3950, G loss: 0.6957\n",
      "[1444/1778] D loss: 1.3872, G loss: 0.6838\n",
      "[1564/1778] D loss: 1.3630, G loss: 0.8447\n",
      "[1684/1778] D loss: 1.3989, G loss: 0.6270\n",
      "train error: \n",
      " D loss: 1.306695, G loss: 0.809510, D accuracy: 54.0%, cell accuracy: 21953.4%%, board accuracy: 89.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.302692, G loss: 0.834487, D accuracy: 54.5%, cell accuracy: 21944.4%%, board accuracy: 86.7% \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0687, G loss: 1.2410\n",
      "[124/1778] D loss: 1.3884, G loss: 0.6340\n",
      "[244/1778] D loss: 1.3970, G loss: 0.6093\n",
      "[364/1778] D loss: 1.3912, G loss: 0.7576\n",
      "[484/1778] D loss: 1.3870, G loss: 0.7081\n",
      "[604/1778] D loss: 1.3904, G loss: 0.7535\n",
      "[724/1778] D loss: 1.3971, G loss: 0.7917\n",
      "[844/1778] D loss: 1.3918, G loss: 0.6255\n",
      "[964/1778] D loss: 1.2247, G loss: 0.7038\n",
      "[1084/1778] D loss: 1.4013, G loss: 0.7382\n",
      "[1204/1778] D loss: 1.3936, G loss: 0.6239\n",
      "[1324/1778] D loss: 1.3978, G loss: 0.7272\n",
      "[1444/1778] D loss: 1.4000, G loss: 0.5782\n",
      "[1564/1778] D loss: 1.0897, G loss: 1.2747\n",
      "[1684/1778] D loss: 1.3027, G loss: 0.8375\n",
      "train error: \n",
      " D loss: 1.315674, G loss: 0.839204, D accuracy: 54.9%, cell accuracy: 21946.2%%, board accuracy: 83.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.301101, G loss: 0.879935, D accuracy: 55.4%, cell accuracy: 21935.6%%, board accuracy: 82.4% \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3843, G loss: 0.6243\n",
      "[124/1778] D loss: 1.1355, G loss: 1.5031\n",
      "[244/1778] D loss: 1.3999, G loss: 0.6310\n",
      "[364/1778] D loss: 1.0675, G loss: 1.2523\n",
      "[484/1778] D loss: 1.1169, G loss: 0.9202\n",
      "[604/1778] D loss: 1.1543, G loss: 0.9556\n",
      "[724/1778] D loss: 1.3926, G loss: 0.7085\n",
      "[844/1778] D loss: 1.0781, G loss: 1.1428\n",
      "[964/1778] D loss: 1.1943, G loss: 0.7297\n",
      "[1084/1778] D loss: 1.0844, G loss: 1.1162\n",
      "[1204/1778] D loss: 1.3888, G loss: 0.6250\n",
      "[1324/1778] D loss: 1.3975, G loss: 0.8048\n",
      "[1444/1778] D loss: 1.3934, G loss: 0.6068\n",
      "[1564/1778] D loss: 1.3993, G loss: 0.7724\n",
      "[1684/1778] D loss: 1.3127, G loss: 0.8565\n",
      "train error: \n",
      " D loss: 1.295706, G loss: 0.867171, D accuracy: 54.5%, cell accuracy: 21958.3%%, board accuracy: 88.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.290901, G loss: 0.876464, D accuracy: 54.8%, cell accuracy: 21952.0%%, board accuracy: 87.8% \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3890, G loss: 0.6603\n",
      "[124/1778] D loss: 1.3958, G loss: 0.6213\n",
      "[244/1778] D loss: 1.3869, G loss: 0.6866\n",
      "[364/1778] D loss: 1.0763, G loss: 1.1794\n",
      "[484/1778] D loss: 1.3922, G loss: 0.7376\n",
      "[604/1778] D loss: 1.0895, G loss: 1.0887\n",
      "[724/1778] D loss: 1.4418, G loss: 0.9638\n",
      "[844/1778] D loss: 1.3971, G loss: 0.6015\n",
      "[964/1778] D loss: 1.4006, G loss: 0.7745\n",
      "[1084/1778] D loss: 1.4013, G loss: 0.7765\n",
      "[1204/1778] D loss: 1.0732, G loss: 1.2356\n",
      "[1324/1778] D loss: 1.0904, G loss: 1.1348\n",
      "[1444/1778] D loss: 1.3931, G loss: 0.6793\n",
      "[1564/1778] D loss: 1.0458, G loss: 1.4557\n",
      "[1684/1778] D loss: 1.3893, G loss: 0.6479\n",
      "train error: \n",
      " D loss: 1.294078, G loss: 0.923289, D accuracy: 53.8%, cell accuracy: 21959.6%%, board accuracy: 90.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.280219, G loss: 0.951734, D accuracy: 54.8%, cell accuracy: 21951.6%%, board accuracy: 89.2% \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3910, G loss: 0.7576\n",
      "[124/1778] D loss: 1.3886, G loss: 0.7254\n",
      "[244/1778] D loss: 1.3878, G loss: 0.7205\n",
      "[364/1778] D loss: 1.3935, G loss: 0.7480\n",
      "[484/1778] D loss: 1.3899, G loss: 0.6519\n",
      "[604/1778] D loss: 1.4013, G loss: 0.6840\n",
      "[724/1778] D loss: 1.4017, G loss: 0.6180\n",
      "[844/1778] D loss: 1.3872, G loss: 0.6810\n",
      "[964/1778] D loss: 1.3825, G loss: 0.6364\n",
      "[1084/1778] D loss: 1.1513, G loss: 0.9120\n",
      "[1204/1778] D loss: 1.2693, G loss: 0.6174\n",
      "[1324/1778] D loss: 1.4083, G loss: 0.5806\n",
      "[1444/1778] D loss: 1.3931, G loss: 0.6238\n",
      "[1564/1778] D loss: 1.3971, G loss: 0.6696\n",
      "[1684/1778] D loss: 1.0801, G loss: 0.9637\n",
      "train error: \n",
      " D loss: 1.302993, G loss: 0.783017, D accuracy: 54.7%, cell accuracy: 21954.5%%, board accuracy: 87.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.297218, G loss: 0.818386, D accuracy: 54.4%, cell accuracy: 21947.5%%, board accuracy: 88.1% \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3938, G loss: 0.6348\n",
      "[124/1778] D loss: 1.3906, G loss: 0.7356\n",
      "[244/1778] D loss: 1.1181, G loss: 0.9056\n",
      "[364/1778] D loss: 1.3878, G loss: 0.6967\n",
      "[484/1778] D loss: 1.2999, G loss: 1.0057\n",
      "[604/1778] D loss: 1.6643, G loss: 1.0758\n",
      "[724/1778] D loss: 1.3258, G loss: 1.3895\n",
      "[844/1778] D loss: 1.0620, G loss: 1.7207\n",
      "[964/1778] D loss: 1.0798, G loss: 1.0487\n",
      "[1084/1778] D loss: 0.7513, G loss: 1.8669\n",
      "[1204/1778] D loss: 1.3876, G loss: 0.6589\n",
      "[1324/1778] D loss: 1.4104, G loss: 0.7586\n",
      "[1444/1778] D loss: 1.0728, G loss: 1.1120\n",
      "[1564/1778] D loss: 0.7492, G loss: 1.5595\n",
      "[1684/1778] D loss: 1.3899, G loss: 0.7000\n",
      "train error: \n",
      " D loss: 1.308636, G loss: 0.875262, D accuracy: 53.6%, cell accuracy: 21956.4%%, board accuracy: 89.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.294436, G loss: 0.904041, D accuracy: 54.6%, cell accuracy: 21946.4%%, board accuracy: 87.4% \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3876, G loss: 0.6837\n",
      "[124/1778] D loss: 1.3875, G loss: 0.7117\n",
      "[244/1778] D loss: 1.3930, G loss: 0.6773\n",
      "[364/1778] D loss: 1.3892, G loss: 0.7453\n",
      "[484/1778] D loss: 1.0490, G loss: 1.3231\n",
      "[604/1778] D loss: 1.3904, G loss: 0.6423\n",
      "[724/1778] D loss: 1.3881, G loss: 0.6923\n",
      "[844/1778] D loss: 1.3898, G loss: 0.7351\n",
      "[964/1778] D loss: 1.3898, G loss: 0.6545\n",
      "[1084/1778] D loss: 1.4023, G loss: 0.6194\n",
      "[1204/1778] D loss: 1.4054, G loss: 0.5947\n",
      "[1324/1778] D loss: 1.3903, G loss: 0.7349\n",
      "[1444/1778] D loss: 1.3617, G loss: 0.7065\n",
      "[1564/1778] D loss: 1.3899, G loss: 0.7082\n",
      "[1684/1778] D loss: 1.0995, G loss: 1.2626\n",
      "train error: \n",
      " D loss: 1.295616, G loss: 0.911601, D accuracy: 54.0%, cell accuracy: 21958.5%%, board accuracy: 89.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.284927, G loss: 0.935023, D accuracy: 54.7%, cell accuracy: 21951.8%%, board accuracy: 88.1% \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3924, G loss: 0.7487\n",
      "[124/1778] D loss: 1.4028, G loss: 0.7004\n",
      "[244/1778] D loss: 1.3869, G loss: 0.6802\n",
      "[364/1778] D loss: 1.3933, G loss: 0.7416\n",
      "[484/1778] D loss: 1.3912, G loss: 0.6316\n",
      "[604/1778] D loss: 1.4437, G loss: 0.8521\n",
      "[724/1778] D loss: 1.3911, G loss: 0.6430\n",
      "[844/1778] D loss: 1.3892, G loss: 0.7024\n",
      "[964/1778] D loss: 1.3899, G loss: 0.6650\n",
      "[1084/1778] D loss: 1.3882, G loss: 0.6789\n",
      "[1204/1778] D loss: 1.3932, G loss: 0.7446\n",
      "[1324/1778] D loss: 1.0475, G loss: 1.4251\n",
      "[1444/1778] D loss: 1.3897, G loss: 0.6811\n",
      "[1564/1778] D loss: 1.3888, G loss: 0.6900\n",
      "[1684/1778] D loss: 1.0584, G loss: 1.1979\n",
      "train error: \n",
      " D loss: 1.288308, G loss: 0.894414, D accuracy: 54.3%, cell accuracy: 21957.0%%, board accuracy: 89.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.273404, G loss: 0.929628, D accuracy: 54.7%, cell accuracy: 21947.3%%, board accuracy: 87.6% \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3931, G loss: 0.7156\n",
      "[124/1778] D loss: 1.0828, G loss: 0.9904\n",
      "[244/1778] D loss: 1.3864, G loss: 0.6721\n",
      "[364/1778] D loss: 1.3877, G loss: 0.6971\n",
      "[484/1778] D loss: 1.3888, G loss: 0.6882\n",
      "[604/1778] D loss: 1.3892, G loss: 0.7200\n",
      "[724/1778] D loss: 1.0816, G loss: 1.1231\n",
      "[844/1778] D loss: 1.3892, G loss: 0.6395\n",
      "[964/1778] D loss: 1.3900, G loss: 0.7563\n",
      "[1084/1778] D loss: 1.3577, G loss: 0.7142\n",
      "[1204/1778] D loss: 1.0624, G loss: 1.2627\n",
      "[1324/1778] D loss: 1.1074, G loss: 0.9781\n",
      "[1444/1778] D loss: 1.3857, G loss: 0.6837\n",
      "[1564/1778] D loss: 1.0807, G loss: 1.0091\n",
      "[1684/1778] D loss: 1.3984, G loss: 0.6279\n",
      "train error: \n",
      " D loss: 1.284470, G loss: 0.874968, D accuracy: 54.7%, cell accuracy: 21950.3%%, board accuracy: 86.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.261425, G loss: 0.928222, D accuracy: 56.6%, cell accuracy: 21937.2%%, board accuracy: 81.5% \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0493, G loss: 1.2631\n",
      "[124/1778] D loss: 1.3918, G loss: 0.7350\n",
      "[244/1778] D loss: 1.3785, G loss: 0.7573\n",
      "[364/1778] D loss: 1.3322, G loss: 0.7590\n",
      "[484/1778] D loss: 1.3796, G loss: 0.6408\n",
      "[604/1778] D loss: 1.0598, G loss: 1.3429\n",
      "[724/1778] D loss: 1.3764, G loss: 0.7159\n",
      "[844/1778] D loss: 1.0488, G loss: 1.4032\n",
      "[964/1778] D loss: 1.0546, G loss: 1.2460\n",
      "[1084/1778] D loss: 1.3889, G loss: 0.6976\n",
      "[1204/1778] D loss: 1.3876, G loss: 0.7077\n",
      "[1324/1778] D loss: 1.3584, G loss: 0.7141\n",
      "[1444/1778] D loss: 1.0559, G loss: 1.2217\n",
      "[1564/1778] D loss: 1.3877, G loss: 0.6366\n",
      "[1684/1778] D loss: 1.3881, G loss: 0.7273\n",
      "train error: \n",
      " D loss: 1.283998, G loss: 0.846223, D accuracy: 54.8%, cell accuracy: 21955.6%%, board accuracy: 88.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.266986, G loss: 0.892594, D accuracy: 55.6%, cell accuracy: 21945.7%%, board accuracy: 86.7% \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4009, G loss: 0.7315\n",
      "[124/1778] D loss: 1.3871, G loss: 0.7034\n",
      "[244/1778] D loss: 1.3868, G loss: 0.6949\n",
      "[364/1778] D loss: 1.0528, G loss: 1.3169\n",
      "[484/1778] D loss: 1.3861, G loss: 0.7059\n",
      "[604/1778] D loss: 1.3909, G loss: 0.7449\n",
      "[724/1778] D loss: 1.3952, G loss: 0.7544\n",
      "[844/1778] D loss: 1.3871, G loss: 0.7018\n",
      "[964/1778] D loss: 1.3626, G loss: 0.7274\n",
      "[1084/1778] D loss: 1.4142, G loss: 0.6793\n",
      "[1204/1778] D loss: 1.3868, G loss: 0.6801\n",
      "[1324/1778] D loss: 1.3889, G loss: 0.7152\n",
      "[1444/1778] D loss: 1.3878, G loss: 0.6861\n",
      "[1564/1778] D loss: 1.3881, G loss: 0.7245\n",
      "[1684/1778] D loss: 1.0526, G loss: 1.2763\n",
      "train error: \n",
      " D loss: 1.285911, G loss: 0.911539, D accuracy: 54.2%, cell accuracy: 21960.0%%, board accuracy: 90.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.265438, G loss: 0.971738, D accuracy: 55.5%, cell accuracy: 21950.5%%, board accuracy: 88.3% \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0542, G loss: 1.3617\n",
      "[124/1778] D loss: 1.0608, G loss: 1.5241\n",
      "[244/1778] D loss: 1.3899, G loss: 0.7634\n",
      "[364/1778] D loss: 1.3894, G loss: 0.7563\n",
      "[484/1778] D loss: 1.3900, G loss: 0.6514\n",
      "[604/1778] D loss: 1.0606, G loss: 1.2364\n",
      "[724/1778] D loss: 1.3920, G loss: 0.7005\n",
      "[844/1778] D loss: 1.3883, G loss: 0.7089\n",
      "[964/1778] D loss: 1.3892, G loss: 0.7069\n",
      "[1084/1778] D loss: 1.4018, G loss: 0.7647\n",
      "[1204/1778] D loss: 1.0537, G loss: 1.2657\n",
      "[1324/1778] D loss: 1.3868, G loss: 0.6859\n",
      "[1444/1778] D loss: 1.3956, G loss: 0.6784\n",
      "[1564/1778] D loss: 1.3879, G loss: 0.6735\n",
      "[1684/1778] D loss: 1.0474, G loss: 1.4498\n",
      "train error: \n",
      " D loss: 1.285176, G loss: 0.871818, D accuracy: 54.3%, cell accuracy: 21958.4%%, board accuracy: 90.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.269362, G loss: 0.929757, D accuracy: 55.1%, cell accuracy: 21949.1%%, board accuracy: 87.8% \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3906, G loss: 0.6670\n",
      "[124/1778] D loss: 1.3874, G loss: 0.7022\n",
      "[244/1778] D loss: 1.0480, G loss: 1.5679\n",
      "[364/1778] D loss: 1.0510, G loss: 1.4702\n",
      "[484/1778] D loss: 1.3878, G loss: 0.7187\n",
      "[604/1778] D loss: 1.0739, G loss: 1.1140\n",
      "[724/1778] D loss: 1.1023, G loss: 1.1780\n",
      "[844/1778] D loss: 1.3881, G loss: 0.6629\n",
      "[964/1778] D loss: 1.3887, G loss: 0.6826\n",
      "[1084/1778] D loss: 1.4053, G loss: 0.5879\n",
      "[1204/1778] D loss: 0.7316, G loss: 1.7119\n",
      "[1324/1778] D loss: 1.3924, G loss: 0.7558\n",
      "[1444/1778] D loss: 1.3797, G loss: 0.7115\n",
      "[1564/1778] D loss: 1.3878, G loss: 0.7080\n",
      "[1684/1778] D loss: 1.3884, G loss: 0.7160\n",
      "train error: \n",
      " D loss: 1.294380, G loss: 1.030566, D accuracy: 53.9%, cell accuracy: 21953.6%%, board accuracy: 89.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.282751, G loss: 1.094143, D accuracy: 54.6%, cell accuracy: 21939.6%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4278, G loss: 0.8685\n",
      "[124/1778] D loss: 1.3941, G loss: 0.7703\n",
      "[244/1778] D loss: 1.0483, G loss: 1.2046\n",
      "[364/1778] D loss: 1.4124, G loss: 0.8566\n",
      "[484/1778] D loss: 1.3909, G loss: 0.7958\n",
      "[604/1778] D loss: 1.3870, G loss: 0.6843\n",
      "[724/1778] D loss: 1.3937, G loss: 0.6138\n",
      "[844/1778] D loss: 1.3421, G loss: 0.7091\n",
      "[964/1778] D loss: 1.3953, G loss: 0.7659\n",
      "[1084/1778] D loss: 1.3914, G loss: 0.7549\n",
      "[1204/1778] D loss: 1.0614, G loss: 1.3178\n",
      "[1324/1778] D loss: 1.0520, G loss: 1.3276\n",
      "[1444/1778] D loss: 1.3892, G loss: 0.6901\n",
      "[1564/1778] D loss: 1.3959, G loss: 0.7423\n",
      "[1684/1778] D loss: 1.3923, G loss: 0.7658\n",
      "train error: \n",
      " D loss: 1.294119, G loss: 0.987514, D accuracy: 54.1%, cell accuracy: 21921.8%%, board accuracy: 80.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.283715, G loss: 1.039097, D accuracy: 55.5%, cell accuracy: 21905.9%%, board accuracy: 77.7% \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4024, G loss: 0.7800\n",
      "[124/1778] D loss: 1.4352, G loss: 0.7890\n",
      "[244/1778] D loss: 1.4092, G loss: 0.5785\n",
      "[364/1778] D loss: 1.4422, G loss: 0.8808\n",
      "[484/1778] D loss: 1.2813, G loss: 0.9295\n",
      "[604/1778] D loss: 1.0579, G loss: 1.3469\n",
      "[724/1778] D loss: 1.3870, G loss: 0.6464\n",
      "[844/1778] D loss: 1.3839, G loss: 0.6691\n",
      "[964/1778] D loss: 1.3890, G loss: 0.6837\n",
      "[1084/1778] D loss: 1.0726, G loss: 1.1657\n",
      "[1204/1778] D loss: 1.3911, G loss: 0.6830\n",
      "[1324/1778] D loss: 1.3876, G loss: 0.6819\n",
      "[1444/1778] D loss: 1.4022, G loss: 0.7957\n",
      "[1564/1778] D loss: 1.0766, G loss: 1.3140\n",
      "[1684/1778] D loss: 0.7368, G loss: 1.6938\n",
      "train error: \n",
      " D loss: 1.284457, G loss: 0.890951, D accuracy: 54.4%, cell accuracy: 21956.3%%, board accuracy: 89.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.274121, G loss: 0.938471, D accuracy: 55.3%, cell accuracy: 21946.6%%, board accuracy: 87.2% \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3864, G loss: 0.7051\n",
      "[124/1778] D loss: 1.3850, G loss: 0.6853\n",
      "[244/1778] D loss: 1.3903, G loss: 0.6714\n",
      "[364/1778] D loss: 1.0517, G loss: 1.4293\n",
      "[484/1778] D loss: 1.3896, G loss: 0.7366\n",
      "[604/1778] D loss: 1.3798, G loss: 0.7002\n",
      "[724/1778] D loss: 1.0575, G loss: 1.2004\n",
      "[844/1778] D loss: 1.3892, G loss: 0.6934\n",
      "[964/1778] D loss: 1.1652, G loss: 0.8084\n",
      "[1084/1778] D loss: 1.0586, G loss: 1.2246\n",
      "[1204/1778] D loss: 1.3903, G loss: 0.6350\n",
      "[1324/1778] D loss: 1.0538, G loss: 1.2923\n",
      "[1444/1778] D loss: 1.3905, G loss: 0.6516\n",
      "[1564/1778] D loss: 1.3876, G loss: 0.7061\n",
      "[1684/1778] D loss: 1.4085, G loss: 0.5965\n",
      "train error: \n",
      " D loss: 1.289436, G loss: 0.865371, D accuracy: 54.0%, cell accuracy: 21963.8%%, board accuracy: 91.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.262294, G loss: 0.926780, D accuracy: 55.7%, cell accuracy: 21952.0%%, board accuracy: 87.2% \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3894, G loss: 0.6871\n",
      "[124/1778] D loss: 1.3868, G loss: 0.7073\n",
      "[244/1778] D loss: 1.3883, G loss: 0.6815\n",
      "[364/1778] D loss: 1.3874, G loss: 0.6729\n",
      "[484/1778] D loss: 1.0510, G loss: 1.3333\n",
      "[604/1778] D loss: 1.3912, G loss: 0.7560\n",
      "[724/1778] D loss: 1.3882, G loss: 0.7160\n",
      "[844/1778] D loss: 1.3872, G loss: 0.7222\n",
      "[964/1778] D loss: 1.3938, G loss: 0.6571\n",
      "[1084/1778] D loss: 1.3864, G loss: 0.6771\n",
      "[1204/1778] D loss: 1.3870, G loss: 0.6902\n",
      "[1324/1778] D loss: 1.3882, G loss: 0.7161\n",
      "[1444/1778] D loss: 1.3875, G loss: 0.6419\n",
      "[1564/1778] D loss: 1.3869, G loss: 0.6913\n",
      "[1684/1778] D loss: 1.3895, G loss: 0.7145\n",
      "train error: \n",
      " D loss: 1.285261, G loss: 0.956121, D accuracy: 54.4%, cell accuracy: 21952.7%%, board accuracy: 88.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.267843, G loss: 1.030239, D accuracy: 55.7%, cell accuracy: 21937.6%%, board accuracy: 83.6% \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3916, G loss: 0.7407\n",
      "[124/1778] D loss: 0.7120, G loss: 1.9881\n",
      "[244/1778] D loss: 1.4200, G loss: 0.6996\n",
      "[364/1778] D loss: 1.3868, G loss: 0.6720\n",
      "[484/1778] D loss: 1.3880, G loss: 0.6922\n",
      "[604/1778] D loss: 1.0429, G loss: 1.4099\n",
      "[724/1778] D loss: 1.1601, G loss: 0.9168\n",
      "[844/1778] D loss: 1.4003, G loss: 0.5896\n",
      "[964/1778] D loss: 1.3958, G loss: 0.7616\n",
      "[1084/1778] D loss: 1.3900, G loss: 0.6745\n",
      "[1204/1778] D loss: 1.3870, G loss: 0.7135\n",
      "[1324/1778] D loss: 1.3942, G loss: 0.7456\n",
      "[1444/1778] D loss: 1.4292, G loss: 0.7880\n",
      "[1564/1778] D loss: 1.3882, G loss: 0.6568\n",
      "[1684/1778] D loss: 1.3900, G loss: 0.7497\n",
      "train error: \n",
      " D loss: 1.285001, G loss: 0.912526, D accuracy: 54.2%, cell accuracy: 21961.9%%, board accuracy: 91.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.264112, G loss: 0.973755, D accuracy: 55.3%, cell accuracy: 21949.5%%, board accuracy: 88.7% \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3878, G loss: 0.7241\n",
      "[124/1778] D loss: 1.0482, G loss: 1.5952\n",
      "[244/1778] D loss: 1.0566, G loss: 1.2587\n",
      "[364/1778] D loss: 1.3901, G loss: 0.6860\n",
      "[484/1778] D loss: 1.4010, G loss: 0.7346\n",
      "[604/1778] D loss: 1.4130, G loss: 0.8352\n",
      "[724/1778] D loss: 1.4016, G loss: 0.8041\n",
      "[844/1778] D loss: 1.4326, G loss: 0.7224\n",
      "[964/1778] D loss: 0.7068, G loss: 2.1660\n",
      "[1084/1778] D loss: 1.3875, G loss: 0.6862\n",
      "[1204/1778] D loss: 1.3999, G loss: 0.5812\n",
      "[1324/1778] D loss: 1.0549, G loss: 1.2182\n",
      "[1444/1778] D loss: 1.3877, G loss: 0.7107\n",
      "[1564/1778] D loss: 1.0501, G loss: 1.3960\n",
      "[1684/1778] D loss: 1.0482, G loss: 1.6348\n",
      "train error: \n",
      " D loss: 1.291650, G loss: 0.909103, D accuracy: 53.9%, cell accuracy: 21956.4%%, board accuracy: 90.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.265412, G loss: 0.964008, D accuracy: 55.4%, cell accuracy: 21943.7%%, board accuracy: 87.8% \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3940, G loss: 0.7277\n",
      "[124/1778] D loss: 1.0521, G loss: 1.2672\n",
      "[244/1778] D loss: 1.3913, G loss: 0.6395\n",
      "[364/1778] D loss: 1.3879, G loss: 0.7214\n",
      "[484/1778] D loss: 1.3891, G loss: 0.6920\n",
      "[604/1778] D loss: 1.3889, G loss: 0.7456\n",
      "[724/1778] D loss: 1.3930, G loss: 0.6413\n",
      "[844/1778] D loss: 1.0975, G loss: 0.9450\n",
      "[964/1778] D loss: 1.3940, G loss: 0.7317\n",
      "[1084/1778] D loss: 1.0572, G loss: 1.2616\n",
      "[1204/1778] D loss: 1.3885, G loss: 0.6915\n",
      "[1324/1778] D loss: 1.3651, G loss: 0.6700\n",
      "[1444/1778] D loss: 1.0471, G loss: 1.4483\n",
      "[1564/1778] D loss: 1.3875, G loss: 0.7151\n",
      "[1684/1778] D loss: 1.3891, G loss: 0.6433\n",
      "train error: \n",
      " D loss: 1.291999, G loss: 0.918661, D accuracy: 54.1%, cell accuracy: 21957.5%%, board accuracy: 90.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.269680, G loss: 0.990763, D accuracy: 55.3%, cell accuracy: 21944.6%%, board accuracy: 87.6% \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0514, G loss: 1.3797\n",
      "[124/1778] D loss: 1.0533, G loss: 1.2384\n",
      "[244/1778] D loss: 1.0504, G loss: 1.4336\n",
      "[364/1778] D loss: 1.3939, G loss: 0.7622\n",
      "[484/1778] D loss: 1.3871, G loss: 0.7235\n",
      "[604/1778] D loss: 1.3919, G loss: 0.6406\n",
      "[724/1778] D loss: 1.0540, G loss: 1.4676\n",
      "[844/1778] D loss: 1.3956, G loss: 0.6281\n",
      "[964/1778] D loss: 1.3900, G loss: 0.6697\n",
      "[1084/1778] D loss: 1.3617, G loss: 0.7822\n",
      "[1204/1778] D loss: 1.3882, G loss: 0.7021\n",
      "[1324/1778] D loss: 1.3869, G loss: 0.6719\n",
      "[1444/1778] D loss: 1.3882, G loss: 0.7011\n",
      "[1564/1778] D loss: 0.7158, G loss: 2.1867\n",
      "[1684/1778] D loss: 1.3978, G loss: 0.7996\n",
      "train error: \n",
      " D loss: 1.285434, G loss: 0.892910, D accuracy: 54.1%, cell accuracy: 21962.8%%, board accuracy: 91.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.267267, G loss: 0.966345, D accuracy: 54.7%, cell accuracy: 21953.6%%, board accuracy: 89.4% \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3878, G loss: 0.7129\n",
      "[124/1778] D loss: 1.0508, G loss: 1.3044\n",
      "[244/1778] D loss: 1.0490, G loss: 1.3246\n",
      "[364/1778] D loss: 1.0646, G loss: 1.1943\n",
      "[484/1778] D loss: 1.3883, G loss: 0.7375\n",
      "[604/1778] D loss: 1.4051, G loss: 0.7532\n",
      "[724/1778] D loss: 1.3872, G loss: 0.6838\n",
      "[844/1778] D loss: 1.4072, G loss: 0.6846\n",
      "[964/1778] D loss: 1.3886, G loss: 0.6574\n",
      "[1084/1778] D loss: 1.3981, G loss: 0.5829\n",
      "[1204/1778] D loss: 1.3906, G loss: 0.6420\n",
      "[1324/1778] D loss: 1.3901, G loss: 0.6557\n",
      "[1444/1778] D loss: 0.7313, G loss: 1.8860\n",
      "[1564/1778] D loss: 1.3824, G loss: 0.6809\n",
      "[1684/1778] D loss: 1.0497, G loss: 1.3444\n",
      "train error: \n",
      " D loss: 1.286034, G loss: 0.897066, D accuracy: 54.0%, cell accuracy: 21964.1%%, board accuracy: 91.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.272265, G loss: 0.952191, D accuracy: 55.1%, cell accuracy: 21952.7%%, board accuracy: 88.5% \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3887, G loss: 0.6922\n",
      "[124/1778] D loss: 1.3823, G loss: 0.6807\n",
      "[244/1778] D loss: 1.4022, G loss: 0.5873\n",
      "[364/1778] D loss: 1.3977, G loss: 0.6289\n",
      "[484/1778] D loss: 1.3878, G loss: 0.6602\n",
      "[604/1778] D loss: 1.3871, G loss: 0.6595\n",
      "[724/1778] D loss: 1.3927, G loss: 0.6290\n",
      "[844/1778] D loss: 1.3880, G loss: 0.6845\n",
      "[964/1778] D loss: 1.0559, G loss: 1.2072\n",
      "[1084/1778] D loss: 1.0443, G loss: 1.5580\n",
      "[1204/1778] D loss: 1.3881, G loss: 0.6865\n",
      "[1324/1778] D loss: 1.3887, G loss: 0.7033\n",
      "[1444/1778] D loss: 1.4011, G loss: 0.5969\n",
      "[1564/1778] D loss: 1.0540, G loss: 1.3008\n",
      "[1684/1778] D loss: 1.3990, G loss: 0.7668\n",
      "train error: \n",
      " D loss: 1.286727, G loss: 0.839850, D accuracy: 54.7%, cell accuracy: 21961.6%%, board accuracy: 89.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.267302, G loss: 0.889034, D accuracy: 55.4%, cell accuracy: 21952.5%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3938, G loss: 0.6433\n",
      "[124/1778] D loss: 1.3925, G loss: 0.6697\n",
      "[244/1778] D loss: 1.3872, G loss: 0.6908\n",
      "[364/1778] D loss: 1.0670, G loss: 1.2269\n",
      "[484/1778] D loss: 1.3883, G loss: 0.6638\n",
      "[604/1778] D loss: 1.3874, G loss: 0.7154\n",
      "[724/1778] D loss: 1.3866, G loss: 0.6996\n",
      "[844/1778] D loss: 1.3877, G loss: 0.6941\n",
      "[964/1778] D loss: 1.0700, G loss: 1.2656\n",
      "[1084/1778] D loss: 1.0539, G loss: 1.3867\n",
      "[1204/1778] D loss: 1.3931, G loss: 0.6754\n",
      "[1324/1778] D loss: 1.3875, G loss: 0.7371\n",
      "[1444/1778] D loss: 1.3883, G loss: 0.6865\n",
      "[1564/1778] D loss: 1.0578, G loss: 1.2311\n",
      "[1684/1778] D loss: 1.3987, G loss: 0.5889\n",
      "train error: \n",
      " D loss: 1.280807, G loss: 0.922535, D accuracy: 55.0%, cell accuracy: 21940.8%%, board accuracy: 88.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.264436, G loss: 0.977542, D accuracy: 55.5%, cell accuracy: 21925.5%%, board accuracy: 85.4% \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3721, G loss: 0.7495\n",
      "[124/1778] D loss: 1.3259, G loss: 0.9128\n",
      "[244/1778] D loss: 1.3921, G loss: 0.6674\n",
      "[364/1778] D loss: 1.3588, G loss: 0.6675\n",
      "[484/1778] D loss: 1.3977, G loss: 0.6476\n",
      "[604/1778] D loss: 1.3886, G loss: 0.6618\n",
      "[724/1778] D loss: 1.3906, G loss: 0.7362\n",
      "[844/1778] D loss: 1.0640, G loss: 1.2849\n",
      "[964/1778] D loss: 1.3763, G loss: 0.6611\n",
      "[1084/1778] D loss: 1.3925, G loss: 0.7493\n",
      "[1204/1778] D loss: 1.3935, G loss: 0.7402\n",
      "[1324/1778] D loss: 1.3965, G loss: 0.6353\n",
      "[1444/1778] D loss: 1.4379, G loss: 0.8469\n",
      "[1564/1778] D loss: 1.3838, G loss: 0.7384\n",
      "[1684/1778] D loss: 1.3889, G loss: 0.7008\n",
      "train error: \n",
      " D loss: 1.285202, G loss: 0.944965, D accuracy: 54.1%, cell accuracy: 21963.6%%, board accuracy: 91.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.267312, G loss: 1.004890, D accuracy: 55.2%, cell accuracy: 21953.6%%, board accuracy: 88.7% \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3892, G loss: 0.7572\n",
      "[124/1778] D loss: 1.3880, G loss: 0.7813\n",
      "[244/1778] D loss: 1.4041, G loss: 0.7630\n",
      "[364/1778] D loss: 1.3923, G loss: 0.6528\n",
      "[484/1778] D loss: 1.0514, G loss: 1.4268\n",
      "[604/1778] D loss: 1.3872, G loss: 0.6859\n",
      "[724/1778] D loss: 1.3857, G loss: 0.7425\n",
      "[844/1778] D loss: 1.3887, G loss: 0.6741\n",
      "[964/1778] D loss: 1.0526, G loss: 1.3237\n",
      "[1084/1778] D loss: 1.5579, G loss: 0.6351\n",
      "[1204/1778] D loss: 1.3922, G loss: 0.7291\n",
      "[1324/1778] D loss: 1.3959, G loss: 0.6357\n",
      "[1444/1778] D loss: 1.3881, G loss: 0.6675\n",
      "[1564/1778] D loss: 1.0699, G loss: 1.1730\n",
      "[1684/1778] D loss: 1.3876, G loss: 0.7165\n",
      "train error: \n",
      " D loss: 1.291168, G loss: 0.901536, D accuracy: 54.2%, cell accuracy: 21962.4%%, board accuracy: 90.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.266976, G loss: 0.945762, D accuracy: 55.4%, cell accuracy: 21955.0%%, board accuracy: 87.8% \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3901, G loss: 0.7266\n",
      "[124/1778] D loss: 1.3869, G loss: 0.7086\n",
      "[244/1778] D loss: 1.3931, G loss: 0.6666\n",
      "[364/1778] D loss: 1.3899, G loss: 0.6547\n",
      "[484/1778] D loss: 1.3951, G loss: 0.7670\n",
      "[604/1778] D loss: 1.3876, G loss: 0.7050\n",
      "[724/1778] D loss: 1.2176, G loss: 0.9941\n",
      "[844/1778] D loss: 1.3417, G loss: 0.7275\n",
      "[964/1778] D loss: 1.0477, G loss: 1.4321\n",
      "[1084/1778] D loss: 0.7046, G loss: 2.5189\n",
      "[1204/1778] D loss: 1.3887, G loss: 0.7103\n",
      "[1324/1778] D loss: 1.0561, G loss: 1.3041\n",
      "[1444/1778] D loss: 1.0462, G loss: 1.7131\n",
      "[1564/1778] D loss: 1.3896, G loss: 0.7304\n",
      "[1684/1778] D loss: 1.0457, G loss: 1.3921\n",
      "train error: \n",
      " D loss: 1.286836, G loss: 0.934798, D accuracy: 54.9%, cell accuracy: 21930.3%%, board accuracy: 84.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.268165, G loss: 1.021811, D accuracy: 56.4%, cell accuracy: 21908.3%%, board accuracy: 79.7% \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "[4/1778] D loss: 0.7047, G loss: 2.3930\n",
      "[124/1778] D loss: 1.3490, G loss: 0.8723\n",
      "[244/1778] D loss: 1.3897, G loss: 0.6337\n",
      "[364/1778] D loss: 1.3683, G loss: 0.7548\n",
      "[484/1778] D loss: 1.4361, G loss: 1.0291\n",
      "[604/1778] D loss: 1.3904, G loss: 0.7330\n",
      "[724/1778] D loss: 1.0531, G loss: 1.3759\n",
      "[844/1778] D loss: 1.3574, G loss: 0.6567\n",
      "[964/1778] D loss: 1.3920, G loss: 0.7575\n",
      "[1084/1778] D loss: 1.3910, G loss: 0.7356\n",
      "[1204/1778] D loss: 1.3736, G loss: 0.7759\n",
      "[1324/1778] D loss: 1.3904, G loss: 0.7530\n",
      "[1444/1778] D loss: 1.3864, G loss: 0.7039\n",
      "[1564/1778] D loss: 1.3889, G loss: 0.6248\n",
      "[1684/1778] D loss: 1.0337, G loss: 1.4476\n",
      "train error: \n",
      " D loss: 1.284823, G loss: 0.973069, D accuracy: 54.5%, cell accuracy: 21941.2%%, board accuracy: 86.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.267131, G loss: 1.043586, D accuracy: 55.6%, cell accuracy: 21920.5%%, board accuracy: 83.1% \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3608, G loss: 0.7290\n",
      "[124/1778] D loss: 1.4788, G loss: 1.0273\n",
      "[244/1778] D loss: 1.0624, G loss: 1.2270\n",
      "[364/1778] D loss: 1.3888, G loss: 0.7192\n",
      "[484/1778] D loss: 1.3090, G loss: 0.7470\n",
      "[604/1778] D loss: 1.3900, G loss: 0.7121\n",
      "[724/1778] D loss: 1.3940, G loss: 0.6218\n",
      "[844/1778] D loss: 1.5786, G loss: 0.5826\n",
      "[964/1778] D loss: 1.0803, G loss: 2.2161\n",
      "[1084/1778] D loss: 1.3886, G loss: 0.7252\n",
      "[1204/1778] D loss: 1.0560, G loss: 1.2771\n",
      "[1324/1778] D loss: 1.3866, G loss: 0.6827\n",
      "[1444/1778] D loss: 1.0607, G loss: 1.2923\n",
      "[1564/1778] D loss: 1.3973, G loss: 0.6080\n",
      "[1684/1778] D loss: 1.0578, G loss: 1.2836\n",
      "train error: \n",
      " D loss: 1.291580, G loss: 0.982043, D accuracy: 54.4%, cell accuracy: 21959.7%%, board accuracy: 89.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.293083, G loss: 1.026886, D accuracy: 55.0%, cell accuracy: 21951.1%%, board accuracy: 88.5% \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0507, G loss: 1.4521\n",
      "[124/1778] D loss: 1.3423, G loss: 0.7341\n",
      "[244/1778] D loss: 1.3913, G loss: 0.6769\n",
      "[364/1778] D loss: 1.3932, G loss: 0.7458\n",
      "[484/1778] D loss: 1.3940, G loss: 0.7357\n",
      "[604/1778] D loss: 1.3965, G loss: 0.6157\n",
      "[724/1778] D loss: 1.4028, G loss: 0.6257\n",
      "[844/1778] D loss: 1.3900, G loss: 0.7036\n",
      "[964/1778] D loss: 1.0549, G loss: 1.4421\n",
      "[1084/1778] D loss: 1.0458, G loss: 1.5280\n",
      "[1204/1778] D loss: 1.3880, G loss: 0.6998\n",
      "[1324/1778] D loss: 1.3880, G loss: 0.6653\n",
      "[1444/1778] D loss: 1.3934, G loss: 0.6265\n",
      "[1564/1778] D loss: 1.3884, G loss: 0.7298\n",
      "[1684/1778] D loss: 1.0521, G loss: 1.3920\n",
      "train error: \n",
      " D loss: 1.279912, G loss: 0.931567, D accuracy: 54.4%, cell accuracy: 21959.5%%, board accuracy: 90.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.271811, G loss: 0.976076, D accuracy: 55.3%, cell accuracy: 21948.6%%, board accuracy: 87.2% \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0485, G loss: 1.3931\n",
      "[124/1778] D loss: 1.0784, G loss: 1.3960\n",
      "[244/1778] D loss: 1.3899, G loss: 0.7770\n",
      "[364/1778] D loss: 0.7278, G loss: 1.8858\n",
      "[484/1778] D loss: 1.0457, G loss: 1.6977\n",
      "[604/1778] D loss: 1.0577, G loss: 1.3015\n",
      "[724/1778] D loss: 1.3869, G loss: 0.6950\n",
      "[844/1778] D loss: 1.0451, G loss: 1.5185\n",
      "[964/1778] D loss: 1.3898, G loss: 0.7209\n",
      "[1084/1778] D loss: 1.0774, G loss: 1.1252\n",
      "[1204/1778] D loss: 1.3890, G loss: 0.6286\n",
      "[1324/1778] D loss: 1.3928, G loss: 0.6022\n",
      "[1444/1778] D loss: 1.0454, G loss: 1.4992\n",
      "[1564/1778] D loss: 1.3953, G loss: 0.6106\n",
      "[1684/1778] D loss: 1.3883, G loss: 0.7274\n",
      "train error: \n",
      " D loss: 1.289339, G loss: 0.992471, D accuracy: 54.0%, cell accuracy: 21957.4%%, board accuracy: 90.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.273352, G loss: 1.084054, D accuracy: 55.0%, cell accuracy: 21946.6%%, board accuracy: 87.4% \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.1012, G loss: 1.4234\n",
      "[124/1778] D loss: 1.0564, G loss: 1.1933\n",
      "[244/1778] D loss: 1.3890, G loss: 0.7481\n",
      "[364/1778] D loss: 0.8260, G loss: 1.5017\n",
      "[484/1778] D loss: 1.3880, G loss: 0.6770\n",
      "[604/1778] D loss: 1.3907, G loss: 0.7489\n",
      "[724/1778] D loss: 1.0444, G loss: 1.6188\n",
      "[844/1778] D loss: 1.3914, G loss: 0.7497\n",
      "[964/1778] D loss: 1.4142, G loss: 0.7377\n",
      "[1084/1778] D loss: 1.3907, G loss: 0.7326\n",
      "[1204/1778] D loss: 1.3895, G loss: 0.7069\n",
      "[1324/1778] D loss: 1.3947, G loss: 0.6183\n",
      "[1444/1778] D loss: 1.3870, G loss: 0.6815\n",
      "[1564/1778] D loss: 1.3944, G loss: 0.6392\n",
      "[1684/1778] D loss: 1.0508, G loss: 1.3694\n",
      "train error: \n",
      " D loss: 1.291275, G loss: 0.928468, D accuracy: 54.2%, cell accuracy: 21960.6%%, board accuracy: 90.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.266516, G loss: 1.011840, D accuracy: 55.2%, cell accuracy: 21949.5%%, board accuracy: 88.5% \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3908, G loss: 0.7051\n",
      "[124/1778] D loss: 1.0486, G loss: 1.6146\n",
      "[244/1778] D loss: 1.3932, G loss: 0.6809\n",
      "[364/1778] D loss: 1.3873, G loss: 0.6716\n",
      "[484/1778] D loss: 1.0477, G loss: 1.4911\n",
      "[604/1778] D loss: 1.3914, G loss: 0.6614\n",
      "[724/1778] D loss: 1.0510, G loss: 1.5095\n",
      "[844/1778] D loss: 1.0503, G loss: 1.4063\n",
      "[964/1778] D loss: 1.3870, G loss: 0.7004\n",
      "[1084/1778] D loss: 1.3946, G loss: 0.7039\n",
      "[1204/1778] D loss: 1.3888, G loss: 0.6660\n",
      "[1324/1778] D loss: 1.5058, G loss: 0.9429\n",
      "[1444/1778] D loss: 1.3953, G loss: 0.7134\n",
      "[1564/1778] D loss: 1.3867, G loss: 0.6888\n",
      "[1684/1778] D loss: 1.3954, G loss: 0.6099\n",
      "train error: \n",
      " D loss: 1.287772, G loss: 0.902961, D accuracy: 54.0%, cell accuracy: 21965.9%%, board accuracy: 91.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.267104, G loss: 0.949709, D accuracy: 54.8%, cell accuracy: 21955.9%%, board accuracy: 89.6% \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3906, G loss: 0.7557\n",
      "[124/1778] D loss: 1.0509, G loss: 1.4230\n",
      "[244/1778] D loss: 1.3873, G loss: 0.7081\n",
      "[364/1778] D loss: 1.3871, G loss: 0.6921\n",
      "[484/1778] D loss: 1.0493, G loss: 1.4670\n",
      "[604/1778] D loss: 1.3982, G loss: 0.5874\n",
      "[724/1778] D loss: 1.0457, G loss: 1.5353\n",
      "[844/1778] D loss: 1.3871, G loss: 0.6739\n",
      "[964/1778] D loss: 1.3871, G loss: 0.7155\n",
      "[1084/1778] D loss: 1.3909, G loss: 0.6687\n",
      "[1204/1778] D loss: 1.3878, G loss: 0.6689\n",
      "[1324/1778] D loss: 1.3886, G loss: 0.7122\n",
      "[1444/1778] D loss: 1.0501, G loss: 1.4692\n",
      "[1564/1778] D loss: 1.0570, G loss: 1.6295\n",
      "[1684/1778] D loss: 1.0598, G loss: 1.2245\n",
      "train error: \n",
      " D loss: 1.292670, G loss: 0.979763, D accuracy: 54.2%, cell accuracy: 21953.1%%, board accuracy: 89.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.274116, G loss: 1.058842, D accuracy: 55.4%, cell accuracy: 21937.6%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.4017, G loss: 0.8108\n",
      "[124/1778] D loss: 1.3887, G loss: 0.6755\n",
      "[244/1778] D loss: 1.4087, G loss: 0.6298\n",
      "[364/1778] D loss: 1.0458, G loss: 1.4896\n",
      "[484/1778] D loss: 1.3866, G loss: 0.6742\n",
      "[604/1778] D loss: 1.1537, G loss: 0.9400\n",
      "[724/1778] D loss: 1.3696, G loss: 0.7528\n",
      "[844/1778] D loss: 1.3878, G loss: 0.7145\n",
      "[964/1778] D loss: 1.0055, G loss: 1.2574\n",
      "[1084/1778] D loss: 1.0570, G loss: 1.3186\n",
      "[1204/1778] D loss: 1.3913, G loss: 0.6814\n",
      "[1324/1778] D loss: 1.0507, G loss: 1.3257\n",
      "[1444/1778] D loss: 1.3873, G loss: 0.6866\n",
      "[1564/1778] D loss: 1.0496, G loss: 1.6164\n",
      "[1684/1778] D loss: 1.0478, G loss: 1.7146\n",
      "train error: \n",
      " D loss: 1.289163, G loss: 0.870890, D accuracy: 53.9%, cell accuracy: 21962.7%%, board accuracy: 91.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.268101, G loss: 0.927471, D accuracy: 55.0%, cell accuracy: 21954.1%%, board accuracy: 89.4% \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3919, G loss: 0.6402\n",
      "[124/1778] D loss: 1.0443, G loss: 1.5482\n",
      "[244/1778] D loss: 1.0509, G loss: 1.5275\n",
      "[364/1778] D loss: 1.3977, G loss: 0.6051\n",
      "[484/1778] D loss: 1.3895, G loss: 0.7187\n",
      "[604/1778] D loss: 1.3886, G loss: 0.6568\n",
      "[724/1778] D loss: 1.3919, G loss: 0.7450\n",
      "[844/1778] D loss: 1.3958, G loss: 0.6399\n",
      "[964/1778] D loss: 1.3908, G loss: 0.7349\n",
      "[1084/1778] D loss: 1.0513, G loss: 1.3888\n",
      "[1204/1778] D loss: 1.0580, G loss: 1.5049\n",
      "[1324/1778] D loss: 1.3883, G loss: 0.6707\n",
      "[1444/1778] D loss: 1.4095, G loss: 0.8378\n",
      "[1564/1778] D loss: 1.3878, G loss: 0.6919\n",
      "[1684/1778] D loss: 1.3879, G loss: 0.6684\n",
      "train error: \n",
      " D loss: 1.283363, G loss: 0.919605, D accuracy: 54.2%, cell accuracy: 21958.2%%, board accuracy: 90.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.264866, G loss: 0.975351, D accuracy: 55.5%, cell accuracy: 21948.2%%, board accuracy: 88.1% \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3866, G loss: 0.6826\n",
      "[124/1778] D loss: 1.3901, G loss: 0.6753\n",
      "[244/1778] D loss: 1.3890, G loss: 0.6506\n",
      "[364/1778] D loss: 1.0551, G loss: 1.2776\n",
      "[484/1778] D loss: 1.3889, G loss: 0.6842\n",
      "[604/1778] D loss: 1.2630, G loss: 0.7560\n",
      "[724/1778] D loss: 1.4036, G loss: 0.5879\n",
      "[844/1778] D loss: 1.0909, G loss: 1.2087\n",
      "[964/1778] D loss: 1.3877, G loss: 0.6866\n",
      "[1084/1778] D loss: 1.3877, G loss: 0.6800\n",
      "[1204/1778] D loss: 1.0421, G loss: 1.7777\n",
      "[1324/1778] D loss: 1.3953, G loss: 0.7506\n",
      "[1444/1778] D loss: 1.3914, G loss: 0.6926\n",
      "[1564/1778] D loss: 1.3927, G loss: 0.8229\n",
      "[1684/1778] D loss: 1.3904, G loss: 0.6517\n",
      "train error: \n",
      " D loss: 1.286751, G loss: 0.939512, D accuracy: 54.1%, cell accuracy: 21958.6%%, board accuracy: 90.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.260783, G loss: 0.997283, D accuracy: 55.3%, cell accuracy: 21942.8%%, board accuracy: 87.6% \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3874, G loss: 0.7244\n",
      "[124/1778] D loss: 1.3866, G loss: 0.6989\n",
      "[244/1778] D loss: 1.3874, G loss: 0.6594\n",
      "[364/1778] D loss: 1.0461, G loss: 1.5375\n",
      "[484/1778] D loss: 1.4904, G loss: 0.6577\n",
      "[604/1778] D loss: 1.4192, G loss: 0.7673\n",
      "[724/1778] D loss: 1.0489, G loss: 1.6661\n",
      "[844/1778] D loss: 1.3220, G loss: 0.7909\n",
      "[964/1778] D loss: 1.0440, G loss: 1.4405\n",
      "[1084/1778] D loss: 1.3893, G loss: 0.6654\n",
      "[1204/1778] D loss: 1.3935, G loss: 0.6148\n",
      "[1324/1778] D loss: 1.3915, G loss: 0.7650\n",
      "[1444/1778] D loss: 1.3915, G loss: 0.6705\n",
      "[1564/1778] D loss: 1.3882, G loss: 0.7065\n",
      "[1684/1778] D loss: 1.0505, G loss: 1.3885\n",
      "train error: \n",
      " D loss: 1.284574, G loss: 0.905502, D accuracy: 54.7%, cell accuracy: 21952.4%%, board accuracy: 88.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.264824, G loss: 0.969196, D accuracy: 55.5%, cell accuracy: 21942.1%%, board accuracy: 85.4% \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3993, G loss: 0.6154\n",
      "[124/1778] D loss: 1.3943, G loss: 0.7386\n",
      "[244/1778] D loss: 1.0580, G loss: 1.2904\n",
      "[364/1778] D loss: 1.3975, G loss: 0.5857\n",
      "[484/1778] D loss: 1.3896, G loss: 0.7171\n",
      "[604/1778] D loss: 1.3880, G loss: 0.7011\n",
      "[724/1778] D loss: 1.0454, G loss: 1.6758\n",
      "[844/1778] D loss: 1.3883, G loss: 0.7382\n",
      "[964/1778] D loss: 1.0481, G loss: 1.5292\n",
      "[1084/1778] D loss: 1.3896, G loss: 0.6479\n",
      "[1204/1778] D loss: 1.3864, G loss: 0.6794\n",
      "[1324/1778] D loss: 1.3865, G loss: 0.7125\n",
      "[1444/1778] D loss: 1.0489, G loss: 1.3552\n",
      "[1564/1778] D loss: 1.3863, G loss: 0.7203\n",
      "[1684/1778] D loss: 1.0419, G loss: 1.4505\n",
      "train error: \n",
      " D loss: 1.289774, G loss: 1.025551, D accuracy: 53.7%, cell accuracy: 21950.1%%, board accuracy: 89.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.273885, G loss: 1.092285, D accuracy: 54.6%, cell accuracy: 21939.4%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3943, G loss: 0.7627\n",
      "[124/1778] D loss: 1.0319, G loss: 1.9929\n",
      "[244/1778] D loss: 1.3966, G loss: 0.7830\n",
      "[364/1778] D loss: 1.0453, G loss: 1.5401\n",
      "[484/1778] D loss: 1.0490, G loss: 1.5506\n",
      "[604/1778] D loss: 1.3904, G loss: 0.6424\n",
      "[724/1778] D loss: 1.3967, G loss: 0.7299\n",
      "[844/1778] D loss: 1.1406, G loss: 1.6807\n",
      "[964/1778] D loss: 1.3893, G loss: 0.7024\n",
      "[1084/1778] D loss: 1.3869, G loss: 0.6778\n",
      "[1204/1778] D loss: 1.3891, G loss: 0.7222\n",
      "[1324/1778] D loss: 1.3936, G loss: 0.7460\n",
      "[1444/1778] D loss: 1.3866, G loss: 0.6941\n",
      "[1564/1778] D loss: 1.3976, G loss: 0.7883\n",
      "[1684/1778] D loss: 1.3929, G loss: 0.6511\n",
      "train error: \n",
      " D loss: 1.287944, G loss: 0.988011, D accuracy: 54.0%, cell accuracy: 21953.2%%, board accuracy: 89.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.267999, G loss: 1.047151, D accuracy: 55.0%, cell accuracy: 21943.2%%, board accuracy: 86.9% \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0459, G loss: 1.7885\n",
      "[124/1778] D loss: 1.0467, G loss: 1.4237\n",
      "[244/1778] D loss: 1.4084, G loss: 0.8179\n",
      "[364/1778] D loss: 1.3936, G loss: 0.7765\n",
      "[484/1778] D loss: 1.3873, G loss: 0.7103\n",
      "[604/1778] D loss: 1.0478, G loss: 1.4812\n",
      "[724/1778] D loss: 1.3955, G loss: 0.6147\n",
      "[844/1778] D loss: 1.0447, G loss: 1.5694\n",
      "[964/1778] D loss: 1.3947, G loss: 0.7792\n",
      "[1084/1778] D loss: 1.3910, G loss: 0.6935\n",
      "[1204/1778] D loss: 1.3892, G loss: 0.7234\n",
      "[1324/1778] D loss: 1.4434, G loss: 0.5768\n",
      "[1444/1778] D loss: 1.3882, G loss: 0.7430\n",
      "[1564/1778] D loss: 1.0497, G loss: 1.3375\n",
      "[1684/1778] D loss: 1.3871, G loss: 0.7241\n",
      "train error: \n",
      " D loss: 1.286916, G loss: 0.978538, D accuracy: 54.0%, cell accuracy: 21953.3%%, board accuracy: 89.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.270349, G loss: 1.056029, D accuracy: 55.5%, cell accuracy: 21940.8%%, board accuracy: 86.5% \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3868, G loss: 0.7369\n",
      "[124/1778] D loss: 1.0507, G loss: 1.5645\n",
      "[244/1778] D loss: 1.3865, G loss: 0.7099\n",
      "[364/1778] D loss: 1.3910, G loss: 0.6082\n",
      "[484/1778] D loss: 1.3939, G loss: 0.6615\n",
      "[604/1778] D loss: 0.7147, G loss: 2.3896\n",
      "[724/1778] D loss: 1.3869, G loss: 0.6998\n",
      "[844/1778] D loss: 1.2787, G loss: 0.8867\n",
      "[964/1778] D loss: 1.4780, G loss: 0.7899\n",
      "[1084/1778] D loss: 1.3934, G loss: 0.7705\n",
      "[1204/1778] D loss: 1.3811, G loss: 0.6906\n",
      "[1324/1778] D loss: 1.3908, G loss: 0.6887\n",
      "[1444/1778] D loss: 1.3875, G loss: 0.6900\n",
      "[1564/1778] D loss: 1.3876, G loss: 0.6736\n",
      "[1684/1778] D loss: 1.3897, G loss: 0.7317\n",
      "train error: \n",
      " D loss: 1.288992, G loss: 1.044151, D accuracy: 53.8%, cell accuracy: 21948.8%%, board accuracy: 89.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.275967, G loss: 1.101985, D accuracy: 54.7%, cell accuracy: 21941.9%%, board accuracy: 87.4% \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3938, G loss: 0.7702\n",
      "[124/1778] D loss: 1.3933, G loss: 0.7033\n",
      "[244/1778] D loss: 1.3839, G loss: 0.6930\n",
      "[364/1778] D loss: 1.3689, G loss: 0.7228\n",
      "[484/1778] D loss: 1.3875, G loss: 0.7142\n",
      "[604/1778] D loss: 1.3871, G loss: 0.7137\n",
      "[724/1778] D loss: 1.3913, G loss: 0.7390\n",
      "[844/1778] D loss: 1.0443, G loss: 1.8233\n",
      "[964/1778] D loss: 1.3881, G loss: 0.7058\n",
      "[1084/1778] D loss: 1.3944, G loss: 0.6051\n",
      "[1204/1778] D loss: 1.3973, G loss: 0.7805\n",
      "[1324/1778] D loss: 1.0496, G loss: 1.3830\n",
      "[1444/1778] D loss: 1.0585, G loss: 1.2304\n",
      "[1564/1778] D loss: 1.3896, G loss: 0.7500\n",
      "[1684/1778] D loss: 1.3934, G loss: 0.6323\n",
      "train error: \n",
      " D loss: 1.288531, G loss: 0.851949, D accuracy: 54.4%, cell accuracy: 21950.6%%, board accuracy: 90.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.265546, G loss: 0.916682, D accuracy: 55.5%, cell accuracy: 21939.9%%, board accuracy: 88.3% \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0538, G loss: 1.3919\n",
      "[124/1778] D loss: 1.3885, G loss: 0.7245\n",
      "[244/1778] D loss: 1.0468, G loss: 1.4261\n",
      "[364/1778] D loss: 1.0496, G loss: 1.4668\n",
      "[484/1778] D loss: 1.3946, G loss: 0.8078\n",
      "[604/1778] D loss: 1.3916, G loss: 0.6253\n",
      "[724/1778] D loss: 1.3871, G loss: 0.7305\n",
      "[844/1778] D loss: 1.3914, G loss: 0.6781\n",
      "[964/1778] D loss: 1.4125, G loss: 0.5565\n",
      "[1084/1778] D loss: 1.3867, G loss: 0.7057\n",
      "[1204/1778] D loss: 1.4174, G loss: 0.6550\n",
      "[1324/1778] D loss: 1.3888, G loss: 0.6989\n",
      "[1444/1778] D loss: 1.3844, G loss: 0.7435\n",
      "[1564/1778] D loss: 1.3879, G loss: 0.6771\n",
      "[1684/1778] D loss: 1.0629, G loss: 1.2342\n",
      "train error: \n",
      " D loss: 1.290871, G loss: 0.967061, D accuracy: 54.7%, cell accuracy: 21951.9%%, board accuracy: 88.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.271938, G loss: 1.005053, D accuracy: 55.4%, cell accuracy: 21944.6%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3920, G loss: 0.7424\n",
      "[124/1778] D loss: 1.0595, G loss: 1.2749\n",
      "[244/1778] D loss: 1.3938, G loss: 0.6663\n",
      "[364/1778] D loss: 1.0864, G loss: 1.0012\n",
      "[484/1778] D loss: 1.3912, G loss: 0.6935\n",
      "[604/1778] D loss: 1.2003, G loss: 1.5311\n",
      "[724/1778] D loss: 1.3877, G loss: 0.6905\n",
      "[844/1778] D loss: 1.3918, G loss: 0.6403\n",
      "[964/1778] D loss: 1.2913, G loss: 1.3171\n",
      "[1084/1778] D loss: 1.3613, G loss: 0.7913\n",
      "[1204/1778] D loss: 0.7546, G loss: 2.1265\n",
      "[1324/1778] D loss: 1.3961, G loss: 0.6776\n",
      "[1444/1778] D loss: 1.0530, G loss: 1.3351\n",
      "[1564/1778] D loss: 1.3927, G loss: 0.7786\n",
      "[1684/1778] D loss: 1.3986, G loss: 0.5963\n",
      "train error: \n",
      " D loss: 1.286511, G loss: 0.898514, D accuracy: 54.6%, cell accuracy: 21958.2%%, board accuracy: 88.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.279375, G loss: 0.934556, D accuracy: 54.5%, cell accuracy: 21955.4%%, board accuracy: 87.4% \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3872, G loss: 0.6697\n",
      "[124/1778] D loss: 1.0831, G loss: 1.3186\n",
      "[244/1778] D loss: 1.3921, G loss: 0.6675\n",
      "[364/1778] D loss: 1.0596, G loss: 1.2277\n",
      "[484/1778] D loss: 1.1716, G loss: 0.7133\n",
      "[604/1778] D loss: 1.3904, G loss: 0.7649\n",
      "[724/1778] D loss: 1.0993, G loss: 0.9735\n",
      "[844/1778] D loss: 1.3849, G loss: 0.6975\n",
      "[964/1778] D loss: 1.1901, G loss: 1.5825\n",
      "[1084/1778] D loss: 1.3865, G loss: 0.7149\n",
      "[1204/1778] D loss: 1.3893, G loss: 0.6407\n",
      "[1324/1778] D loss: 1.3925, G loss: 0.7216\n",
      "[1444/1778] D loss: 1.3874, G loss: 0.7011\n",
      "[1564/1778] D loss: 1.3883, G loss: 0.6462\n",
      "[1684/1778] D loss: 1.3952, G loss: 0.6822\n",
      "train error: \n",
      " D loss: 1.303039, G loss: 0.850424, D accuracy: 54.2%, cell accuracy: 21960.7%%, board accuracy: 89.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.301850, G loss: 0.893612, D accuracy: 54.6%, cell accuracy: 21957.2%%, board accuracy: 87.8% \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3892, G loss: 0.7241\n",
      "[124/1778] D loss: 0.7343, G loss: 1.8314\n",
      "[244/1778] D loss: 1.0572, G loss: 1.5416\n",
      "[364/1778] D loss: 1.3880, G loss: 0.6419\n",
      "[484/1778] D loss: 1.3921, G loss: 0.6363\n",
      "[604/1778] D loss: 1.3879, G loss: 0.6779\n",
      "[724/1778] D loss: 1.0475, G loss: 1.5276\n",
      "[844/1778] D loss: 1.3896, G loss: 0.7206\n",
      "[964/1778] D loss: 1.3941, G loss: 0.7618\n",
      "[1084/1778] D loss: 1.3930, G loss: 0.6452\n",
      "[1204/1778] D loss: 1.0782, G loss: 1.0244\n",
      "[1324/1778] D loss: 1.3165, G loss: 0.7551\n",
      "[1444/1778] D loss: 1.3881, G loss: 0.7352\n",
      "[1564/1778] D loss: 1.3894, G loss: 0.6864\n",
      "[1684/1778] D loss: 1.3914, G loss: 0.6055\n",
      "train error: \n",
      " D loss: 1.304216, G loss: 0.908175, D accuracy: 53.8%, cell accuracy: 21961.1%%, board accuracy: 89.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.294096, G loss: 0.954557, D accuracy: 54.6%, cell accuracy: 21952.3%%, board accuracy: 87.4% \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0795, G loss: 1.3211\n",
      "[124/1778] D loss: 1.2053, G loss: 1.3952\n",
      "[244/1778] D loss: 1.3869, G loss: 0.6797\n",
      "[364/1778] D loss: 1.3870, G loss: 0.7087\n",
      "[484/1778] D loss: 1.3906, G loss: 0.6808\n",
      "[604/1778] D loss: 1.0531, G loss: 1.3056\n",
      "[724/1778] D loss: 1.3884, G loss: 0.7233\n",
      "[844/1778] D loss: 1.3914, G loss: 0.6381\n",
      "[964/1778] D loss: 1.4380, G loss: 0.9495\n",
      "[1084/1778] D loss: 1.0572, G loss: 1.5667\n",
      "[1204/1778] D loss: 1.3798, G loss: 0.6997\n",
      "[1324/1778] D loss: 0.9584, G loss: 1.4175\n",
      "[1444/1778] D loss: 1.3927, G loss: 0.7791\n",
      "[1564/1778] D loss: 1.3902, G loss: 0.6045\n",
      "[1684/1778] D loss: 1.0924, G loss: 1.2041\n",
      "train error: \n",
      " D loss: 1.306575, G loss: 0.920296, D accuracy: 54.7%, cell accuracy: 21947.6%%, board accuracy: 86.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.300613, G loss: 0.975771, D accuracy: 55.4%, cell accuracy: 21930.9%%, board accuracy: 82.0% \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3869, G loss: 0.7007\n",
      "[124/1778] D loss: 1.3632, G loss: 0.6996\n",
      "[244/1778] D loss: 1.3885, G loss: 0.6875\n",
      "[364/1778] D loss: 1.3854, G loss: 0.6247\n",
      "[484/1778] D loss: 1.3940, G loss: 0.6094\n",
      "[604/1778] D loss: 1.3878, G loss: 0.6665\n",
      "[724/1778] D loss: 1.3927, G loss: 0.6669\n",
      "[844/1778] D loss: 1.3899, G loss: 0.7214\n",
      "[964/1778] D loss: 1.3912, G loss: 0.6079\n",
      "[1084/1778] D loss: 1.4110, G loss: 0.8399\n",
      "[1204/1778] D loss: 1.3919, G loss: 0.6904\n",
      "[1324/1778] D loss: 1.3899, G loss: 0.6757\n",
      "[1444/1778] D loss: 1.3886, G loss: 0.7031\n",
      "[1564/1778] D loss: 1.0662, G loss: 1.1550\n",
      "[1684/1778] D loss: 1.3879, G loss: 0.7086\n",
      "train error: \n",
      " D loss: 1.321871, G loss: 0.958082, D accuracy: 53.2%, cell accuracy: 21957.1%%, board accuracy: 89.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.327491, G loss: 0.987493, D accuracy: 53.3%, cell accuracy: 21943.5%%, board accuracy: 85.1% \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3297, G loss: 1.0560\n",
      "[124/1778] D loss: 1.3888, G loss: 0.7151\n",
      "[244/1778] D loss: 1.3896, G loss: 0.6566\n",
      "[364/1778] D loss: 1.3935, G loss: 0.6668\n",
      "[484/1778] D loss: 1.0541, G loss: 1.2601\n",
      "[604/1778] D loss: 1.3876, G loss: 0.6655\n",
      "[724/1778] D loss: 1.2787, G loss: 0.8226\n",
      "[844/1778] D loss: 1.3978, G loss: 0.5850\n",
      "[964/1778] D loss: 1.3941, G loss: 0.6387\n",
      "[1084/1778] D loss: 1.3860, G loss: 0.6940\n",
      "[1204/1778] D loss: 1.3973, G loss: 0.6487\n",
      "[1324/1778] D loss: 1.0771, G loss: 1.2814\n",
      "[1444/1778] D loss: 1.3891, G loss: 0.7487\n",
      "[1564/1778] D loss: 1.3906, G loss: 0.7843\n",
      "[1684/1778] D loss: 1.3898, G loss: 0.7449\n",
      "train error: \n",
      " D loss: 1.307231, G loss: 0.855362, D accuracy: 54.6%, cell accuracy: 21955.2%%, board accuracy: 88.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.308064, G loss: 0.882134, D accuracy: 55.0%, cell accuracy: 21948.4%%, board accuracy: 86.5% \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0642, G loss: 1.1610\n",
      "[124/1778] D loss: 1.3933, G loss: 0.6750\n",
      "[244/1778] D loss: 1.6714, G loss: 0.9467\n",
      "[364/1778] D loss: 1.3885, G loss: 0.6464\n",
      "[484/1778] D loss: 1.0616, G loss: 1.0788\n",
      "[604/1778] D loss: 1.3941, G loss: 0.6095\n",
      "[724/1778] D loss: 1.3893, G loss: 0.7886\n",
      "[844/1778] D loss: 0.7527, G loss: 2.2783\n",
      "[964/1778] D loss: 1.2864, G loss: 0.7327\n",
      "[1084/1778] D loss: 1.3876, G loss: 0.7203\n",
      "[1204/1778] D loss: 1.0961, G loss: 0.9336\n",
      "[1324/1778] D loss: 1.0738, G loss: 1.0705\n",
      "[1444/1778] D loss: 1.3929, G loss: 0.7818\n",
      "[1564/1778] D loss: 1.1005, G loss: 0.9903\n",
      "[1684/1778] D loss: 1.3969, G loss: 0.7265\n",
      "train error: \n",
      " D loss: 1.296542, G loss: 0.871139, D accuracy: 54.5%, cell accuracy: 21951.5%%, board accuracy: 88.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.290872, G loss: 0.908400, D accuracy: 55.3%, cell accuracy: 21936.3%%, board accuracy: 84.2% \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3895, G loss: 0.7099\n",
      "[124/1778] D loss: 1.1083, G loss: 0.8351\n",
      "[244/1778] D loss: 1.3592, G loss: 0.6774\n",
      "[364/1778] D loss: 1.3873, G loss: 0.6829\n",
      "[484/1778] D loss: 1.0636, G loss: 1.1066\n",
      "[604/1778] D loss: 1.3882, G loss: 0.6681\n",
      "[724/1778] D loss: 1.0894, G loss: 0.9110\n",
      "[844/1778] D loss: 1.3876, G loss: 0.6889\n",
      "[964/1778] D loss: 1.3239, G loss: 0.7967\n",
      "[1084/1778] D loss: 1.4203, G loss: 0.5632\n",
      "[1204/1778] D loss: 1.3878, G loss: 0.6645\n",
      "[1324/1778] D loss: 1.0523, G loss: 1.4053\n",
      "[1444/1778] D loss: 1.3878, G loss: 0.6882\n",
      "[1564/1778] D loss: 1.3967, G loss: 0.7298\n",
      "[1684/1778] D loss: 1.3378, G loss: 0.8689\n",
      "train error: \n",
      " D loss: 1.301896, G loss: 0.823361, D accuracy: 55.3%, cell accuracy: 21954.2%%, board accuracy: 88.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.306763, G loss: 0.854597, D accuracy: 55.7%, cell accuracy: 21942.8%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3735, G loss: 0.6596\n",
      "[124/1778] D loss: 1.3100, G loss: 0.7669\n",
      "[244/1778] D loss: 1.0649, G loss: 1.5238\n",
      "[364/1778] D loss: 1.4061, G loss: 0.5758\n",
      "[484/1778] D loss: 1.0865, G loss: 1.0164\n",
      "[604/1778] D loss: 1.3885, G loss: 0.6389\n",
      "[724/1778] D loss: 1.3974, G loss: 0.6204\n",
      "[844/1778] D loss: 1.5307, G loss: 1.5854\n",
      "[964/1778] D loss: 1.3909, G loss: 0.7499\n",
      "[1084/1778] D loss: 1.3869, G loss: 0.6858\n",
      "[1204/1778] D loss: 1.2019, G loss: 1.3703\n",
      "[1324/1778] D loss: 1.0752, G loss: 1.0707\n",
      "[1444/1778] D loss: 0.3856, G loss: 2.7869\n",
      "[1564/1778] D loss: 1.3875, G loss: 0.7269\n",
      "[1684/1778] D loss: 1.3559, G loss: 0.7068\n",
      "train error: \n",
      " D loss: 1.285666, G loss: 0.910197, D accuracy: 54.0%, cell accuracy: 21962.2%%, board accuracy: 89.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.285719, G loss: 0.941599, D accuracy: 54.5%, cell accuracy: 21953.8%%, board accuracy: 88.1% \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0597, G loss: 1.1752\n",
      "[124/1778] D loss: 1.0223, G loss: 0.9846\n",
      "[244/1778] D loss: 1.3876, G loss: 0.7460\n",
      "[364/1778] D loss: 1.3063, G loss: 1.1087\n",
      "[484/1778] D loss: 1.0571, G loss: 1.2337\n",
      "[604/1778] D loss: 1.2045, G loss: 1.4640\n",
      "[724/1778] D loss: 1.3890, G loss: 0.7207\n",
      "[844/1778] D loss: 1.3876, G loss: 0.6576\n",
      "[964/1778] D loss: 1.3920, G loss: 0.7024\n",
      "[1084/1778] D loss: 1.3860, G loss: 0.6978\n",
      "[1204/1778] D loss: 1.3864, G loss: 0.7004\n",
      "[1324/1778] D loss: 1.3873, G loss: 0.7023\n",
      "[1444/1778] D loss: 1.0532, G loss: 1.2939\n",
      "[1564/1778] D loss: 1.0617, G loss: 1.1521\n",
      "[1684/1778] D loss: 1.4395, G loss: 0.6341\n",
      "train error: \n",
      " D loss: 1.307922, G loss: 0.755652, D accuracy: 55.2%, cell accuracy: 21949.2%%, board accuracy: 86.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.299175, G loss: 0.788211, D accuracy: 56.0%, cell accuracy: 21938.1%%, board accuracy: 84.5% \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0376, G loss: 1.1425\n",
      "[124/1778] D loss: 1.3905, G loss: 0.7491\n",
      "[244/1778] D loss: 1.3913, G loss: 0.6988\n",
      "[364/1778] D loss: 1.4226, G loss: 0.5381\n",
      "[484/1778] D loss: 1.1592, G loss: 1.1590\n",
      "[604/1778] D loss: 1.5694, G loss: 1.1150\n",
      "[724/1778] D loss: 1.3938, G loss: 0.7612\n",
      "[844/1778] D loss: 1.3517, G loss: 0.7334\n",
      "[964/1778] D loss: 1.3893, G loss: 0.6814\n",
      "[1084/1778] D loss: 1.4001, G loss: 0.5909\n",
      "[1204/1778] D loss: 1.3878, G loss: 0.6354\n",
      "[1324/1778] D loss: 1.3824, G loss: 0.7093\n",
      "[1444/1778] D loss: 1.3466, G loss: 1.0637\n",
      "[1564/1778] D loss: 1.3869, G loss: 0.6932\n",
      "[1684/1778] D loss: 1.3899, G loss: 0.6852\n",
      "train error: \n",
      " D loss: 1.309313, G loss: 0.911306, D accuracy: 53.5%, cell accuracy: 21961.3%%, board accuracy: 89.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.301651, G loss: 0.966448, D accuracy: 54.1%, cell accuracy: 21954.1%%, board accuracy: 87.6% \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3314, G loss: 0.8929\n",
      "[124/1778] D loss: 1.3884, G loss: 0.7296\n",
      "[244/1778] D loss: 1.3217, G loss: 0.8363\n",
      "[364/1778] D loss: 1.3883, G loss: 0.6982\n",
      "[484/1778] D loss: 0.7447, G loss: 1.5731\n",
      "[604/1778] D loss: 0.7813, G loss: 1.4215\n",
      "[724/1778] D loss: 1.3868, G loss: 0.6748\n",
      "[844/1778] D loss: 1.1332, G loss: 0.9459\n",
      "[964/1778] D loss: 1.3909, G loss: 0.6426\n",
      "[1084/1778] D loss: 1.0668, G loss: 1.1386\n",
      "[1204/1778] D loss: 0.7904, G loss: 1.1115\n",
      "[1324/1778] D loss: 1.2258, G loss: 0.6737\n",
      "[1444/1778] D loss: 1.3920, G loss: 0.6496\n",
      "[1564/1778] D loss: 1.3882, G loss: 0.7482\n",
      "[1684/1778] D loss: 1.3749, G loss: 0.7142\n",
      "train error: \n",
      " D loss: 1.304790, G loss: 0.846036, D accuracy: 54.4%, cell accuracy: 21953.8%%, board accuracy: 87.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.308080, G loss: 0.884589, D accuracy: 54.8%, cell accuracy: 21943.2%%, board accuracy: 84.7% \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3881, G loss: 0.7069\n",
      "[124/1778] D loss: 1.3944, G loss: 0.6382\n",
      "[244/1778] D loss: 1.3883, G loss: 0.6532\n",
      "[364/1778] D loss: 1.3873, G loss: 0.7216\n",
      "[484/1778] D loss: 1.3877, G loss: 0.7105\n",
      "[604/1778] D loss: 1.2601, G loss: 0.6177\n",
      "[724/1778] D loss: 1.3990, G loss: 0.5963\n",
      "[844/1778] D loss: 1.3883, G loss: 0.7017\n",
      "[964/1778] D loss: 1.3905, G loss: 0.6797\n",
      "[1084/1778] D loss: 1.3891, G loss: 0.6365\n",
      "[1204/1778] D loss: 1.3890, G loss: 0.6447\n",
      "[1324/1778] D loss: 1.4014, G loss: 0.6055\n",
      "[1444/1778] D loss: 1.3898, G loss: 0.6419\n",
      "[1564/1778] D loss: 1.4104, G loss: 0.7958\n",
      "[1684/1778] D loss: 1.3905, G loss: 0.6638\n",
      "train error: \n",
      " D loss: 1.309286, G loss: 0.751825, D accuracy: 54.1%, cell accuracy: 21957.5%%, board accuracy: 88.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.306896, G loss: 0.799857, D accuracy: 54.5%, cell accuracy: 21945.5%%, board accuracy: 85.4% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 300\n",
    "\n",
    "gen = TetrisModel().to(device)\n",
    "disc = TetrisDiscriminator().to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer_gen = torch.optim.Adam(gen.parameters(), lr=learning_rate)\n",
    "optimizer_disc = torch.optim.Adam(disc.parameters(), lr=learning_rate)\n",
    "\n",
    "log_dir = os.path.join(\"runs\", \"tetris_emulator\")\n",
    "log_subdir = os.path.join(log_dir, datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "tb_writer = SummaryWriter(log_subdir)\n",
    "\n",
    "train_examples = find_interesting_examples(train_dataset)\n",
    "test_examples = find_interesting_examples(test_dataset)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, gen, disc, loss_fn, optimizer_gen, optimizer_disc)\n",
    "    test_loop(\"train\", train_dataloader, gen, disc, loss_fn, tb_writer, epoch, train_examples)\n",
    "    test_loop(\"test\", test_dataloader, gen, disc, loss_fn, tb_writer, epoch, test_examples)\n",
    "    gen_zero_grads = 0\n",
    "    for name, weight in gen.named_parameters():\n",
    "        tb_writer.add_histogram(f\"Weights/{name}\", weight, epoch)\n",
    "        if weight.grad is not None:\n",
    "            tb_writer.add_histogram(f\"Gradients/{name}\", weight.grad, epoch)\n",
    "            gen_zero_grads += weight.grad.numel() - weight.grad.count_nonzero().item()\n",
    "    tb_writer.add_scalar(f\"Zero gradients\", gen_zero_grads, epoch)\n",
    "    disc_zero_grads = 0\n",
    "    for name, weight in disc.named_parameters():\n",
    "        tb_writer.add_histogram(f\"Discriminator weights/{name}\", weight, epoch)\n",
    "        if weight.grad is not None:\n",
    "            tb_writer.add_histogram(f\"Discriminator gradients/{name}\", weight.grad, epoch)\n",
    "            disc_zero_grads += weight.grad.numel() - weight.grad.count_nonzero().item()\n",
    "    tb_writer.add_scalar(f\"Discriminator zero gradients\", disc_zero_grads, epoch)\n",
    "\n",
    "tb_writer.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.2935, G loss: 1.5377\n",
      "[124/1778] D loss: 1.3951, G loss: 0.7786\n",
      "[244/1778] D loss: 1.0764, G loss: 1.0401\n",
      "[364/1778] D loss: 1.1325, G loss: 0.8081\n",
      "[484/1778] D loss: 1.0554, G loss: 1.3675\n",
      "[604/1778] D loss: 1.3883, G loss: 0.7006\n",
      "[724/1778] D loss: 1.0814, G loss: 1.1674\n",
      "[844/1778] D loss: 1.1213, G loss: 1.5481\n",
      "[964/1778] D loss: 1.1007, G loss: 1.0564\n",
      "[1084/1778] D loss: 1.3871, G loss: 0.7127\n",
      "[1204/1778] D loss: 1.3914, G loss: 0.7048\n",
      "[1324/1778] D loss: 1.3956, G loss: 0.7548\n",
      "[1444/1778] D loss: 1.3821, G loss: 0.6860\n",
      "[1564/1778] D loss: 1.3896, G loss: 0.7105\n",
      "[1684/1778] D loss: 1.3888, G loss: 0.6877\n",
      "train error: \n",
      " D loss: 1.299540, G loss: 0.901601, D accuracy: 53.5%, cell accuracy: 21961.2%%, board accuracy: 88.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.290595, G loss: 0.975329, D accuracy: 54.5%, cell accuracy: 21951.4%%, board accuracy: 86.3% \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3890, G loss: 0.7333\n",
      "[124/1778] D loss: 1.3867, G loss: 0.6867\n",
      "[244/1778] D loss: 1.3968, G loss: 0.6704\n",
      "[364/1778] D loss: 1.3909, G loss: 0.7414\n",
      "[484/1778] D loss: 1.4011, G loss: 0.5943\n",
      "[604/1778] D loss: 1.3884, G loss: 0.7097\n",
      "[724/1778] D loss: 1.3898, G loss: 0.7334\n",
      "[844/1778] D loss: 1.3864, G loss: 0.7469\n",
      "[964/1778] D loss: 1.3791, G loss: 0.7202\n",
      "[1084/1778] D loss: 1.0552, G loss: 1.6176\n",
      "[1204/1778] D loss: 1.4435, G loss: 0.6786\n",
      "[1324/1778] D loss: 1.3884, G loss: 0.6868\n",
      "[1444/1778] D loss: 1.0796, G loss: 1.0106\n",
      "[1564/1778] D loss: 1.3844, G loss: 0.6104\n",
      "[1684/1778] D loss: 1.3895, G loss: 0.6410\n",
      "train error: \n",
      " D loss: 1.301374, G loss: 0.865253, D accuracy: 53.9%, cell accuracy: 21956.2%%, board accuracy: 88.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.304913, G loss: 0.906203, D accuracy: 54.3%, cell accuracy: 21948.4%%, board accuracy: 86.9% \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3876, G loss: 0.7136\n",
      "[124/1778] D loss: 1.3879, G loss: 0.6847\n",
      "[244/1778] D loss: 1.3946, G loss: 0.6223\n",
      "[364/1778] D loss: 1.3886, G loss: 0.6789\n",
      "[484/1778] D loss: 1.3920, G loss: 0.7895\n",
      "[604/1778] D loss: 1.3908, G loss: 0.6831\n",
      "[724/1778] D loss: 1.3878, G loss: 0.6907\n",
      "[844/1778] D loss: 1.0862, G loss: 1.0453\n",
      "[964/1778] D loss: 1.3929, G loss: 0.6084\n",
      "[1084/1778] D loss: 1.3929, G loss: 0.6396\n",
      "[1204/1778] D loss: 1.3967, G loss: 0.7928\n",
      "[1324/1778] D loss: 1.3878, G loss: 0.7127\n",
      "[1444/1778] D loss: 1.3894, G loss: 0.6469\n",
      "[1564/1778] D loss: 1.3874, G loss: 0.7113\n",
      "[1684/1778] D loss: 1.0651, G loss: 1.1502\n",
      "train error: \n",
      " D loss: 1.305247, G loss: 0.855022, D accuracy: 53.7%, cell accuracy: 21961.2%%, board accuracy: 89.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.311758, G loss: 0.879037, D accuracy: 53.9%, cell accuracy: 21955.6%%, board accuracy: 87.8% \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.0992, G loss: 0.9937\n",
      "[124/1778] D loss: 1.3880, G loss: 0.6696\n",
      "[244/1778] D loss: 1.5747, G loss: 1.0477\n",
      "[364/1778] D loss: 1.4939, G loss: 0.6566\n",
      "[484/1778] D loss: 1.3904, G loss: 0.6928\n",
      "[604/1778] D loss: 1.4092, G loss: 0.5531\n",
      "[724/1778] D loss: 1.3859, G loss: 0.6960\n",
      "[844/1778] D loss: 1.3888, G loss: 0.6731\n",
      "[964/1778] D loss: 1.0556, G loss: 1.2959\n",
      "[1084/1778] D loss: 1.3936, G loss: 0.6661\n",
      "[1204/1778] D loss: 1.3883, G loss: 0.6886\n",
      "[1324/1778] D loss: 1.3906, G loss: 0.7146\n",
      "[1444/1778] D loss: 1.3883, G loss: 0.6934\n",
      "[1564/1778] D loss: 1.1182, G loss: 1.1973\n",
      "[1684/1778] D loss: 1.3992, G loss: 1.0663\n",
      "train error: \n",
      " D loss: 1.303479, G loss: 0.889120, D accuracy: 53.7%, cell accuracy: 21959.0%%, board accuracy: 87.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.312733, G loss: 0.940040, D accuracy: 53.7%, cell accuracy: 21947.7%%, board accuracy: 85.4% \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3872, G loss: 0.7187\n",
      "[124/1778] D loss: 1.0097, G loss: 1.2334\n",
      "[244/1778] D loss: 1.3671, G loss: 0.8482\n",
      "[364/1778] D loss: 1.3928, G loss: 0.7468\n",
      "[484/1778] D loss: 1.3865, G loss: 0.6940\n",
      "[604/1778] D loss: 1.3866, G loss: 0.6755\n",
      "[724/1778] D loss: 1.3967, G loss: 0.6275\n",
      "[844/1778] D loss: 1.3898, G loss: 0.6699\n",
      "[964/1778] D loss: 1.3904, G loss: 0.6612\n",
      "[1084/1778] D loss: 1.3869, G loss: 0.6825\n",
      "[1204/1778] D loss: 1.4995, G loss: 0.6126\n",
      "[1324/1778] D loss: 1.4281, G loss: 0.8790\n",
      "[1444/1778] D loss: 1.0244, G loss: 1.3753\n",
      "[1564/1778] D loss: 1.0997, G loss: 1.5151\n",
      "[1684/1778] D loss: 1.3998, G loss: 0.7890\n",
      "train error: \n",
      " D loss: 1.304079, G loss: 0.959028, D accuracy: 53.5%, cell accuracy: 21963.1%%, board accuracy: 89.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.309609, G loss: 1.044811, D accuracy: 53.8%, cell accuracy: 21955.2%%, board accuracy: 87.2% \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3932, G loss: 0.7605\n",
      "[124/1778] D loss: 1.3905, G loss: 0.6731\n",
      "[244/1778] D loss: 1.3852, G loss: 0.6795\n",
      "[364/1778] D loss: 1.3869, G loss: 0.7008\n",
      "[484/1778] D loss: 1.3856, G loss: 0.7284\n",
      "[604/1778] D loss: 1.4474, G loss: 0.6155\n",
      "[724/1778] D loss: 1.3865, G loss: 0.7059\n",
      "[844/1778] D loss: 1.3068, G loss: 0.7261\n",
      "[964/1778] D loss: 1.4098, G loss: 0.5639\n",
      "[1084/1778] D loss: 1.0850, G loss: 1.2577\n",
      "[1204/1778] D loss: 1.0553, G loss: 1.1038\n",
      "[1324/1778] D loss: 1.3953, G loss: 0.6994\n",
      "[1444/1778] D loss: 1.3874, G loss: 0.7143\n",
      "[1564/1778] D loss: 1.3867, G loss: 0.7073\n",
      "[1684/1778] D loss: 1.3936, G loss: 0.6406\n",
      "train error: \n",
      " D loss: 1.335186, G loss: 0.721273, D accuracy: 53.7%, cell accuracy: 21961.1%%, board accuracy: 89.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.360385, G loss: 0.712007, D accuracy: 52.8%, cell accuracy: 21952.5%%, board accuracy: 87.2% \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3935, G loss: 0.6305\n",
      "[124/1778] D loss: 1.3894, G loss: 0.6736\n",
      "[244/1778] D loss: 1.3868, G loss: 0.7099\n",
      "[364/1778] D loss: 1.3937, G loss: 0.6284\n",
      "[484/1778] D loss: 1.0579, G loss: 1.2021\n",
      "[604/1778] D loss: 1.0573, G loss: 1.1306\n",
      "[724/1778] D loss: 1.3880, G loss: 0.7121\n",
      "[844/1778] D loss: 1.0641, G loss: 1.0630\n",
      "[964/1778] D loss: 1.3876, G loss: 0.6935\n",
      "[1084/1778] D loss: 1.3909, G loss: 0.7135\n",
      "[1204/1778] D loss: 1.7259, G loss: 0.4711\n",
      "[1324/1778] D loss: 1.3881, G loss: 0.6753\n",
      "[1444/1778] D loss: 1.3865, G loss: 0.6875\n",
      "[1564/1778] D loss: 1.3876, G loss: 0.6577\n",
      "[1684/1778] D loss: 1.3912, G loss: 0.6318\n",
      "train error: \n",
      " D loss: 1.307405, G loss: 0.899896, D accuracy: 53.2%, cell accuracy: 21960.2%%, board accuracy: 89.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.308652, G loss: 0.927855, D accuracy: 54.2%, cell accuracy: 21949.1%%, board accuracy: 86.7% \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3896, G loss: 0.7479\n",
      "[124/1778] D loss: 1.3877, G loss: 0.7114\n",
      "[244/1778] D loss: 1.3886, G loss: 0.7065\n",
      "[364/1778] D loss: 1.3897, G loss: 0.6560\n",
      "[484/1778] D loss: 1.0611, G loss: 1.2659\n",
      "[604/1778] D loss: 1.3882, G loss: 0.6788\n",
      "[724/1778] D loss: 1.4027, G loss: 0.5709\n",
      "[844/1778] D loss: 1.3876, G loss: 0.6380\n",
      "[964/1778] D loss: 1.0746, G loss: 1.1756\n",
      "[1084/1778] D loss: 1.3908, G loss: 0.7533\n",
      "[1204/1778] D loss: 1.3908, G loss: 0.7600\n",
      "[1324/1778] D loss: 1.3844, G loss: 0.6649\n",
      "[1444/1778] D loss: 1.3772, G loss: 0.9040\n",
      "[1564/1778] D loss: 1.0693, G loss: 1.3533\n",
      "[1684/1778] D loss: 1.3978, G loss: 0.8209\n",
      "train error: \n",
      " D loss: 1.312105, G loss: 0.755079, D accuracy: 54.5%, cell accuracy: 21957.9%%, board accuracy: 87.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.311553, G loss: 0.775886, D accuracy: 54.2%, cell accuracy: 21950.5%%, board accuracy: 86.0% \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3899, G loss: 0.6452\n",
      "[124/1778] D loss: 1.3941, G loss: 0.6213\n",
      "[244/1778] D loss: 1.3874, G loss: 0.6746\n",
      "[364/1778] D loss: 1.0894, G loss: 0.9137\n",
      "[484/1778] D loss: 0.7080, G loss: 2.1692\n",
      "[604/1778] D loss: 1.1405, G loss: 0.9866\n",
      "[724/1778] D loss: 1.3920, G loss: 0.6356\n",
      "[844/1778] D loss: 1.0586, G loss: 1.2297\n",
      "[964/1778] D loss: 1.3868, G loss: 0.6707\n",
      "[1084/1778] D loss: 1.3930, G loss: 0.6181\n",
      "[1204/1778] D loss: 1.3864, G loss: 0.6873\n",
      "[1324/1778] D loss: 1.5438, G loss: 1.2044\n",
      "[1444/1778] D loss: 1.0691, G loss: 1.2258\n",
      "[1564/1778] D loss: 1.3551, G loss: 0.7298\n",
      "[1684/1778] D loss: 1.3877, G loss: 0.6666\n",
      "train error: \n",
      " D loss: 1.305562, G loss: 0.847150, D accuracy: 54.0%, cell accuracy: 21958.8%%, board accuracy: 89.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.306702, G loss: 0.900851, D accuracy: 54.3%, cell accuracy: 21950.0%%, board accuracy: 86.7% \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "[4/1778] D loss: 1.3907, G loss: 0.6476\n",
      "[124/1778] D loss: 1.3942, G loss: 0.7002\n",
      "[244/1778] D loss: 1.3862, G loss: 0.6850\n",
      "[364/1778] D loss: 1.4056, G loss: 0.7858\n",
      "[484/1778] D loss: 1.3908, G loss: 0.7467\n",
      "[604/1778] D loss: 1.3896, G loss: 0.7333\n",
      "[724/1778] D loss: 1.5442, G loss: 0.5977\n",
      "[844/1778] D loss: 1.3663, G loss: 0.9761\n",
      "[964/1778] D loss: 1.2819, G loss: 1.0227\n",
      "[1084/1778] D loss: 1.3874, G loss: 0.6841\n",
      "[1204/1778] D loss: 1.3886, G loss: 0.6681\n",
      "[1324/1778] D loss: 1.3871, G loss: 0.7065\n",
      "[1444/1778] D loss: 1.0650, G loss: 1.1203\n",
      "[1564/1778] D loss: 1.4034, G loss: 0.5775\n",
      "[1684/1778] D loss: 1.3935, G loss: 0.6411\n",
      "train error: \n",
      " D loss: 1.312131, G loss: 0.760023, D accuracy: 54.6%, cell accuracy: 21955.7%%, board accuracy: 88.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.304200, G loss: 0.799028, D accuracy: 54.7%, cell accuracy: 21948.9%%, board accuracy: 85.6% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Run this as many times as needed to \"top-up\" the training\n",
    "\n",
    "extra_epochs = 10\n",
    "\n",
    "tb_writer = SummaryWriter(log_subdir)\n",
    "\n",
    "for epoch in range(epochs, epochs + extra_epochs):\n",
    "    print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, gen, disc, loss_fn, optimizer_gen, optimizer_disc)\n",
    "    test_loop(\"train\", train_dataloader, gen, disc, loss_fn, tb_writer, epoch, train_examples)\n",
    "    test_loop(\"test\", test_dataloader, gen, disc, loss_fn, tb_writer, epoch, test_examples)\n",
    "    gen_zero_grads = 0\n",
    "    for name, weight in gen.named_parameters():\n",
    "        tb_writer.add_histogram(f\"Weights/{name}\", weight, epoch)\n",
    "        if weight.grad is not None:\n",
    "            tb_writer.add_histogram(f\"Gradients/{name}\", weight.grad, epoch)\n",
    "            gen_zero_grads += weight.grad.numel() - weight.grad.count_nonzero().item()\n",
    "    tb_writer.add_scalar(f\"Zero gradients\", gen_zero_grads, epoch)\n",
    "    disc_zero_grads = 0\n",
    "    for name, weight in disc.named_parameters():\n",
    "        tb_writer.add_histogram(f\"Discriminator weights/{name}\", weight, epoch)\n",
    "        if weight.grad is not None:\n",
    "            tb_writer.add_histogram(f\"Discriminator gradients/{name}\", weight.grad, epoch)\n",
    "            disc_zero_grads += weight.grad.numel() - weight.grad.count_nonzero().item()\n",
    "    tb_writer.add_scalar(f\"Discriminator zero gradients\", disc_zero_grads, epoch)\n",
    "\n",
    "tb_writer.close()\n",
    "\n",
    "epochs += extra_epochs\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(example):\n",
    "    x, y = example\n",
    "    pred = gen(x.unsqueeze(0)).squeeze(0)\n",
    "    x, y, pred = x.argmax(0), y.argmax(0), pred.argmax(0)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "    fig.suptitle(\"Prediction vs reality\")\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "    axs[0].set_title(\"Input\")\n",
    "    axs[1].set_title(\"Predicted\")\n",
    "    axs[2].set_title(\"Reality\")\n",
    "\n",
    "    axs[0].imshow(x, vmin=0, vmax=1)\n",
    "    axs[1].imshow(pred, vmin=0, vmax=1)\n",
    "    axs[2].imshow(y, vmin=0, vmax=1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing prediction for training example 1559\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a random training prediction vs reality\n",
    "import random\n",
    "\n",
    "idx = random.randrange(len(train_dataset))\n",
    "print(f\"Showing prediction for training example {idx}\")\n",
    "show_prediction(train_dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing prediction for test example 117\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGdCAYAAACPX3D5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiyUlEQVR4nO3deXBUVeK38W9nX8EAHdb8QojGBUKhYUBlSVgzBHBwWBVk0zGyg4OUUiprSaGIYYABdBRGiAphSkALRVKGGYmCjiAjIJiB4ABxILILIZj0ef/gTQ9NAgQMCZ3zfKqoMrdv33vSfUye3L7d12GMMQIAANbyqeoBAACAqkUMAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDADl0LhxYw0dOtT99aZNm+RwOLRp06YK24fD4dDUqVMrbHs2SkpKUlJSkvvrAwcOyOFwaNmyZVU2JsAbEAO45S1btkwOh8P9LygoSHFxcRo9erSOHDlS1cO7LuvXr+cXfhXjOQBK86vqAQDlNX36dMXExOj8+fPavHmzFi1apPXr12vnzp0KCQmp1LG0b99eBQUFCggIuK77rV+/XgsXLizzl1FBQYH8/PhfsiJFR0eroKBA/v7+7mVXew4AW/GTB16jW7duatmypSTpiSeeUO3atTV37lytXbtWjzzySJn3OXv2rEJDQyt8LD4+PgoKCqrQbVb09m5l586dq5SAKzmSBODqeJkAXqtjx46SpNzcXEnS0KFDFRYWpn379iklJUXh4eEaOHCgJMnlciktLU1NmzZVUFCQ6tatq9TUVJ04ccJjm8YYzZw5U40aNVJISIg6dOigXbt2ldr3lc4Z2Lp1q1JSUhQREaHQ0FA1b95c8+bNc49v4cKFkuTxskeJss4Z2L59u7p166YaNWooLCxMnTp10pYtWzzWKXkZJTs7W08//bScTqdCQ0P18MMPKz8//6qP4Zw5c+RwOPTDDz+Uuu25555TQECA+zHKyclR7969Va9ePQUFBalRo0YaMGCATp06ddV9JCUlqVmzZvr666/Vvn17hYSEaPLkyZKkwsJCTZkyRbfffrsCAwMVFRWlSZMmqbCw0GMbS5cuVceOHRUZGanAwEDdc889WrRo0VX3K5U+Z+BKz4ExRo0bN9bvfve7Uts4f/68atasqdTU1GvuD/BWHBmA19q3b58kqXbt2u5lRUVFSk5OVtu2bTVnzhz3X5+pqalatmyZhg0bprFjxyo3N1cLFizQ9u3blZ2d7T6M/OKLL2rmzJlKSUlRSkqKtm3bpq5du+rChQvXHM/GjRvVo0cP1a9fX+PGjVO9evX03Xff6cMPP9S4ceOUmpqqvLw8bdy4UcuXL7/m9nbt2qV27dqpRo0amjRpkvz9/bVkyRIlJSXp73//u1q3bu2x/pgxYxQREaEpU6bowIEDSktL0+jRo7Vy5cor7qNfv36aNGmSVq1apWeeecbjtlWrVqlr166KiIjQhQsXlJycrMLCQo0ZM0b16tXT4cOH9eGHH+rkyZOqWbPmVb+XY8eOqVu3bhowYIAGDRqkunXryuVy6aGHHtLmzZv15JNP6u6779a3336r1157Td9//73WrFnjvv+iRYvUtGlTPfTQQ/Lz89MHH3ygkSNHyuVyadSoUdd8LEtc6TlwOBwaNGiQXn75ZR0/fly1atVy3/bBBx/o9OnTGjRoULn3A3gdA9zili5daiSZzMxMk5+fbw4ePGjee+89U7t2bRMcHGwOHTpkjDFmyJAhRpJ59tlnPe7/2WefGUkmPT3dY/nHH3/ssfzo0aMmICDAdO/e3bhcLvd6kydPNpLMkCFD3MuysrKMJJOVlWWMMaaoqMjExMSY6Ohoc+LECY/9XLqtUaNGmSv9byfJTJkyxf11r169TEBAgNm3b597WV5engkPDzft27cv9fh07tzZY18TJkwwvr6+5uTJk2Xur8QDDzxgEhISPJZ9+eWXRpJ5++23jTHGbN++3UgyGRkZV91WWRITE40ks3jxYo/ly5cvNz4+Puazzz7zWL548WIjyWRnZ7uXnTt3rtR2k5OTTZMmTUrtKzEx0f11bm6ukWSWLl3qXnal52Dv3r1Gklm0aJHH8oceesg0btzY47EFqhteJoDX6Ny5s5xOp6KiojRgwACFhYXp/fffV8OGDT3WGzFihMfXGRkZqlmzprp06aKffvrJ/S8hIUFhYWHKysqSJGVmZurChQsaM2aMx+H78ePHX3Ns27dvV25ursaPH6/bbrvN47ZLt1VexcXF+uSTT9SrVy81adLEvbx+/fp69NFHtXnzZp0+fdrjPk8++aTHvtq1a6fi4uIyXwK4VP/+/fX111+7j7RI0sqVKxUYGOg+bF7yl/+GDRt07ty56/5+AgMDNWzYMI9lGRkZuvvuu3XXXXd5PC8lL/+UPC+SFBwc7P7vU6dO6aefflJiYqL2799/zZcpyisuLk6tW7dWenq6e9nx48f10UcfaeDAgTf0PALeghiA11i4cKE2btyorKws7d69W/v371dycrLHOn5+fmrUqJHHspycHJ06dUqRkZFyOp0e/37++WcdPXpUkty/NO+44w6P+zudTkVERFx1bCW/SJs1a/arvscS+fn5OnfunO68885St919991yuVw6ePCgx/L/+7//8/i6ZMyXnxdxub59+8rHx8f9coIxRhkZGe5zFSQpJiZGTz/9tP7yl7+oTp06Sk5O1sKFC8v9i7hhw4al3nmRk5OjXbt2lXpO4uLiJMn9vEhSdna2OnfurNDQUN12221yOp3u8w4qKgYkafDgwcrOznbPhYyMDP3yyy967LHHKmwfwK2IcwbgNVq1auV+N8GVBAYGysfHs3FdLpciIyM9/uK7lNPprLAxViVfX98ylxtjrnq/Bg0aqF27dlq1apUmT56sLVu26D//+Y9mz57tsd6rr76qoUOHau3atfrkk080duxYzZo1S1u2bCkVYJe79C/7Ei6XS/Hx8Zo7d26Z94mKipJ0MbQ6deqku+66S3PnzlVUVJQCAgK0fv16vfbaa3K5XFfd9/UYMGCAJkyYoPT0dE2ePFkrVqxQy5Yty4wyoDohBlDtxcbGKjMzU23atCnzl1KJ6OhoSRf/Yr300Hx+fv41/7qOjY2VJO3cuVOdO3e+4nrlPdTsdDoVEhKivXv3lrptz5498vHxcf+yrAj9+/fXyJEjtXfvXq1cuVIhISHq2bNnqfXi4+MVHx+v559/Xp9//rnatGmjxYsXa+bMmde9z9jYWO3YsUOdOnW66uPywQcfqLCwUOvWrfM4+nHpywjX42r7qlWrlrp376709HQNHDhQ2dnZSktLu6H9AN6ElwlQ7fXr10/FxcWaMWNGqduKiop08uRJSRfPSfD399f8+fM9/pouzy+D++67TzExMUpLS3Nvr8Sl2yr5zIPL17mcr6+vunbtqrVr1+rAgQPu5UeOHNE777yjtm3bug/hV4TevXvL19dX7777rjIyMtSjRw+Pz2c4ffq0ioqKPO4THx8vHx+fUm8DLK9+/frp8OHDeuONN0rdVlBQoLNnz0r63xGPSx/HU6dOaenSpTe032s9B4899ph2796tZ555Rr6+vhowYMAN7QfwJhwZQLWXmJio1NRUzZo1S9988426du0qf39/5eTkKCMjQ/PmzVOfPn3kdDo1ceJEzZo1Sz169FBKSoq2b9+ujz76SHXq1LnqPnx8fLRo0SL17NlTLVq00LBhw1S/fn3t2bNHu3bt0oYNGyRJCQkJkqSxY8cqOTn5qr9sZs6cqY0bN6pt27YaOXKk/Pz8tGTJEhUWFurll1+u0McoMjJSHTp00Ny5c3XmzBn179/f4/ZPP/1Uo0ePVt++fRUXF6eioiItX75cvr6+6t279w3t87HHHtOqVav01FNPKSsrS23atFFxcbH27NmjVatWacOGDWrZsqW6du2qgIAA9ezZU6mpqfr555/1xhtvKDIyUj/++ON17/daz0H37t1Vu3Zt93kTkZGRN/T9AV6lSt/LAJRDyVvnvvrqq6uuN2TIEBMaGnrF219//XWTkJBggoODTXh4uImPjzeTJk0yeXl57nWKi4vNtGnTTP369U1wcLBJSkoyO3fuNNHR0Vd9a2GJzZs3my5dupjw8HATGhpqmjdvbubPn+++vaioyIwZM8Y4nU7jcDg83uKmy95aaIwx27ZtM8nJySYsLMyEhISYDh06mM8//7xcj8+Vxnglb7zxhpFkwsPDTUFBgcdt+/fvN8OHDzexsbEmKCjI1KpVy3To0MFkZmZec7uJiYmmadOmZd524cIFM3v2bNO0aVMTGBhoIiIiTEJCgpk2bZo5deqUe71169aZ5s2bm6CgINO4cWMze/Zs89ZbbxlJJjc312Nf13pr4dWegxIjR440ksw777xzze8PqA4cxlzj7CIAsMyECRP05ptv6r///W+lX/cCqAqcMwAAlzh//rxWrFih3r17EwKwBucMAIAufq5BZmamVq9erWPHjmncuHFVPSSg0hADACBp9+7dGjhwoCIjI/WnP/1JLVq0qOohAZWGcwYAALAc5wwAAGA5YgAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAQAALEcMAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsBwxAACA5YgBAAAsRwwAAGA5YgAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAcAyjRs31tChQ91fb9q0SQ6HQ5s2baqyMV3u8jEC5XHgwAE5HA4tW7bMvWzq1KlyOBxVNygvYXUMLFu2TA6HQ//85z+reig6d+6cpk6dekv9QMbNUTLvSv4FBQUpLi5Oo0eP1pEjR6p6eOW2fv16TZ06taqHgVvQ5XPcz89PDRs21NChQ3X48OGqHp5eeuklrVmzpqqHcUuxOgZuJefOndO0adOIAYtMnz5dy5cv14IFC/Tggw9q0aJFeuCBB3Tu3LlKHUf79u1VUFCg9u3bX9f91q9fr2nTpt2kUaE6KJnjixcvVrdu3bRixQolJibq/PnzlTaG559/XgUFBR7LiIHS/Kp6AICtunXrppYtW0qSnnjiCdWuXVtz587V2rVr9cgjj5Ra/+zZswoNDa3wcfj4+CgoKKjCtwtcPsfr1Kmj2bNna926derXr1+ljMHPz09+fvyquxaODFxi6NChCgsL0+HDh9WrVy+FhYXJ6XRq4sSJKi4udq9X8rrUnDlz9Nprryk6OlrBwcFKTEzUzp07PbaZlJSkpKSkMvfVuHFj9/acTqckadq0ae5DaxyCtUvHjh0lSbm5ue65uG/fPqWkpCg8PFwDBw6UJLlcLqWlpalp06YKCgpS3bp1lZqaqhMnTnhszxijmTNnqlGjRgoJCVGHDh20a9euUvu90jkDW7duVUpKiiIiIhQaGqrmzZtr3rx5ki7O34ULF0qSx+HgEhU9RlQP7dq1kyTt27fPvWzPnj3q06ePatWqpaCgILVs2VLr1q3zuN/x48c1ceJExcfHKywsTDVq1FC3bt20Y8eOa+7z8nMGHA6Hzp49q7/+9a/ueTt06FBlZWXJ4XDo/fffL7WNd955Rw6HQ1988cWNfuu3PHLpMsXFxUpOTlbr1q01Z84cZWZm6tVXX1VsbKxGjBjhse7bb7+tM2fOaNSoUTp//rzmzZunjh076ttvv1XdunXLvU+n06lFixZpxIgRevjhh/X73/9ektS8efMK/d5wayv5AVm7dm1JUlFRkZKTk9W2bVvNmTNHISEhkqTU1FQtW7ZMw4YN09ixY5Wbm6sFCxZo+/btys7Olr+/vyTpxRdf1MyZM5WSkqKUlBRt27ZNXbt21YULF645lo0bN6pHjx6qX7++xo0bp3r16um7777Thx9+qHHjxik1NVV5eXnauHGjli9fXur+lTFGeJ8DBw5IkiIiIiRJu3btUps2bdSwYUM9++yzCg0N1apVq9SrVy/97W9/08MPPyxJ2r9/v9asWaO+ffsqJiZGR44c0ZIlS5SYmKjdu3erQYMG5R7D8uXL9cQTT6hVq1Z68sknJUmxsbG6//77FRUVpfT0dPd+S6Snpys2NlYPPPBABTwKtyhjsaVLlxpJ5quvvjLGGDNkyBAjyUyfPt1jvXvvvdckJCS4v87NzTWSTHBwsDl06JB7+datW40kM2HCBPeyxMREk5iYWGrfQ4YMMdHR0e6v8/PzjSQzZcqUivnmcMsqmXeZmZkmPz/fHDx40Lz33numdu3a7jlVMhefffZZj/t+9tlnRpJJT0/3WP7xxx97LD969KgJCAgw3bt3Ny6Xy73e5MmTjSQzZMgQ97KsrCwjyWRlZRljjCkqKjIxMTEmOjranDhxwmM/l25r1KhRpqwfITdjjPAuZc3x1atXG6fTaQIDA83BgweNMcZ06tTJxMfHm/Pnz7vv63K5zIMPPmjuuOMO97Lz58+b4uJij33k5uaawMBAj5/XJT+bly5d6l42ZcqUUvM0NDS0zPn13HPPmcDAQHPy5En3sqNHjxo/P79q/7OZlwnK8NRTT3l83a5dO+3fv7/Uer169VLDhg3dX7dq1UqtW7fW+vXrb/oY4f06d+4sp9OpqKgoDRgwQGFhYXr//fc95tTlR6MyMjJUs2ZNdenSRT/99JP7X0JCgsLCwpSVlSVJyszM1IULFzRmzBiPQ6Tjx4+/5ri2b9+u3NxcjR8/XrfddpvHbeV5i1ZljBHe4dI53qdPH4WGhmrdunVq1KiRjh8/rk8//VT9+vXTmTNn3PPk2LFjSk5OVk5OjvudB4GBgfLxufjrqri4WMeOHVNYWJjuvPNObdu2rcLGO3jwYBUWFmr16tXuZStXrlRRUZEGDRpUYfu5FfEywWWCgoLcr9+XiIiIKPVapyTdcccdpZbFxcVp1apVN218qD4WLlyouLg4+fn5qW7durrzzjvdP/Ckiyc+NWrUyOM+OTk5OnXqlCIjI8vc5tGjRyVJP/zwg6TSc9TpdLoP0V5JycsVzZo1u75vqBLHCO9QMsdPnTqlt956S//4xz8UGBgoSfr3v/8tY4xeeOEFvfDCC2Xe/+jRo2rYsKFcLpfmzZunP//5z8rNzfU4h6vkZbWKcNddd+k3v/mN0tPT9fjjj0u6+BLB/fffr9tvv73C9nMrIgYu4+vrW6HbczgcMsaUWn7pZIadWrVq5T7TuiyX/jVUwuVyKTIyUunp6WXe5/KQrQreMEZUjkvneK9evdS2bVs9+uij2rt3r1wulyRp4sSJSk5OLvP+Jb+AX3rpJb3wwgsaPny4ZsyYoVq1asnHx0fjx493b6eiDB48WOPGjdOhQ4dUWFioLVu2aMGCBRW6j1sRMfAr5OTklFr2/fffu98lIF08qlDWSwwlfxWV4BOyUB6xsbHKzMxUmzZtFBwcfMX1oqOjJV2co02aNHEvz8/PL/Mo1+X7kKSdO3eqc+fOV1zvSnO2MsYI7+Pr66tZs2apQ4cOWrBggYYPHy5J8vf3v+o8k6TVq1erQ4cOevPNNz2Wnzx5UnXq1LnusVzt5+2AAQP09NNP691331VBQYH8/f3Vv3//696Ht+GcgV9hzZo1Hp+m9eWXX2rr1q3q1q2be1lsbKz27Nmj/Px897IdO3YoOzvbY1slZ4qfPHny5g4aXq1fv34qLi7WjBkzSt1WVFTknj+dO3eWv7+/5s+f73FkKi0t7Zr7uO+++xQTE6O0tLRS8/HSbZV85sHl61TGGOGdkpKS1KpVK6WlpalGjRpKSkrSkiVL9OOPP5Za99Kfmb6+vqWOsGZkZNzwpxmGhoZe8WdtnTp13B+QlJ6ert/+9rc3FBzehiMDv8Ltt9+utm3basSIESosLFRaWppq166tSZMmudcZPny45s6dq+TkZD3++OM6evSoFi9erKZNm+r06dPu9YKDg3XPPfdo5cqViouLU61atdSsWbMbft0W1VNiYqJSU1M1a9YsffPNN+ratav8/f2Vk5OjjIwMzZs3T3369HF/PsasWbPUo0cPpaSkaPv27froo4+u+YPNx8dHixYtUs+ePdWiRQsNGzZM9evX1549e7Rr1y5t2LBBkpSQkCBJGjt2rJKTk+Xr66sBAwZUyhjhvZ555hn17dtXy5Yt08KFC9W2bVvFx8frD3/4g5o0aaIjR47oiy++0KFDh9yfI9CjRw9Nnz5dw4YN04MPPqhvv/1W6enpHkeUrkdCQoIyMzM1d+5cNWjQQDExMWrdurX79sGDB6tPnz6SVGbUVktV+VaGqlbWWwtDQ0NLrXf5W1NK3r7yyiuvmFdffdVERUWZwMBA065dO7Njx45S91+xYoVp0qSJCQgIMC1atDAbNmwo9dZCY4z5/PPPTUJCggkICOBthtXY5fOuLFeaiyVef/11k5CQYIKDg014eLiJj483kyZNMnl5ee51iouLzbRp00z9+vVNcHCwSUpKMjt37jTR0dFXfWthic2bN5suXbqY8PBwExoaapo3b27mz5/vvr2oqMiMGTPGOJ1O43A4Sr19qyLHCO9ytTleXFxsYmNjTWxsrCkqKjL79u0zgwcPNvXq1TP+/v6mYcOGpkePHmb16tXu+5w/f9788Y9/dM+TNm3amC+++KLUW7fL+9bCPXv2mPbt25vg4OAy38ZaWFhoIiIiTM2aNU1BQUGFPCa3OocxZZzdhqs6cOCAYmJi9Morr2jixIlVPRwAQAUqKipSgwYN1LNnz1LnKVRXnDMAAMAl1qxZo/z8fA0ePLiqh1JpOGcAAABdvB7Hv/71L82YMUP33nuvEhMTq3pIlYYjAwAASO5rxERGRurtt9+u6uFUKs4ZAADAchwZAADAcsQAAACWK9cJhC6XS3l5eQoPD+djc3HDjDE6c+aMGjRoUOoz928W5i4qAnMX3qq8c7dcMZCXl6eoqKgKGxzsdvDgwVJX47tZmLuoSMxdeKtrzd1yxUB4eLgkqa1S5Cf/ihkZrFOkX7RZ693zqTJ449x9//tvq3oI1+XhuPiqHsJNx9wtH+burae8c7dcMVByiMpP/vJzeMekxC3o/79vpTIPeXrj3K0R7l2n8njL4/qrMHfLhbl7Cyrn3PWuZw4AAFQ4YgAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAQAALEcMAABgOWIAAADLEQMAAFiOGAAAwHLlulARgMqT3KDFDd1vQ943lb5P4FLMXe/FkQEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsBwxAACA5YgBAAAsRwwAAGA5YgAAAMsRAwAAWI4YAADAcly1EKgmuHobvBVzt+pxZAAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAQAALEcMAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsBwxAACA5YgBAAAsRwwAAGA5YgAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAQAALEcMAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByflU9gJtlQ943VT2E65LcoEVVDwG3COYuvBVz13txZAAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAQAALEcMAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDlqu0ljG/00pS/5hKcXA4TFYG5C2/F3PVeHBkAAMByxAAAAJYjBgAAsBwxAACA5YgBAAAsRwwAAGA5YgAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAQAALFdtr1r4a66CVRX75MpbKMHchbdi7novjgwAAGA5YgAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAQAALEcMAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwXLW9hPGNXpqSy2GiqjF34a2Yu96LIwMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsBwxAACA5artVQt/zVWwgKrE3IW3Yu56L44MAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsFy1vYRxVfg1l+9MbtCiwsYBXC/mLrwVc7dicGQAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsBwxAACA5YgBAAAsRwwAAGA5YgAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAQAALEcMAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAlvOr6gHcLMkNWlT6PjfkfVPp+0T1w9yFt2Luei+ODAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsBwxAACA5YgBAAAsRwwAAGA5YgAAAMsRAwAAWI4YAADAcsQAAACWIwYAALBctb2EMZe1hLdi7sJbMXe9F0cGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsBwxAACA5YgBAAAsRwwAAGA5YgAAAMtV26sWepsbvdpXcoMWFToO4Hoxd+GtmLv/w5EBAAAsRwwAAGA5YgAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAQAALEcMAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAluMSxreI6nhJTNiBuQtvxdz9H44MAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJartlctvNGrUW3I+6bS9wlcirkLb8Xc9V4cGQAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsBwxAACA5YgBAAAsRwwAAGC5ansJ419zSUxv2ieX76x+mLvwVsxd78WRAQAALEcMAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJartpcwvtFLTFbF5TCl6nlJTNwY5i68FXPXe3FkAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsBwxAACA5YgBAAAsRwwAAGA5YgAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAQAALEcMAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsBwxAACA5YgBAAAsRwwAAGA5YgAAAMsRAwAAWI4YAADAcsQAAACWIwYAALAcMQAAgOWIAQAALEcMAABgOWIAAADLEQMAAFiOGAAAwHLEAAAAliMGAACwHDEAAIDliAEAACxHDAAAYDliAAAAyxEDAABYjhgAAMByxAAAAJYjBgAAsBwxAACA5YgBAAAsRwwAAGA5v/KsZIyRJBXpF8nc1PFUudNnXFWy3yLzS5XstzIV6eL3WDKfKgNz9+Zj7t4czN2bj7n7Pw5Tjtl96NAhRUVFVczIYL2DBw+qUaNGlbIv5i4qEnMX3upac7dcMeByuZSXl6fw8HA5HI4KHSDsYYzRmTNn1KBBA/n4VM4rVMxdVATmLrxVeeduuWIAAABUX5xACACA5YgBAAAsRwwAAGA5YgAAAMsRAwAAWI4YAADAcsQAAACW+3/KsOGeUfbNugAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a random test prediction vs reality\n",
    "idx = random.randrange(len(test_dataset))\n",
    "print(f\"Showing prediction for test example {idx}\")\n",
    "show_prediction(test_dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mistake_counts(gen, dataloader):\n",
    "    mistake_counts = np.zeros(len(dataloader.dataset), dtype=np.int32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            batch_size = X.size(0)\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_fake = gen(X)\n",
    "\n",
    "            classes_y = torch.argmax(y, dim=1)\n",
    "            classes_y_fake = torch.argmax(y_fake, dim=1)\n",
    "            num_mistakes = (classes_y_fake != classes_y).type(torch.int).sum(dim=[1, 2]).numpy()\n",
    "            batch_start = batch * dataloader.batch_size\n",
    "            batch_end = batch_start + batch_size\n",
    "            mistake_counts[batch_start:batch_end] = num_mistakes\n",
    "    \n",
    "    return mistake_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mistake_counts_train = get_mistake_counts(gen, train_dataloader)\n",
    "mistake_counts_test = get_mistake_counts(gen, test_dataloader)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "fig.suptitle(\"Mistake counts\")\n",
    "axs[0].set_title(\"Train\")\n",
    "axs[0].hist(mistake_counts_train, bins=list(range(mistake_counts_train.max())))\n",
    "axs[1].set_title(\"Test\")\n",
    "axs[1].hist(mistake_counts_test, bins=list(range(mistake_counts_test.max())))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mistake_heatmap(gen, dataloader):\n",
    "    heatmap = np.zeros((22, 10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            batch_size = X.size(0)\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_fake = gen(X)\n",
    "\n",
    "            classes_y = torch.argmax(y, dim=1)\n",
    "            classes_y_fake = torch.argmax(y_fake, dim=1)\n",
    "            mistakes = (classes_y_fake != classes_y).type(torch.float).sum(dim=0).numpy()\n",
    "            heatmap += mistakes\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap_train = get_mistake_heatmap(gen, train_dataloader)\n",
    "heatmap_test = get_mistake_heatmap(gen, test_dataloader)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 4))\n",
    "fig.suptitle(\"Mistake heatmap\")\n",
    "axs[0].set_title(\"Train\")\n",
    "axs[0].imshow(heatmap_train)\n",
    "axs[1].set_title(\"Test\")\n",
    "axs[1].imshow(heatmap_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   1.   0. 103.  92.  84.  58.   0.   0.   0.]\n",
      " [  0.   0.   0.  65.  94.  78.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   1.   1.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   2.   1.   1.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   1.   1.   2.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   1.   1.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   1.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   2.   3.   5.   2.   0.   0.   0.]\n",
      " [  0.   0.   0.   4.   2.   5.   2.   0.   0.   0.]\n",
      " [  0.   0.   0.   1.   3.   2.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   4.   2.   3.   1.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   5.   3.   1.   0.   0.   0.]\n",
      " [  0.   0.   0.   3.   5.   4.   6.   0.   0.   0.]\n",
      " [  0.   0.   0.   6.   4.   3.   2.   0.   0.   0.]\n",
      " [  0.   0.   0.   2.   3.   2.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   1.   4.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   1.   2.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   3.   1.   3.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   1.   4.   3.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   1.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   9.  15.   7.   3.   0.   0.   0.]\n",
      " [  0.   2.   3.   5.   0.  26.   2.   0.   0.   0.]]\n",
      "774.0\n"
     ]
    }
   ],
   "source": [
    "print(heatmap_train)\n",
    "print(heatmap_train.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_failed_example_by_cell(dataloader, cell):\n",
    "    i, j = cell\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            batch_size = X.size(0)\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_fake = gen(X)\n",
    "\n",
    "            classes_X = torch.argmax(X, dim=1)\n",
    "            classes_y = torch.argmax(y, dim=1)\n",
    "            classes_y_fake = torch.argmax(y_fake, dim=1)\n",
    "            mistakes = (classes_y_fake != classes_y)\n",
    "            failed_in_cell = mistakes[:, i, j]\n",
    "            for batch_idx, failed in enumerate(failed_in_cell):\n",
    "                if failed:\n",
    "                    dataset_idx = batch * dataloader.batch_size + batch_idx\n",
    "                    return dataset_idx, (classes_X[batch_idx], classes_y_fake[batch_idx], classes_y[batch_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing failed prediction for example 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19b141bb5e0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell = (1, 5)\n",
    "\n",
    "idx, (x, y_fake, y) = find_failed_example_by_cell(train_dataloader, cell)\n",
    "print(f\"Showing failed prediction for example {idx}\")\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_title(\"Input | Predicted | Real\")\n",
    "ax.imshow(render_prediction(x, y_fake, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen.state_dict(), \"tetris_emulator.pth\")\n",
    "torch.save(disc.state_dict(), \"tetris_discriminator.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
