{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 024\n",
    "\n",
    "In this experiment, we'll try passing Gaussian random noise to the generator instead of uniform noise, to see if it improves the performance. We'll continue using a learning rate of 1e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordingDataset(Dataset):\n",
    "    def __init__(self, path: str):\n",
    "        self.path = path\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError()\n",
    "        with os.scandir(self.path) as it:\n",
    "            entry: os.DirEntry = next(iter(it))\n",
    "            _, self.ext = os.path.splitext(entry.name)\n",
    "            self.highest_index = max((int(Path(file.path).stem) for file in it), default=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.highest_index + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = os.path.join(self.path, f\"{idx}{self.ext}\")\n",
    "        if not os.path.exists(file):\n",
    "            raise IndexError()\n",
    "        boards = np.load(file)\n",
    "\n",
    "        def transform(board):\n",
    "            board = torch.tensor(board, dtype=torch.long)\n",
    "            board = F.one_hot(board, 2) # One-hot encode the cell types\n",
    "            board = board.type(torch.float) # Convert to floating-point\n",
    "            board = board.permute((2, 0, 1)) # Move channels/classes to dimension 0\n",
    "            return board\n",
    "\n",
    "        x = transform(boards[-2]) # Ignore all boards except the last two\n",
    "        y = transform(boards[-1])\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 22, 10]) torch.float32\n",
      "torch.Size([4, 2, 22, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "train_dataset = RecordingDataset(os.path.join(\"data\", \"tetris_emulator\", \"train\"))\n",
    "test_dataset = RecordingDataset(os.path.join(\"data\", \"tetris_emulator\", \"test\"))\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "print(x.shape, x.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrisModel(nn.Module):\n",
    "    \"\"\"Predicts the next state of the cells.\n",
    "\n",
    "    Inputs:\n",
    "        x: Tensor of float32 of shape (batch_size, channels, height, width). channels = 2 is the one-hot encoding of cell types, with\n",
    "           0 for empty cells and 1 for filled cells. height = 22 and width = 10 are the dimensions of the game board. The entries\n",
    "           should be 0 for empty cells and 1 for filled cells.\n",
    "        z: Tensor of float32 of shape (batch_size, 4). The entries should be random numbers sampled from a uniform distribution.\n",
    "    \n",
    "    Returns: Tensor of float32 of shape (batch_size, height, width), logits for the new cells. Probabilities close to 0 (negative logits)\n",
    "             correspond to empty cells, and probabilities close to 1 (positive logits) correspond to filled cells.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.glob = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 16, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(160, 10)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(26, 16, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 2, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        z = z[:, :, None, None] # Expand dims to match x\n",
    "        z = z.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, z), dim=1)\n",
    "\n",
    "        x = self.loc(x)\n",
    "\n",
    "        x_glob = self.glob(x)\n",
    "        x_glob = x_glob[:, :, None, None] # Expand dims\n",
    "        x_glob = x_glob.repeat(1, 1, height, width) # Upscale to image size\n",
    "        x = torch.cat((x, x_glob), dim=1)\n",
    "\n",
    "        y = self.head(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrisDiscriminator(nn.Module):\n",
    "    \"\"\"A discriminator for the cell state predictions. Assesses the output of the generator.\n",
    "\n",
    "    Inputs:\n",
    "        x: Tensor of float32 of shape (batch_size, channels, height, width). channels = 2 is the one-hot encoding of cell types, with\n",
    "           0 for empty cells and 1 for filled cells. height = 22 and width = 10 are the dimensions of the game board. The entries\n",
    "           should be 0 for empty cells and 1 for filled cells.\n",
    "        y: Tensor of float32 of shape (batch_size, channels, height, width), as with x. This should be either the output of the\n",
    "           generator (with exp applied) or the one-hot encoding of the ground truth of the next cell states.\n",
    "    \n",
    "    Returns: Tensor of float32 of shape (batch_size, 1), decisions on whether the data are real or fake. Probabilities close to 0 (negative logits)\n",
    "             correspond to fake data, and probabilities close to 1 (positive logits) correspond to real data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.body = nn.Sequential(\n",
    "                nn.Conv2d(4, 16, 3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.Conv2d(16, 16, 3),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(16, 16, 3),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(112, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 1),\n",
    "                nn.Flatten(start_dim=0)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        logits = self.body(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(noise_type, size):\n",
    "    if noise_type == \"uniform\":\n",
    "        return torch.rand(size, device=device)\n",
    "    if noise_type == \"normal\":\n",
    "        return torch.randn(size, device=device)\n",
    "    raise ValueError(noise_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of generator parameters: 17996\n",
      "Number of discriminator parameters: 7057\n",
      "Predicted label for real data: 0.5604432821273804\n",
      "Predicted label for fake data: 0.5596185922622681\n"
     ]
    }
   ],
   "source": [
    "gen = TetrisModel().to(device)\n",
    "disc = TetrisDiscriminator().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dataloader))\n",
    "    z = get_noise(\"uniform\", (batch_size, 4))\n",
    "    y_gen = gen(X, z)\n",
    "    pred_on_real = F.sigmoid(disc(X, y)[0])\n",
    "    pred_on_fake = F.sigmoid(disc(X, y_gen)[0])\n",
    "    print(f\"Number of generator parameters: {count_parameters(gen)}\")\n",
    "    print(f\"Number of discriminator parameters: {count_parameters(disc)}\")\n",
    "    print(f\"Predicted label for real data: {pred_on_real}\")\n",
    "    print(f\"Predicted label for fake data: {pred_on_fake}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def find_interesting_examples(dataset, num=3):\n",
    "    num_spawns = num\n",
    "    \n",
    "    def inner():\n",
    "        num_spawns_left = num_spawns\n",
    "\n",
    "        for x, y in dataset:\n",
    "            # Check for block spawn\n",
    "            if (x.argmax(0)[0] == 0).all() & (y.argmax(0)[0] == 1).any():\n",
    "                if num_spawns_left > 0:\n",
    "                    num_spawns_left -= 1\n",
    "                    yield x, y\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "    return list(itertools.islice(inner(), num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prediction(x, pred, y):\n",
    "    \"\"\"Renders an example and prediction into a single-image array.\n",
    "    \n",
    "    Inputs:\n",
    "        x: Tensor of shape (height, width), the model input.\n",
    "        pred: Tensor of shape (height, width), the model prediction.\n",
    "        y: Tensor of shape (height, width), the target.\n",
    "    \"\"\"\n",
    "    assert len(x.shape) == 2, f\"Expected tensors of shape (width, height) but got {x.shape}\"\n",
    "    assert x.shape == pred.shape, f\"Shapes do not match: {x.shape} != {pred.shape}\"\n",
    "    assert x.shape == y.shape, f\"Shapes do not match: {x.shape} != {y.shape}\"\n",
    "    height, width = x.shape\n",
    "    with torch.no_grad():\n",
    "        separator = torch.ones(height, 1, dtype=x.dtype)\n",
    "        return torch.cat((x, separator, pred, separator, y), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [\n",
    "    torch.tensor(\n",
    "        [[0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        dtype=torch.int), # I\n",
    "\n",
    "    torch.tensor(\n",
    "        [[0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        dtype=torch.int), # O\n",
    "\n",
    "    torch.tensor(\n",
    "        [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        dtype=torch.int), # J\n",
    "\n",
    "    torch.tensor(\n",
    "        [[0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        dtype=torch.int), # T\n",
    "\n",
    "    torch.tensor(\n",
    "        [[0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        dtype=torch.int), # S\n",
    "\n",
    "    torch.tensor(\n",
    "        [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        dtype=torch.int), # L\n",
    "\n",
    "    torch.tensor(\n",
    "        [[0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        dtype=torch.int) # Z\n",
    "]\n",
    "\n",
    "def get_valid_block_spawns(classes_X, classes_y_fake):\n",
    "    \"\"\"Determines whether predicted block spawns have a valid shape.\n",
    "    \n",
    "    Inputs:\n",
    "        classes_X: Tensor of int32 of shape (batch_size, height, width), the first time step (with argmax applied on cell types).\n",
    "        classes_y_fake: Tensor of int32 of shape (batch_size, height, width), the model's prediction (with argmax applied on cell types).\n",
    "\n",
    "    Returns: Tensor of bool of shape (batch_size,), whether the items are predicted block spawns AND valid.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size = classes_X.size(0)\n",
    "        ret = torch.full((batch_size,), False)\n",
    "\n",
    "        # Take difference to see which cells are full but weren't before.\n",
    "        diff = classes_y_fake - classes_X\n",
    "\n",
    "        # It's only a valid block spawn if the change in the first 3 rows matches\n",
    "        # one of the valid configurations.\n",
    "        for block in blocks:\n",
    "            ret |= (diff[:, :3, :] == block).all(-1).all(-1)\n",
    "        \n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "\n",
    "\n",
    "def train_loop(dataloader, gen, disc, loss_fn, optimizer_gen, optimizer_disc, noise_type):\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        ##################################################################\n",
    "        # (1) Update discriminator: minimize -log(D(x)) - log(1 - D(G(z)))\n",
    "        ##################################################################\n",
    "        disc.zero_grad()\n",
    "\n",
    "        ## Train with all-real batch\n",
    "        # Format batch\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        batch_size = X.size(0)\n",
    "        real_labels = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through discriminator\n",
    "        output = torch.flatten(disc(X, y))\n",
    "        # Calculate loss on all-real batch\n",
    "        err_disc_real = loss_fn(output, real_labels)\n",
    "        # Calculate gradients for discriminator in backward pass\n",
    "        err_disc_real.backward()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        z = get_noise(noise_type, (batch_size, 4))\n",
    "        # Generate fake image batch with generator\n",
    "        y_fake = gen(X, z)\n",
    "        fake_labels = torch.full((batch_size,), fake_label, dtype=torch.float, device=device)\n",
    "        # Classify all fake batch with discriminator\n",
    "        output = torch.flatten(disc(X, y_fake.detach()))\n",
    "        # Calculate discriminator's loss on the all-fake batch\n",
    "        err_disc_fake = loss_fn(output, fake_labels)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        err_disc_fake.backward()\n",
    "\n",
    "        ## Update discriminator weights\n",
    "        # Compute error of discriminator as sum over the fake and the real batches\n",
    "        err_disc = err_disc_real + err_disc_fake\n",
    "        # Update discriminator\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        ##############################################\n",
    "        # (2) Update generator: minimize -log(D(G(z)))\n",
    "        ##############################################\n",
    "        gen.zero_grad()\n",
    "        # Since we just updated the discriminator, perform another forward pass of the all-fake batch through it\n",
    "        output = torch.flatten(disc(X, y_fake))\n",
    "        # Calculate the generator's loss based on this output\n",
    "        # We use real labels because the generator wants to fool the discriminator\n",
    "        err_gen = loss_fn(output, real_labels)\n",
    "        # Calculate gradients for generator\n",
    "        err_gen.backward()\n",
    "        # Update generator\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if batch % 20 == 0:\n",
    "            current = batch * dataloader.batch_size + batch_size\n",
    "            print(f\"[{current}/{size}] D loss: {err_disc.item():.4f}, G loss: {err_gen.item():.4f}\")\n",
    "\n",
    "\n",
    "def test_loop(split_name, dataloader, gen, disc, loss_fn, tb_writer, epoch, examples, noise_type):\n",
    "    gen.eval()\n",
    "    disc.eval()\n",
    "\n",
    "    loss_disc = 0.0\n",
    "    loss_gen = 0.0\n",
    "    disc_accuracy = 0.0\n",
    "    cell_accuracy = 0.0\n",
    "    board_accuracy = 0.0\n",
    "    spawn_recall = 0.0\n",
    "    num_spawns = 0.0\n",
    "    spawn_validity = 0.0\n",
    "    num_predicted_spawns = 0.0\n",
    "    spawn_precision = 0.0\n",
    "    scores_real = np.zeros(len(dataloader.dataset))\n",
    "    scores_fake = np.zeros(len(dataloader.dataset))\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    with torch.no_grad():        \n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            batch_size = X.size(0)\n",
    "            real_labels = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "            fake_labels = torch.full((batch_size,), fake_label, dtype=torch.float, device=device)\n",
    "\n",
    "            output_real = disc(X, y)\n",
    "            loss_disc += loss_fn(output_real, real_labels).item()\n",
    "\n",
    "            z = get_noise(noise_type, (batch_size, 4))\n",
    "            y_fake = gen(X, z)\n",
    "            output_fake = disc(X, y_fake)\n",
    "            \n",
    "            loss_disc += loss_fn(output_fake, fake_labels).item()\n",
    "            loss_gen += loss_fn(output_fake, real_labels).item()\n",
    "\n",
    "            pred_real = (output_real > 0.0)\n",
    "            pred_fake = (output_fake > 0.0)\n",
    "            disc_accuracy += pred_real.type(torch.float).mean().item()\n",
    "            disc_accuracy += (~pred_fake).type(torch.float).mean().item()\n",
    "\n",
    "            classes_X = torch.argmax(X, dim=1)\n",
    "            classes_y = torch.argmax(y, dim=1)\n",
    "            classes_y_fake = torch.argmax(y_fake, dim=1)\n",
    "            cell_accuracy += (classes_y_fake == classes_y).type(torch.float).mean().item()\n",
    "            board_accuracy += (classes_y_fake == classes_y).all(-1).all(-1).type(torch.float).mean().item()\n",
    "\n",
    "            actual_spawns = (classes_X[:, 0, :] == 0).all(-1) & (classes_y[:, 0, :] == 1).any(-1)\n",
    "            predicted_spawns = (classes_X[:, 0, :] == 0).all(-1) & (classes_y_fake[:, 0, :] == 1).any(-1)\n",
    "            num_true_positives = (actual_spawns & predicted_spawns).type(torch.float).sum().item()\n",
    "            spawn_recall += num_true_positives\n",
    "            spawn_precision += num_true_positives\n",
    "            num_spawns += actual_spawns.type(torch.float).sum().item()\n",
    "            valid_spawns = get_valid_block_spawns(classes_X, classes_y_fake)\n",
    "            spawn_validity += valid_spawns.type(torch.float).sum().item()\n",
    "            num_predicted_spawns += predicted_spawns.type(torch.float).sum().item()\n",
    "\n",
    "            start_index = dataloader.batch_size * batch\n",
    "            end_index = start_index + batch_size\n",
    "            scores_real[start_index:end_index] = torch.sigmoid(output_real).numpy()\n",
    "            scores_fake[start_index:end_index] = torch.sigmoid(output_fake).numpy()\n",
    "\n",
    "    loss_disc /= num_batches\n",
    "    loss_gen /= num_batches\n",
    "    cell_accuracy /= num_batches\n",
    "    board_accuracy /= num_batches\n",
    "    spawn_recall /= num_spawns\n",
    "    spawn_precision = np.nan if (num_predicted_spawns == 0.0) else spawn_precision / num_predicted_spawns\n",
    "    disc_accuracy /= (2.0 * num_batches)\n",
    "    spawn_validity = np.nan if (num_predicted_spawns == 0.0) else spawn_validity / num_predicted_spawns\n",
    "\n",
    "    print(f\"{split_name} error: \\n D loss: {loss_disc:>8f}, G loss: {loss_gen:>8f}, D accuracy: {(100*disc_accuracy):>0.1f}%, cell accuracy: {(100*cell_accuracy):>0.1f}%, board accuracy: {(100*board_accuracy):>0.1f}% \\n\")\n",
    "\n",
    "    tb_writer.add_scalar(f\"Discriminator loss/{split_name}\", loss_disc, epoch)\n",
    "    tb_writer.add_scalar(f\"Loss/{split_name}\", loss_gen, epoch)\n",
    "    tb_writer.add_scalar(f\"Discriminator accuracy/{split_name}\", disc_accuracy, epoch)\n",
    "    tb_writer.add_scalar(f\"Cell accuracy/{split_name}\", cell_accuracy, epoch)\n",
    "    tb_writer.add_scalar(f\"Board accuracy/{split_name}\", board_accuracy, epoch)\n",
    "    tb_writer.add_scalar(f\"Spawn recall/{split_name}\", spawn_recall, epoch)\n",
    "    tb_writer.add_scalar(f\"Spawn precision/{split_name}\", spawn_precision, epoch)\n",
    "    tb_writer.add_scalar(f\"Spawn validity/{split_name}\", spawn_validity, epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(examples):\n",
    "            X, y = X.unsqueeze(0), y.unsqueeze(0)\n",
    "            z = get_noise(noise_type, (1, 4))\n",
    "            y_fake = gen(X, z)\n",
    "            X, y, y_fake = X.squeeze(0), y.squeeze(0), y_fake.squeeze(0)\n",
    "            X, y, y_fake = X.argmax(0), y.argmax(0), y_fake.argmax(0)\n",
    "            img = render_prediction(X, y_fake, y)\n",
    "            tb_writer.add_image(f\"Predictions/{split_name}/{i}\", img, epoch, dataformats=\"HW\")\n",
    "    \n",
    "    tb_writer.add_histogram(f\"Discriminator scores/{split_name}/real\", scores_real, epoch)\n",
    "    tb_writer.add_histogram(f\"Discriminator scores/{split_name}/fake\", scores_fake, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(run_name=\"\", learning_rate=1e-4, epochs=100, noise_type=\"uniform\"):\n",
    "    gen = TetrisModel().to(device)\n",
    "    disc = TetrisDiscriminator().to(device)\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer_gen = torch.optim.Adam(gen.parameters(), lr=learning_rate)\n",
    "    optimizer_disc = torch.optim.Adam(disc.parameters(), lr=learning_rate)\n",
    "\n",
    "    log_dir = os.path.join(\"runs\", \"experiment_024\")\n",
    "    log_subdir = os.path.join(log_dir, run_name + \"_\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    tb_writer = SummaryWriter(log_subdir)\n",
    "\n",
    "    train_examples = find_interesting_examples(train_dataset)\n",
    "    test_examples = find_interesting_examples(test_dataset)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, gen, disc, loss_fn, optimizer_gen, optimizer_disc, noise_type)\n",
    "        test_loop(\"train\", train_dataloader, gen, disc, loss_fn, tb_writer, epoch, train_examples, noise_type)\n",
    "        test_loop(\"test\", test_dataloader, gen, disc, loss_fn, tb_writer, epoch, test_examples, noise_type)\n",
    "        gen_zero_grads = 0\n",
    "        for name, weight in gen.named_parameters():\n",
    "            tb_writer.add_histogram(f\"Weights/{name}\", weight, epoch)\n",
    "            if weight.grad is not None:\n",
    "                tb_writer.add_histogram(f\"Gradients/{name}\", weight.grad, epoch)\n",
    "                gen_zero_grads += weight.grad.numel() - weight.grad.count_nonzero().item()\n",
    "        tb_writer.add_scalar(f\"Zero gradients\", gen_zero_grads, epoch)\n",
    "        disc_zero_grads = 0\n",
    "        for name, weight in disc.named_parameters():\n",
    "            tb_writer.add_histogram(f\"Discriminator weights/{name}\", weight, epoch)\n",
    "            tb_writer.add_histogram(f\"Discriminator gradients/{name}\", weight.grad, epoch)\n",
    "            disc_zero_grads += weight.grad.numel() - weight.grad.count_nonzero().item()\n",
    "        tb_writer.add_scalar(f\"Discriminator zero gradients\", disc_zero_grads, epoch)\n",
    "\n",
    "    tb_writer.close()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3843, G loss: 0.7142\n",
      "[84/1762] D loss: 1.3688, G loss: 0.7068\n",
      "[164/1762] D loss: 1.3564, G loss: 0.7065\n",
      "[244/1762] D loss: 1.3272, G loss: 0.7099\n",
      "[324/1762] D loss: 1.2895, G loss: 0.7141\n",
      "[404/1762] D loss: 1.2409, G loss: 0.7221\n",
      "[484/1762] D loss: 1.1726, G loss: 0.7560\n",
      "[564/1762] D loss: 1.0589, G loss: 0.7927\n",
      "[644/1762] D loss: 0.9504, G loss: 0.9203\n",
      "[724/1762] D loss: 0.7842, G loss: 1.1473\n",
      "[804/1762] D loss: 0.6508, G loss: 1.3802\n",
      "[884/1762] D loss: 0.5398, G loss: 1.7840\n",
      "[964/1762] D loss: 0.4920, G loss: 2.3308\n",
      "[1044/1762] D loss: 0.5200, G loss: 2.4436\n",
      "[1124/1762] D loss: 0.3301, G loss: 2.3382\n",
      "[1204/1762] D loss: 0.3126, G loss: 2.6338\n",
      "[1284/1762] D loss: 0.2896, G loss: 2.9629\n",
      "[1364/1762] D loss: 0.2483, G loss: 3.1701\n",
      "[1444/1762] D loss: 0.2765, G loss: 3.5679\n",
      "[1524/1762] D loss: 0.2424, G loss: 4.4108\n",
      "[1604/1762] D loss: 0.4031, G loss: 4.1916\n",
      "[1684/1762] D loss: 0.1923, G loss: 4.0148\n",
      "[1762/1762] D loss: 0.1658, G loss: 5.7760\n",
      "train error: \n",
      " D loss: 0.285925, G loss: 4.513531, D accuracy: 99.1%, cell accuracy: 67.0%, board accuracy: 0.0% \n",
      "\n",
      "test error: \n",
      " D loss: 0.263440, G loss: 4.766742, D accuracy: 99.2%, cell accuracy: 65.3%, board accuracy: 0.0% \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "[4/1762] D loss: 0.2397, G loss: 4.1347\n",
      "[84/1762] D loss: 0.2285, G loss: 4.7467\n",
      "[164/1762] D loss: 0.3359, G loss: 5.1178\n",
      "[244/1762] D loss: 0.1749, G loss: 4.7694\n",
      "[324/1762] D loss: 0.2462, G loss: 5.5471\n",
      "[404/1762] D loss: 0.2875, G loss: 4.6785\n",
      "[484/1762] D loss: 0.2694, G loss: 5.3735\n",
      "[564/1762] D loss: 0.2572, G loss: 6.3118\n",
      "[644/1762] D loss: 0.2559, G loss: 5.3135\n",
      "[724/1762] D loss: 0.2672, G loss: 5.4209\n",
      "[804/1762] D loss: 0.2962, G loss: 5.8227\n",
      "[884/1762] D loss: 0.2112, G loss: 5.3675\n",
      "[964/1762] D loss: 0.2578, G loss: 5.2534\n",
      "[1044/1762] D loss: 0.1024, G loss: 4.7109\n",
      "[1124/1762] D loss: 0.1981, G loss: 6.2348\n",
      "[1204/1762] D loss: 0.2964, G loss: 5.4585\n",
      "[1284/1762] D loss: 0.3297, G loss: 4.3290\n",
      "[1364/1762] D loss: 0.1046, G loss: 3.2293\n",
      "[1444/1762] D loss: 0.1971, G loss: 2.7885\n",
      "[1524/1762] D loss: 0.0940, G loss: 3.5920\n",
      "[1604/1762] D loss: 0.0904, G loss: 3.3544\n",
      "[1684/1762] D loss: 0.1836, G loss: 2.4404\n",
      "[1762/1762] D loss: 0.1252, G loss: 3.9508\n",
      "train error: \n",
      " D loss: 0.176848, G loss: 3.281995, D accuracy: 100.0%, cell accuracy: 92.5%, board accuracy: 0.0% \n",
      "\n",
      "test error: \n",
      " D loss: 0.166601, G loss: 3.417985, D accuracy: 100.0%, cell accuracy: 92.3%, board accuracy: 0.0% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "[4/1762] D loss: 0.1412, G loss: 3.2727\n",
      "[84/1762] D loss: 0.2014, G loss: 2.5848\n",
      "[164/1762] D loss: 0.1852, G loss: 2.4640\n",
      "[244/1762] D loss: 0.1630, G loss: 2.4694\n",
      "[324/1762] D loss: 0.1490, G loss: 2.9347\n",
      "[404/1762] D loss: 0.1914, G loss: 1.8608\n",
      "[484/1762] D loss: 0.2990, G loss: 1.6351\n",
      "[564/1762] D loss: 0.3506, G loss: 1.8495\n",
      "[644/1762] D loss: 0.2323, G loss: 2.1123\n",
      "[724/1762] D loss: 0.3775, G loss: 3.4893\n",
      "[804/1762] D loss: 0.3450, G loss: 2.7661\n",
      "[884/1762] D loss: 0.3467, G loss: 1.9059\n",
      "[964/1762] D loss: 0.3947, G loss: 1.4444\n",
      "[1044/1762] D loss: 0.3311, G loss: 1.5878\n",
      "[1124/1762] D loss: 0.3734, G loss: 2.1133\n",
      "[1204/1762] D loss: 0.4224, G loss: 1.4904\n",
      "[1284/1762] D loss: 0.5688, G loss: 1.3237\n",
      "[1364/1762] D loss: 0.3193, G loss: 1.9469\n",
      "[1444/1762] D loss: 0.3490, G loss: 1.9263\n",
      "[1524/1762] D loss: 0.5113, G loss: 1.7366\n",
      "[1604/1762] D loss: 0.4682, G loss: 1.5186\n",
      "[1684/1762] D loss: 0.4141, G loss: 2.1038\n",
      "[1762/1762] D loss: 0.2299, G loss: 2.6298\n",
      "train error: \n",
      " D loss: 0.526024, G loss: 1.391198, D accuracy: 94.2%, cell accuracy: 96.6%, board accuracy: 0.0% \n",
      "\n",
      "test error: \n",
      " D loss: 0.506919, G loss: 1.466799, D accuracy: 95.8%, cell accuracy: 96.7%, board accuracy: 0.0% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "[4/1762] D loss: 0.5223, G loss: 1.2122\n",
      "[84/1762] D loss: 0.6406, G loss: 1.6681\n",
      "[164/1762] D loss: 0.9282, G loss: 2.5573\n",
      "[244/1762] D loss: 0.5895, G loss: 1.4048\n",
      "[324/1762] D loss: 0.5240, G loss: 1.2340\n",
      "[404/1762] D loss: 0.5570, G loss: 1.3745\n",
      "[484/1762] D loss: 0.5557, G loss: 1.0009\n",
      "[564/1762] D loss: 0.4632, G loss: 1.8930\n",
      "[644/1762] D loss: 0.5910, G loss: 1.9114\n",
      "[724/1762] D loss: 0.6457, G loss: 1.5571\n",
      "[804/1762] D loss: 0.5773, G loss: 1.3074\n",
      "[884/1762] D loss: 0.9035, G loss: 1.1485\n",
      "[964/1762] D loss: 0.5454, G loss: 2.6394\n",
      "[1044/1762] D loss: 0.5851, G loss: 2.1537\n",
      "[1124/1762] D loss: 1.0270, G loss: 2.7439\n",
      "[1204/1762] D loss: 0.6181, G loss: 1.9628\n",
      "[1284/1762] D loss: 0.5510, G loss: 1.8427\n",
      "[1364/1762] D loss: 0.4136, G loss: 2.3926\n",
      "[1444/1762] D loss: 0.9769, G loss: 1.0571\n",
      "[1524/1762] D loss: 0.7738, G loss: 0.8595\n",
      "[1604/1762] D loss: 0.7254, G loss: 1.4729\n",
      "[1684/1762] D loss: 0.6456, G loss: 1.5074\n",
      "[1762/1762] D loss: 0.6901, G loss: 1.0216\n",
      "train error: \n",
      " D loss: 0.673256, G loss: 1.249394, D accuracy: 91.0%, cell accuracy: 97.2%, board accuracy: 0.0% \n",
      "\n",
      "test error: \n",
      " D loss: 0.675939, G loss: 1.251670, D accuracy: 91.0%, cell accuracy: 97.2%, board accuracy: 0.0% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "[4/1762] D loss: 0.4595, G loss: 1.7736\n",
      "[84/1762] D loss: 0.5419, G loss: 1.5821\n",
      "[164/1762] D loss: 0.6340, G loss: 2.2699\n",
      "[244/1762] D loss: 0.8199, G loss: 2.9107\n",
      "[324/1762] D loss: 0.6430, G loss: 1.9086\n",
      "[404/1762] D loss: 0.4591, G loss: 1.6323\n",
      "[484/1762] D loss: 0.3731, G loss: 1.5424\n",
      "[564/1762] D loss: 0.5693, G loss: 1.5329\n",
      "[644/1762] D loss: 0.6687, G loss: 1.5997\n",
      "[724/1762] D loss: 0.8604, G loss: 1.0186\n",
      "[804/1762] D loss: 0.7654, G loss: 1.0073\n",
      "[884/1762] D loss: 0.6412, G loss: 1.0841\n",
      "[964/1762] D loss: 0.4677, G loss: 2.1560\n",
      "[1044/1762] D loss: 0.5824, G loss: 1.1769\n",
      "[1124/1762] D loss: 0.5384, G loss: 2.2024\n",
      "[1204/1762] D loss: 0.8366, G loss: 2.1562\n",
      "[1284/1762] D loss: 0.5347, G loss: 1.5760\n",
      "[1364/1762] D loss: 0.6142, G loss: 1.5183\n",
      "[1444/1762] D loss: 0.8025, G loss: 1.3049\n",
      "[1524/1762] D loss: 0.6688, G loss: 1.7019\n",
      "[1604/1762] D loss: 0.5685, G loss: 1.4531\n",
      "[1684/1762] D loss: 0.9631, G loss: 1.0424\n",
      "[1762/1762] D loss: 0.3556, G loss: 1.8291\n",
      "train error: \n",
      " D loss: 0.691053, G loss: 1.165523, D accuracy: 85.8%, cell accuracy: 96.6%, board accuracy: 0.0% \n",
      "\n",
      "test error: \n",
      " D loss: 0.706416, G loss: 1.168077, D accuracy: 84.9%, cell accuracy: 96.6%, board accuracy: 0.0% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "[4/1762] D loss: 0.8384, G loss: 1.4613\n",
      "[84/1762] D loss: 0.6402, G loss: 1.4872\n",
      "[164/1762] D loss: 0.7387, G loss: 1.8276\n",
      "[244/1762] D loss: 0.7793, G loss: 2.4362\n",
      "[324/1762] D loss: 0.5531, G loss: 1.6105\n",
      "[404/1762] D loss: 0.6015, G loss: 1.7763\n",
      "[484/1762] D loss: 0.4430, G loss: 1.5337\n",
      "[564/1762] D loss: 0.8703, G loss: 0.7389\n",
      "[644/1762] D loss: 0.8138, G loss: 1.6865\n",
      "[724/1762] D loss: 0.9670, G loss: 0.9306\n",
      "[804/1762] D loss: 0.8564, G loss: 1.7423\n",
      "[884/1762] D loss: 1.3391, G loss: 2.2454\n",
      "[964/1762] D loss: 0.9421, G loss: 1.8625\n",
      "[1044/1762] D loss: 1.0890, G loss: 1.4787\n",
      "[1124/1762] D loss: 1.0069, G loss: 1.3104\n",
      "[1204/1762] D loss: 0.5492, G loss: 2.0935\n",
      "[1284/1762] D loss: 1.1620, G loss: 1.6235\n",
      "[1364/1762] D loss: 1.5986, G loss: 0.8743\n",
      "[1444/1762] D loss: 0.9615, G loss: 0.6144\n",
      "[1524/1762] D loss: 0.8840, G loss: 0.6483\n",
      "[1604/1762] D loss: 0.8121, G loss: 0.6110\n",
      "[1684/1762] D loss: 1.0636, G loss: 1.2422\n",
      "[1762/1762] D loss: 1.0487, G loss: 1.1535\n",
      "train error: \n",
      " D loss: 0.992227, G loss: 1.188964, D accuracy: 77.5%, cell accuracy: 97.7%, board accuracy: 8.2% \n",
      "\n",
      "test error: \n",
      " D loss: 0.991781, G loss: 1.187512, D accuracy: 77.4%, cell accuracy: 97.6%, board accuracy: 6.8% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.1212, G loss: 1.0316\n",
      "[84/1762] D loss: 1.0734, G loss: 1.1703\n",
      "[164/1762] D loss: 0.7759, G loss: 0.9047\n",
      "[244/1762] D loss: 0.8717, G loss: 1.0923\n",
      "[324/1762] D loss: 0.9767, G loss: 1.2055\n",
      "[404/1762] D loss: 0.9405, G loss: 1.4118\n",
      "[484/1762] D loss: 1.0205, G loss: 1.0286\n",
      "[564/1762] D loss: 1.0294, G loss: 1.2180\n",
      "[644/1762] D loss: 1.0194, G loss: 1.6311\n",
      "[724/1762] D loss: 1.2348, G loss: 2.1462\n",
      "[804/1762] D loss: 1.0518, G loss: 2.4033\n",
      "[884/1762] D loss: 1.2081, G loss: 1.1750\n",
      "[964/1762] D loss: 1.1570, G loss: 1.2207\n",
      "[1044/1762] D loss: 0.9528, G loss: 1.5244\n",
      "[1124/1762] D loss: 0.9579, G loss: 0.9162\n",
      "[1204/1762] D loss: 0.7108, G loss: 1.0739\n",
      "[1284/1762] D loss: 1.0484, G loss: 0.8229\n",
      "[1364/1762] D loss: 0.8225, G loss: 1.2925\n",
      "[1444/1762] D loss: 1.6986, G loss: 0.4139\n",
      "[1524/1762] D loss: 1.0253, G loss: 0.7789\n",
      "[1604/1762] D loss: 0.9037, G loss: 1.4506\n",
      "[1684/1762] D loss: 1.3095, G loss: 1.2760\n",
      "[1762/1762] D loss: 1.1439, G loss: 1.6208\n",
      "train error: \n",
      " D loss: 1.051869, G loss: 1.109283, D accuracy: 77.0%, cell accuracy: 98.4%, board accuracy: 8.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.057562, G loss: 1.120082, D accuracy: 75.8%, cell accuracy: 98.4%, board accuracy: 7.0% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "[4/1762] D loss: 0.9033, G loss: 1.1808\n",
      "[84/1762] D loss: 1.0300, G loss: 0.8263\n",
      "[164/1762] D loss: 1.1373, G loss: 0.9966\n",
      "[244/1762] D loss: 0.8686, G loss: 1.2192\n",
      "[324/1762] D loss: 1.1680, G loss: 0.7366\n",
      "[404/1762] D loss: 1.1074, G loss: 0.6989\n",
      "[484/1762] D loss: 1.1071, G loss: 1.0484\n",
      "[564/1762] D loss: 0.9666, G loss: 1.3269\n",
      "[644/1762] D loss: 1.0348, G loss: 0.8051\n",
      "[724/1762] D loss: 1.1797, G loss: 0.8710\n",
      "[804/1762] D loss: 1.1984, G loss: 0.9973\n",
      "[884/1762] D loss: 1.2666, G loss: 0.9498\n",
      "[964/1762] D loss: 1.0137, G loss: 0.9446\n",
      "[1044/1762] D loss: 1.1588, G loss: 1.3681\n",
      "[1124/1762] D loss: 1.5346, G loss: 0.6611\n",
      "[1204/1762] D loss: 1.2910, G loss: 1.0804\n",
      "[1284/1762] D loss: 1.1295, G loss: 1.2457\n",
      "[1364/1762] D loss: 1.4644, G loss: 0.6517\n",
      "[1444/1762] D loss: 1.2084, G loss: 0.9185\n",
      "[1524/1762] D loss: 1.1877, G loss: 1.2670\n",
      "[1604/1762] D loss: 0.8437, G loss: 1.2087\n",
      "[1684/1762] D loss: 1.1233, G loss: 0.8332\n",
      "[1762/1762] D loss: 1.0208, G loss: 1.0310\n",
      "train error: \n",
      " D loss: 1.150291, G loss: 0.899470, D accuracy: 70.3%, cell accuracy: 98.7%, board accuracy: 16.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.156148, G loss: 0.899786, D accuracy: 71.0%, cell accuracy: 98.7%, board accuracy: 15.5% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.0861, G loss: 1.0972\n",
      "[84/1762] D loss: 0.9063, G loss: 1.4872\n",
      "[164/1762] D loss: 1.0767, G loss: 1.3009\n",
      "[244/1762] D loss: 1.2732, G loss: 0.9445\n",
      "[324/1762] D loss: 1.1872, G loss: 0.6885\n",
      "[404/1762] D loss: 0.9711, G loss: 1.2233\n",
      "[484/1762] D loss: 1.1976, G loss: 0.9390\n",
      "[564/1762] D loss: 0.8443, G loss: 0.8207\n",
      "[644/1762] D loss: 1.2062, G loss: 1.0636\n",
      "[724/1762] D loss: 1.2026, G loss: 1.1171\n",
      "[804/1762] D loss: 1.2676, G loss: 0.6578\n",
      "[884/1762] D loss: 1.3423, G loss: 0.8152\n",
      "[964/1762] D loss: 1.1494, G loss: 0.9922\n",
      "[1044/1762] D loss: 0.8092, G loss: 1.6348\n",
      "[1124/1762] D loss: 1.1114, G loss: 0.9288\n",
      "[1204/1762] D loss: 1.0199, G loss: 1.1915\n",
      "[1284/1762] D loss: 0.9869, G loss: 0.9889\n",
      "[1364/1762] D loss: 1.2041, G loss: 0.7200\n",
      "[1444/1762] D loss: 0.8790, G loss: 1.2593\n",
      "[1524/1762] D loss: 0.8505, G loss: 2.0742\n",
      "[1604/1762] D loss: 1.1871, G loss: 0.8117\n",
      "[1684/1762] D loss: 1.1390, G loss: 0.6576\n",
      "[1762/1762] D loss: 1.1037, G loss: 0.9609\n",
      "train error: \n",
      " D loss: 1.210230, G loss: 0.717931, D accuracy: 66.4%, cell accuracy: 98.9%, board accuracy: 21.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.212971, G loss: 0.708604, D accuracy: 65.5%, cell accuracy: 98.9%, board accuracy: 21.1% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3881, G loss: 0.5046\n",
      "[84/1762] D loss: 1.3933, G loss: 0.5702\n",
      "[164/1762] D loss: 1.0223, G loss: 0.8314\n",
      "[244/1762] D loss: 1.3762, G loss: 0.5753\n",
      "[324/1762] D loss: 1.1991, G loss: 0.6331\n",
      "[404/1762] D loss: 1.6816, G loss: 0.3907\n",
      "[484/1762] D loss: 1.1308, G loss: 1.3869\n",
      "[564/1762] D loss: 1.2403, G loss: 1.1348\n",
      "[644/1762] D loss: 1.3480, G loss: 0.5447\n",
      "[724/1762] D loss: 1.5235, G loss: 1.1293\n",
      "[804/1762] D loss: 1.4033, G loss: 1.2763\n",
      "[884/1762] D loss: 1.1859, G loss: 0.7818\n",
      "[964/1762] D loss: 1.1250, G loss: 0.6887\n",
      "[1044/1762] D loss: 1.1521, G loss: 1.0736\n",
      "[1124/1762] D loss: 1.1588, G loss: 1.1108\n",
      "[1204/1762] D loss: 1.2541, G loss: 1.0066\n",
      "[1284/1762] D loss: 1.1217, G loss: 0.5995\n",
      "[1364/1762] D loss: 1.2313, G loss: 0.8003\n",
      "[1444/1762] D loss: 1.3205, G loss: 1.0000\n",
      "[1524/1762] D loss: 0.9148, G loss: 1.3784\n",
      "[1604/1762] D loss: 1.3024, G loss: 0.9976\n",
      "[1684/1762] D loss: 1.4228, G loss: 0.8818\n",
      "[1762/1762] D loss: 0.9786, G loss: 1.0034\n",
      "train error: \n",
      " D loss: 1.189583, G loss: 0.889889, D accuracy: 68.0%, cell accuracy: 98.9%, board accuracy: 22.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.194580, G loss: 0.891661, D accuracy: 69.1%, cell accuracy: 98.9%, board accuracy: 22.0% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2309, G loss: 0.8117\n",
      "[84/1762] D loss: 1.0503, G loss: 1.4050\n",
      "[164/1762] D loss: 1.1475, G loss: 1.0842\n",
      "[244/1762] D loss: 1.3686, G loss: 0.5197\n",
      "[324/1762] D loss: 1.5641, G loss: 0.4275\n",
      "[404/1762] D loss: 1.1206, G loss: 0.8571\n",
      "[484/1762] D loss: 1.1462, G loss: 0.7845\n",
      "[564/1762] D loss: 1.4230, G loss: 0.5301\n",
      "[644/1762] D loss: 1.3252, G loss: 0.6346\n",
      "[724/1762] D loss: 1.1403, G loss: 0.9999\n",
      "[804/1762] D loss: 0.8790, G loss: 0.9410\n",
      "[884/1762] D loss: 1.1569, G loss: 1.4813\n",
      "[964/1762] D loss: 1.4340, G loss: 1.2088\n",
      "[1044/1762] D loss: 0.9251, G loss: 1.0356\n",
      "[1124/1762] D loss: 1.0888, G loss: 1.0580\n",
      "[1204/1762] D loss: 1.2336, G loss: 0.8312\n",
      "[1284/1762] D loss: 1.4769, G loss: 0.9419\n",
      "[1364/1762] D loss: 0.9791, G loss: 1.1623\n",
      "[1444/1762] D loss: 1.2923, G loss: 0.8406\n",
      "[1524/1762] D loss: 1.3749, G loss: 1.2786\n",
      "[1604/1762] D loss: 1.4596, G loss: 1.1340\n",
      "[1684/1762] D loss: 1.0535, G loss: 0.9593\n",
      "[1762/1762] D loss: 1.1420, G loss: 0.7353\n",
      "train error: \n",
      " D loss: 1.215732, G loss: 0.762793, D accuracy: 65.1%, cell accuracy: 99.1%, board accuracy: 21.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.221226, G loss: 0.758958, D accuracy: 66.1%, cell accuracy: 99.0%, board accuracy: 21.4% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.0981, G loss: 0.8671\n",
      "[84/1762] D loss: 1.3117, G loss: 1.0302\n",
      "[164/1762] D loss: 1.2824, G loss: 0.6936\n",
      "[244/1762] D loss: 1.2723, G loss: 1.2520\n",
      "[324/1762] D loss: 1.2920, G loss: 1.0145\n",
      "[404/1762] D loss: 1.3467, G loss: 0.9348\n",
      "[484/1762] D loss: 1.3428, G loss: 0.5734\n",
      "[564/1762] D loss: 1.4432, G loss: 0.7660\n",
      "[644/1762] D loss: 1.5144, G loss: 1.1961\n",
      "[724/1762] D loss: 1.2060, G loss: 0.6580\n",
      "[804/1762] D loss: 1.2053, G loss: 0.6097\n",
      "[884/1762] D loss: 0.9160, G loss: 1.3202\n",
      "[964/1762] D loss: 1.2627, G loss: 1.0217\n",
      "[1044/1762] D loss: 1.0312, G loss: 1.0035\n",
      "[1124/1762] D loss: 1.1973, G loss: 0.7487\n",
      "[1204/1762] D loss: 1.0911, G loss: 1.1600\n",
      "[1284/1762] D loss: 1.3092, G loss: 0.9959\n",
      "[1364/1762] D loss: 1.5150, G loss: 1.1866\n",
      "[1444/1762] D loss: 1.4790, G loss: 0.8140\n",
      "[1524/1762] D loss: 1.2798, G loss: 0.7658\n",
      "[1604/1762] D loss: 1.3058, G loss: 1.0408\n",
      "[1684/1762] D loss: 1.3684, G loss: 0.9638\n",
      "[1762/1762] D loss: 1.5014, G loss: 1.0004\n",
      "train error: \n",
      " D loss: 1.294003, G loss: 0.578170, D accuracy: 58.9%, cell accuracy: 99.2%, board accuracy: 27.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.296247, G loss: 0.582394, D accuracy: 58.8%, cell accuracy: 99.1%, board accuracy: 27.0% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.5320, G loss: 0.3427\n",
      "[84/1762] D loss: 1.3633, G loss: 0.6749\n",
      "[164/1762] D loss: 1.2464, G loss: 0.7781\n",
      "[244/1762] D loss: 1.2322, G loss: 1.5044\n",
      "[324/1762] D loss: 1.2027, G loss: 0.6581\n",
      "[404/1762] D loss: 1.1100, G loss: 1.2855\n",
      "[484/1762] D loss: 1.4940, G loss: 0.7840\n",
      "[564/1762] D loss: 1.1063, G loss: 0.9989\n",
      "[644/1762] D loss: 1.0002, G loss: 0.7463\n",
      "[724/1762] D loss: 1.2377, G loss: 0.5622\n",
      "[804/1762] D loss: 1.2506, G loss: 0.7322\n",
      "[884/1762] D loss: 1.0401, G loss: 0.9610\n",
      "[964/1762] D loss: 1.0224, G loss: 1.6921\n",
      "[1044/1762] D loss: 1.2330, G loss: 0.6910\n",
      "[1124/1762] D loss: 1.3614, G loss: 0.7444\n",
      "[1204/1762] D loss: 1.2965, G loss: 0.8337\n",
      "[1284/1762] D loss: 1.4628, G loss: 0.9187\n",
      "[1364/1762] D loss: 1.0623, G loss: 0.7834\n",
      "[1444/1762] D loss: 1.2798, G loss: 0.9109\n",
      "[1524/1762] D loss: 1.4454, G loss: 0.9517\n",
      "[1604/1762] D loss: 1.1883, G loss: 0.7492\n",
      "[1684/1762] D loss: 1.5420, G loss: 1.0138\n",
      "[1762/1762] D loss: 1.0840, G loss: 0.9659\n",
      "train error: \n",
      " D loss: 1.286216, G loss: 0.866611, D accuracy: 61.8%, cell accuracy: 99.5%, board accuracy: 43.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.292405, G loss: 0.870421, D accuracy: 60.2%, cell accuracy: 99.4%, board accuracy: 41.1% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2712, G loss: 0.8079\n",
      "[84/1762] D loss: 1.3842, G loss: 0.6369\n",
      "[164/1762] D loss: 1.3685, G loss: 0.7825\n",
      "[244/1762] D loss: 1.3878, G loss: 0.6932\n",
      "[324/1762] D loss: 1.2715, G loss: 0.7365\n",
      "[404/1762] D loss: 1.4561, G loss: 0.8967\n",
      "[484/1762] D loss: 1.3311, G loss: 0.5547\n",
      "[564/1762] D loss: 1.4495, G loss: 0.7108\n",
      "[644/1762] D loss: 1.6244, G loss: 0.8060\n",
      "[724/1762] D loss: 1.3971, G loss: 0.5986\n",
      "[804/1762] D loss: 1.4000, G loss: 0.9224\n",
      "[884/1762] D loss: 1.3378, G loss: 0.7362\n",
      "[964/1762] D loss: 1.3289, G loss: 0.8323\n",
      "[1044/1762] D loss: 1.5115, G loss: 1.0522\n",
      "[1124/1762] D loss: 1.4368, G loss: 0.8401\n",
      "[1204/1762] D loss: 1.3509, G loss: 0.4817\n",
      "[1284/1762] D loss: 1.1381, G loss: 1.0719\n",
      "[1364/1762] D loss: 1.4016, G loss: 1.0727\n",
      "[1444/1762] D loss: 1.3998, G loss: 0.9178\n",
      "[1524/1762] D loss: 1.3382, G loss: 0.5966\n",
      "[1604/1762] D loss: 1.4572, G loss: 0.8057\n",
      "[1684/1762] D loss: 1.2798, G loss: 0.7703\n",
      "[1762/1762] D loss: 1.3417, G loss: 0.6418\n",
      "train error: \n",
      " D loss: 1.384136, G loss: 0.644191, D accuracy: 55.0%, cell accuracy: 99.6%, board accuracy: 63.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.379280, G loss: 0.647698, D accuracy: 54.9%, cell accuracy: 99.6%, board accuracy: 59.5% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3799, G loss: 0.5759\n",
      "[84/1762] D loss: 1.3421, G loss: 0.5660\n",
      "[164/1762] D loss: 1.1308, G loss: 1.2261\n",
      "[244/1762] D loss: 1.3679, G loss: 0.8546\n",
      "[324/1762] D loss: 1.4409, G loss: 0.8071\n",
      "[404/1762] D loss: 1.3414, G loss: 0.6874\n",
      "[484/1762] D loss: 1.3677, G loss: 0.6232\n",
      "[564/1762] D loss: 1.1531, G loss: 0.5777\n",
      "[644/1762] D loss: 1.4234, G loss: 0.4969\n",
      "[724/1762] D loss: 1.2434, G loss: 0.9316\n",
      "[804/1762] D loss: 1.5253, G loss: 0.4538\n",
      "[884/1762] D loss: 1.5354, G loss: 0.4447\n",
      "[964/1762] D loss: 1.3582, G loss: 0.9242\n",
      "[1044/1762] D loss: 1.1920, G loss: 0.8536\n",
      "[1124/1762] D loss: 1.3747, G loss: 0.6237\n",
      "[1204/1762] D loss: 1.3379, G loss: 0.7388\n",
      "[1284/1762] D loss: 1.4551, G loss: 0.6637\n",
      "[1364/1762] D loss: 1.3964, G loss: 0.7985\n",
      "[1444/1762] D loss: 1.3707, G loss: 0.8062\n",
      "[1524/1762] D loss: 1.4019, G loss: 1.0296\n",
      "[1604/1762] D loss: 1.3748, G loss: 0.7683\n",
      "[1684/1762] D loss: 1.3810, G loss: 0.5931\n",
      "[1762/1762] D loss: 1.4241, G loss: 0.9227\n",
      "train error: \n",
      " D loss: 1.372093, G loss: 0.840352, D accuracy: 54.2%, cell accuracy: 99.7%, board accuracy: 68.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.364947, G loss: 0.852824, D accuracy: 54.3%, cell accuracy: 99.6%, board accuracy: 67.0% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3789, G loss: 0.7580\n",
      "[84/1762] D loss: 1.2512, G loss: 1.0020\n",
      "[164/1762] D loss: 1.3444, G loss: 0.7714\n",
      "[244/1762] D loss: 1.3303, G loss: 0.4436\n",
      "[324/1762] D loss: 1.3329, G loss: 0.9397\n",
      "[404/1762] D loss: 1.4311, G loss: 0.9409\n",
      "[484/1762] D loss: 1.4527, G loss: 0.6716\n",
      "[564/1762] D loss: 1.5096, G loss: 0.5141\n",
      "[644/1762] D loss: 1.3429, G loss: 0.7582\n",
      "[724/1762] D loss: 1.3818, G loss: 0.6082\n",
      "[804/1762] D loss: 1.3431, G loss: 0.7419\n",
      "[884/1762] D loss: 1.3981, G loss: 0.5456\n",
      "[964/1762] D loss: 1.4746, G loss: 0.4516\n",
      "[1044/1762] D loss: 1.4071, G loss: 0.6553\n",
      "[1124/1762] D loss: 1.4621, G loss: 1.0471\n",
      "[1204/1762] D loss: 1.2477, G loss: 0.8402\n",
      "[1284/1762] D loss: 1.4219, G loss: 0.8827\n",
      "[1364/1762] D loss: 1.2994, G loss: 0.9489\n",
      "[1444/1762] D loss: 1.3475, G loss: 0.9922\n",
      "[1524/1762] D loss: 1.3318, G loss: 0.6371\n",
      "[1604/1762] D loss: 1.4428, G loss: 0.5392\n",
      "[1684/1762] D loss: 1.3798, G loss: 1.0501\n",
      "[1762/1762] D loss: 1.3356, G loss: 0.6622\n",
      "train error: \n",
      " D loss: 1.380034, G loss: 0.624290, D accuracy: 54.6%, cell accuracy: 99.7%, board accuracy: 71.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.374489, G loss: 0.630733, D accuracy: 56.1%, cell accuracy: 99.6%, board accuracy: 71.4% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3683, G loss: 0.7828\n",
      "[84/1762] D loss: 1.3461, G loss: 0.9895\n",
      "[164/1762] D loss: 1.2680, G loss: 0.6061\n",
      "[244/1762] D loss: 1.4061, G loss: 0.6798\n",
      "[324/1762] D loss: 1.3268, G loss: 0.9556\n",
      "[404/1762] D loss: 1.3280, G loss: 0.8321\n",
      "[484/1762] D loss: 1.2624, G loss: 0.6969\n",
      "[564/1762] D loss: 1.4358, G loss: 1.0263\n",
      "[644/1762] D loss: 1.5601, G loss: 0.3730\n",
      "[724/1762] D loss: 1.3546, G loss: 1.0265\n",
      "[804/1762] D loss: 1.4010, G loss: 0.5786\n",
      "[884/1762] D loss: 1.4068, G loss: 0.6138\n",
      "[964/1762] D loss: 1.4249, G loss: 0.7568\n",
      "[1044/1762] D loss: 1.4343, G loss: 0.5331\n",
      "[1124/1762] D loss: 1.4299, G loss: 0.4738\n",
      "[1204/1762] D loss: 1.4138, G loss: 0.7826\n",
      "[1284/1762] D loss: 1.3183, G loss: 0.6867\n",
      "[1364/1762] D loss: 1.3914, G loss: 1.0218\n",
      "[1444/1762] D loss: 1.3731, G loss: 0.6532\n",
      "[1524/1762] D loss: 1.4232, G loss: 0.6098\n",
      "[1604/1762] D loss: 1.3247, G loss: 0.7653\n",
      "[1684/1762] D loss: 1.2858, G loss: 0.9703\n",
      "[1762/1762] D loss: 1.3731, G loss: 0.5839\n",
      "train error: \n",
      " D loss: 1.428002, G loss: 0.526196, D accuracy: 53.0%, cell accuracy: 99.7%, board accuracy: 69.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.422385, G loss: 0.530578, D accuracy: 52.7%, cell accuracy: 99.6%, board accuracy: 67.0% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4420, G loss: 0.5799\n",
      "[84/1762] D loss: 1.2794, G loss: 0.8895\n",
      "[164/1762] D loss: 1.4681, G loss: 0.5000\n",
      "[244/1762] D loss: 1.3818, G loss: 0.6875\n",
      "[324/1762] D loss: 1.3230, G loss: 1.0386\n",
      "[404/1762] D loss: 1.4045, G loss: 0.9224\n",
      "[484/1762] D loss: 1.3064, G loss: 0.8831\n",
      "[564/1762] D loss: 1.3495, G loss: 0.5670\n",
      "[644/1762] D loss: 1.4250, G loss: 0.8073\n",
      "[724/1762] D loss: 1.4024, G loss: 0.6713\n",
      "[804/1762] D loss: 1.5071, G loss: 0.4148\n",
      "[884/1762] D loss: 1.4986, G loss: 0.6808\n",
      "[964/1762] D loss: 1.4251, G loss: 0.6732\n",
      "[1044/1762] D loss: 1.3316, G loss: 0.8701\n",
      "[1124/1762] D loss: 1.4611, G loss: 0.5756\n",
      "[1204/1762] D loss: 1.4956, G loss: 0.5578\n",
      "[1284/1762] D loss: 1.4164, G loss: 0.7245\n",
      "[1364/1762] D loss: 1.3729, G loss: 0.9704\n",
      "[1444/1762] D loss: 1.3758, G loss: 0.6889\n",
      "[1524/1762] D loss: 1.3723, G loss: 0.6838\n",
      "[1604/1762] D loss: 1.3699, G loss: 0.7872\n",
      "[1684/1762] D loss: 1.4719, G loss: 0.9415\n",
      "[1762/1762] D loss: 1.2287, G loss: 0.8760\n",
      "train error: \n",
      " D loss: 1.414777, G loss: 0.543449, D accuracy: 53.2%, cell accuracy: 99.7%, board accuracy: 69.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.408052, G loss: 0.550226, D accuracy: 53.9%, cell accuracy: 99.6%, board accuracy: 66.6% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4213, G loss: 0.4846\n",
      "[84/1762] D loss: 1.3441, G loss: 1.0488\n",
      "[164/1762] D loss: 1.3727, G loss: 0.5903\n",
      "[244/1762] D loss: 1.3564, G loss: 0.7947\n",
      "[324/1762] D loss: 1.3149, G loss: 0.9002\n",
      "[404/1762] D loss: 1.3324, G loss: 0.8219\n",
      "[484/1762] D loss: 1.2865, G loss: 0.7574\n",
      "[564/1762] D loss: 1.4050, G loss: 0.7677\n",
      "[644/1762] D loss: 1.3267, G loss: 0.7624\n",
      "[724/1762] D loss: 1.3880, G loss: 0.6417\n",
      "[804/1762] D loss: 1.3847, G loss: 0.7636\n",
      "[884/1762] D loss: 1.4137, G loss: 0.5449\n",
      "[964/1762] D loss: 1.4018, G loss: 0.7426\n",
      "[1044/1762] D loss: 1.4006, G loss: 0.6800\n",
      "[1124/1762] D loss: 1.4818, G loss: 0.5974\n",
      "[1204/1762] D loss: 1.5898, G loss: 0.9798\n",
      "[1284/1762] D loss: 1.5844, G loss: 0.3649\n",
      "[1364/1762] D loss: 1.3917, G loss: 0.8597\n",
      "[1444/1762] D loss: 1.3372, G loss: 0.7292\n",
      "[1524/1762] D loss: 1.3557, G loss: 0.6847\n",
      "[1604/1762] D loss: 1.5371, G loss: 0.8831\n",
      "[1684/1762] D loss: 1.3681, G loss: 0.7205\n",
      "[1762/1762] D loss: 1.1840, G loss: 0.8134\n",
      "train error: \n",
      " D loss: 1.387062, G loss: 0.619753, D accuracy: 54.0%, cell accuracy: 99.7%, board accuracy: 76.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.377854, G loss: 0.630517, D accuracy: 55.5%, cell accuracy: 99.7%, board accuracy: 74.8% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4236, G loss: 0.5024\n",
      "[84/1762] D loss: 1.3785, G loss: 0.8608\n",
      "[164/1762] D loss: 1.4726, G loss: 0.6202\n",
      "[244/1762] D loss: 1.3297, G loss: 0.7258\n",
      "[324/1762] D loss: 1.3613, G loss: 0.6562\n",
      "[404/1762] D loss: 1.2477, G loss: 0.8444\n",
      "[484/1762] D loss: 1.3987, G loss: 0.6310\n",
      "[564/1762] D loss: 1.3509, G loss: 0.5664\n",
      "[644/1762] D loss: 1.4290, G loss: 0.6431\n",
      "[724/1762] D loss: 1.5472, G loss: 0.7262\n",
      "[804/1762] D loss: 1.3347, G loss: 0.7723\n",
      "[884/1762] D loss: 1.4519, G loss: 0.4666\n",
      "[964/1762] D loss: 1.4707, G loss: 0.6426\n",
      "[1044/1762] D loss: 1.4423, G loss: 0.6653\n",
      "[1124/1762] D loss: 1.3352, G loss: 0.8306\n",
      "[1204/1762] D loss: 1.4291, G loss: 0.5312\n",
      "[1284/1762] D loss: 1.5497, G loss: 1.0836\n",
      "[1364/1762] D loss: 1.3989, G loss: 0.6413\n",
      "[1444/1762] D loss: 1.3279, G loss: 0.7870\n",
      "[1524/1762] D loss: 1.3364, G loss: 0.9393\n",
      "[1604/1762] D loss: 1.2975, G loss: 0.6716\n",
      "[1684/1762] D loss: 1.3381, G loss: 0.5809\n",
      "[1762/1762] D loss: 1.3805, G loss: 0.7854\n",
      "train error: \n",
      " D loss: 1.381955, G loss: 0.820276, D accuracy: 52.8%, cell accuracy: 99.7%, board accuracy: 75.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.377381, G loss: 0.832140, D accuracy: 52.6%, cell accuracy: 99.6%, board accuracy: 73.2% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3038, G loss: 0.8910\n",
      "[84/1762] D loss: 1.4084, G loss: 0.9709\n",
      "[164/1762] D loss: 1.4195, G loss: 1.0048\n",
      "[244/1762] D loss: 1.3858, G loss: 0.6560\n",
      "[324/1762] D loss: 1.4070, G loss: 0.8344\n",
      "[404/1762] D loss: 1.3449, G loss: 0.5923\n",
      "[484/1762] D loss: 1.2830, G loss: 0.8369\n",
      "[564/1762] D loss: 1.2572, G loss: 0.9008\n",
      "[644/1762] D loss: 1.3209, G loss: 0.7344\n",
      "[724/1762] D loss: 1.3344, G loss: 0.9285\n",
      "[804/1762] D loss: 1.3376, G loss: 0.7003\n",
      "[884/1762] D loss: 1.4543, G loss: 0.5455\n",
      "[964/1762] D loss: 1.4679, G loss: 0.7876\n",
      "[1044/1762] D loss: 1.3827, G loss: 0.6721\n",
      "[1124/1762] D loss: 1.4011, G loss: 0.6971\n",
      "[1204/1762] D loss: 1.4030, G loss: 0.6502\n",
      "[1284/1762] D loss: 1.5211, G loss: 0.6687\n",
      "[1364/1762] D loss: 1.4105, G loss: 0.7961\n",
      "[1444/1762] D loss: 1.4034, G loss: 0.7678\n",
      "[1524/1762] D loss: 1.3666, G loss: 0.6561\n",
      "[1604/1762] D loss: 1.3768, G loss: 0.6324\n",
      "[1684/1762] D loss: 1.4351, G loss: 0.9155\n",
      "[1762/1762] D loss: 1.3488, G loss: 0.6208\n",
      "train error: \n",
      " D loss: 1.405089, G loss: 0.561841, D accuracy: 52.5%, cell accuracy: 99.7%, board accuracy: 78.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.394771, G loss: 0.574763, D accuracy: 54.3%, cell accuracy: 99.7%, board accuracy: 78.0% \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4137, G loss: 0.5768\n",
      "[84/1762] D loss: 1.3151, G loss: 0.6689\n",
      "[164/1762] D loss: 1.3717, G loss: 0.7735\n",
      "[244/1762] D loss: 1.4436, G loss: 0.7378\n",
      "[324/1762] D loss: 1.2461, G loss: 0.8244\n",
      "[404/1762] D loss: 1.3950, G loss: 0.6881\n",
      "[484/1762] D loss: 1.3320, G loss: 0.8229\n",
      "[564/1762] D loss: 1.4374, G loss: 0.6715\n",
      "[644/1762] D loss: 1.3313, G loss: 0.5788\n",
      "[724/1762] D loss: 1.2736, G loss: 0.7233\n",
      "[804/1762] D loss: 1.4367, G loss: 0.6340\n",
      "[884/1762] D loss: 1.3775, G loss: 1.0130\n",
      "[964/1762] D loss: 1.3903, G loss: 0.5068\n",
      "[1044/1762] D loss: 1.3614, G loss: 0.5532\n",
      "[1124/1762] D loss: 1.3838, G loss: 0.5849\n",
      "[1204/1762] D loss: 1.3783, G loss: 0.8911\n",
      "[1284/1762] D loss: 1.3966, G loss: 0.6508\n",
      "[1364/1762] D loss: 1.3858, G loss: 0.7722\n",
      "[1444/1762] D loss: 1.4218, G loss: 0.8201\n",
      "[1524/1762] D loss: 1.4006, G loss: 0.9595\n",
      "[1604/1762] D loss: 1.4100, G loss: 0.6030\n",
      "[1684/1762] D loss: 1.5130, G loss: 0.5679\n",
      "[1762/1762] D loss: 1.5394, G loss: 0.9374\n",
      "train error: \n",
      " D loss: 1.408486, G loss: 0.911108, D accuracy: 51.6%, cell accuracy: 99.7%, board accuracy: 78.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.401840, G loss: 0.930019, D accuracy: 50.5%, cell accuracy: 99.7%, board accuracy: 76.1% \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3834, G loss: 0.7430\n",
      "[84/1762] D loss: 1.4188, G loss: 0.6089\n",
      "[164/1762] D loss: 1.2843, G loss: 0.6280\n",
      "[244/1762] D loss: 1.4145, G loss: 0.8213\n",
      "[324/1762] D loss: 1.4699, G loss: 0.6284\n",
      "[404/1762] D loss: 1.3534, G loss: 0.7112\n",
      "[484/1762] D loss: 1.5231, G loss: 1.0038\n",
      "[564/1762] D loss: 1.3951, G loss: 0.5370\n",
      "[644/1762] D loss: 1.3860, G loss: 0.8161\n",
      "[724/1762] D loss: 1.3701, G loss: 0.5544\n",
      "[804/1762] D loss: 1.4109, G loss: 0.8964\n",
      "[884/1762] D loss: 1.4034, G loss: 0.7113\n",
      "[964/1762] D loss: 1.3299, G loss: 0.7931\n",
      "[1044/1762] D loss: 1.1647, G loss: 0.9648\n",
      "[1124/1762] D loss: 1.2973, G loss: 0.7322\n",
      "[1204/1762] D loss: 1.3015, G loss: 0.8161\n",
      "[1284/1762] D loss: 1.4009, G loss: 0.7915\n",
      "[1364/1762] D loss: 1.2930, G loss: 0.9111\n",
      "[1444/1762] D loss: 1.3713, G loss: 0.6827\n",
      "[1524/1762] D loss: 1.4142, G loss: 0.6469\n",
      "[1604/1762] D loss: 1.2687, G loss: 0.7698\n",
      "[1684/1762] D loss: 1.3936, G loss: 0.6861\n",
      "[1762/1762] D loss: 1.3892, G loss: 0.7150\n",
      "train error: \n",
      " D loss: 1.404648, G loss: 0.545021, D accuracy: 52.7%, cell accuracy: 99.7%, board accuracy: 76.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.395618, G loss: 0.556765, D accuracy: 54.4%, cell accuracy: 99.7%, board accuracy: 75.0% \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4652, G loss: 0.6781\n",
      "[84/1762] D loss: 1.4108, G loss: 0.6812\n",
      "[164/1762] D loss: 1.3421, G loss: 0.9704\n",
      "[244/1762] D loss: 1.3842, G loss: 0.7208\n",
      "[324/1762] D loss: 1.4085, G loss: 0.7002\n",
      "[404/1762] D loss: 1.4216, G loss: 0.7017\n",
      "[484/1762] D loss: 1.4450, G loss: 0.9073\n",
      "[564/1762] D loss: 1.3505, G loss: 0.9577\n",
      "[644/1762] D loss: 1.3897, G loss: 0.5839\n",
      "[724/1762] D loss: 1.4165, G loss: 1.0055\n",
      "[804/1762] D loss: 1.3765, G loss: 0.7916\n",
      "[884/1762] D loss: 1.4043, G loss: 0.5603\n",
      "[964/1762] D loss: 1.4693, G loss: 0.5070\n",
      "[1044/1762] D loss: 1.4691, G loss: 0.9325\n",
      "[1124/1762] D loss: 1.3930, G loss: 0.6894\n",
      "[1204/1762] D loss: 1.3836, G loss: 0.8430\n",
      "[1284/1762] D loss: 1.4698, G loss: 0.7474\n",
      "[1364/1762] D loss: 1.4400, G loss: 0.5530\n",
      "[1444/1762] D loss: 1.3228, G loss: 0.9099\n",
      "[1524/1762] D loss: 1.3790, G loss: 0.6600\n",
      "[1604/1762] D loss: 1.4231, G loss: 0.9329\n",
      "[1684/1762] D loss: 1.4038, G loss: 0.7290\n",
      "[1762/1762] D loss: 1.1923, G loss: 0.7529\n",
      "train error: \n",
      " D loss: 1.366554, G loss: 0.678525, D accuracy: 54.6%, cell accuracy: 99.7%, board accuracy: 79.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.359161, G loss: 0.688092, D accuracy: 55.3%, cell accuracy: 99.7%, board accuracy: 77.5% \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2837, G loss: 0.7405\n",
      "[84/1762] D loss: 1.3943, G loss: 0.5859\n",
      "[164/1762] D loss: 1.3029, G loss: 0.8221\n",
      "[244/1762] D loss: 1.4179, G loss: 0.8358\n",
      "[324/1762] D loss: 1.3811, G loss: 0.6408\n",
      "[404/1762] D loss: 1.4175, G loss: 0.7570\n",
      "[484/1762] D loss: 1.4155, G loss: 0.9093\n",
      "[564/1762] D loss: 1.4019, G loss: 0.6237\n",
      "[644/1762] D loss: 1.3481, G loss: 0.8734\n",
      "[724/1762] D loss: 1.3776, G loss: 0.7976\n",
      "[804/1762] D loss: 1.4054, G loss: 0.6453\n",
      "[884/1762] D loss: 1.3800, G loss: 0.7384\n",
      "[964/1762] D loss: 1.3966, G loss: 0.8510\n",
      "[1044/1762] D loss: 1.3533, G loss: 0.5870\n",
      "[1124/1762] D loss: 1.3899, G loss: 0.9118\n",
      "[1204/1762] D loss: 1.4014, G loss: 0.6861\n",
      "[1284/1762] D loss: 1.4454, G loss: 0.8731\n",
      "[1364/1762] D loss: 1.4129, G loss: 0.6988\n",
      "[1444/1762] D loss: 1.4429, G loss: 0.8834\n",
      "[1524/1762] D loss: 1.3980, G loss: 0.5925\n",
      "[1604/1762] D loss: 1.3792, G loss: 0.8695\n",
      "[1684/1762] D loss: 1.4450, G loss: 0.5174\n",
      "[1762/1762] D loss: 1.4069, G loss: 0.6504\n",
      "train error: \n",
      " D loss: 1.367484, G loss: 0.764245, D accuracy: 53.4%, cell accuracy: 99.8%, board accuracy: 81.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.355229, G loss: 0.782678, D accuracy: 54.2%, cell accuracy: 99.7%, board accuracy: 80.0% \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2928, G loss: 0.6816\n",
      "[84/1762] D loss: 1.2901, G loss: 0.8807\n",
      "[164/1762] D loss: 1.3899, G loss: 0.6967\n",
      "[244/1762] D loss: 1.4190, G loss: 0.8774\n",
      "[324/1762] D loss: 1.3575, G loss: 0.6884\n",
      "[404/1762] D loss: 1.4704, G loss: 1.0568\n",
      "[484/1762] D loss: 1.4326, G loss: 0.5905\n",
      "[564/1762] D loss: 1.2788, G loss: 0.8398\n",
      "[644/1762] D loss: 1.3170, G loss: 0.6391\n",
      "[724/1762] D loss: 1.3998, G loss: 0.8682\n",
      "[804/1762] D loss: 1.3962, G loss: 0.8045\n",
      "[884/1762] D loss: 1.4019, G loss: 0.8480\n",
      "[964/1762] D loss: 1.3709, G loss: 0.5734\n",
      "[1044/1762] D loss: 1.3718, G loss: 0.7697\n",
      "[1124/1762] D loss: 1.3940, G loss: 0.8323\n",
      "[1204/1762] D loss: 1.3028, G loss: 0.7311\n",
      "[1284/1762] D loss: 1.4382, G loss: 0.7966\n",
      "[1364/1762] D loss: 1.2795, G loss: 0.7135\n",
      "[1444/1762] D loss: 1.3924, G loss: 0.7751\n",
      "[1524/1762] D loss: 1.3847, G loss: 0.7184\n",
      "[1604/1762] D loss: 1.2529, G loss: 1.0238\n",
      "[1684/1762] D loss: 1.2896, G loss: 0.7059\n",
      "[1762/1762] D loss: 1.4152, G loss: 0.9535\n",
      "train error: \n",
      " D loss: 1.383584, G loss: 0.873536, D accuracy: 53.1%, cell accuracy: 99.8%, board accuracy: 79.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.374923, G loss: 0.886002, D accuracy: 53.3%, cell accuracy: 99.7%, board accuracy: 77.5% \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2664, G loss: 0.8540\n",
      "[84/1762] D loss: 1.3494, G loss: 0.7422\n",
      "[164/1762] D loss: 1.2987, G loss: 0.9284\n",
      "[244/1762] D loss: 1.5300, G loss: 0.8413\n",
      "[324/1762] D loss: 1.1404, G loss: 0.7270\n",
      "[404/1762] D loss: 1.3984, G loss: 0.7328\n",
      "[484/1762] D loss: 1.4002, G loss: 0.4679\n",
      "[564/1762] D loss: 1.3204, G loss: 1.0526\n",
      "[644/1762] D loss: 1.4478, G loss: 0.8120\n",
      "[724/1762] D loss: 1.2743, G loss: 0.7760\n",
      "[804/1762] D loss: 1.4059, G loss: 0.5493\n",
      "[884/1762] D loss: 1.3880, G loss: 0.8647\n",
      "[964/1762] D loss: 1.4445, G loss: 0.4387\n",
      "[1044/1762] D loss: 1.3850, G loss: 0.9805\n",
      "[1124/1762] D loss: 1.2294, G loss: 0.6756\n",
      "[1204/1762] D loss: 1.4098, G loss: 0.8618\n",
      "[1284/1762] D loss: 1.4543, G loss: 0.7743\n",
      "[1364/1762] D loss: 1.3895, G loss: 0.6233\n",
      "[1444/1762] D loss: 1.4042, G loss: 0.7405\n",
      "[1524/1762] D loss: 1.3067, G loss: 0.6293\n",
      "[1604/1762] D loss: 1.3702, G loss: 0.7286\n",
      "[1684/1762] D loss: 1.2712, G loss: 0.6588\n",
      "[1762/1762] D loss: 1.4165, G loss: 0.6583\n",
      "train error: \n",
      " D loss: 1.361597, G loss: 0.749505, D accuracy: 54.1%, cell accuracy: 99.8%, board accuracy: 79.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.355234, G loss: 0.757361, D accuracy: 55.2%, cell accuracy: 99.7%, board accuracy: 79.1% \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3473, G loss: 0.7922\n",
      "[84/1762] D loss: 1.4002, G loss: 0.8039\n",
      "[164/1762] D loss: 1.4564, G loss: 0.5606\n",
      "[244/1762] D loss: 1.3984, G loss: 0.8625\n",
      "[324/1762] D loss: 1.3930, G loss: 0.6335\n",
      "[404/1762] D loss: 1.3393, G loss: 0.5681\n",
      "[484/1762] D loss: 1.4365, G loss: 0.8794\n",
      "[564/1762] D loss: 1.3463, G loss: 0.7185\n",
      "[644/1762] D loss: 1.4312, G loss: 0.8559\n",
      "[724/1762] D loss: 1.2830, G loss: 0.8386\n",
      "[804/1762] D loss: 1.4011, G loss: 0.7418\n",
      "[884/1762] D loss: 1.3981, G loss: 0.6985\n",
      "[964/1762] D loss: 1.2494, G loss: 0.8193\n",
      "[1044/1762] D loss: 1.4532, G loss: 0.5565\n",
      "[1124/1762] D loss: 1.3987, G loss: 0.7465\n",
      "[1204/1762] D loss: 1.3828, G loss: 0.6273\n",
      "[1284/1762] D loss: 1.2343, G loss: 0.7778\n",
      "[1364/1762] D loss: 1.3978, G loss: 0.8070\n",
      "[1444/1762] D loss: 1.3977, G loss: 0.7952\n",
      "[1524/1762] D loss: 1.4096, G loss: 0.7120\n",
      "[1604/1762] D loss: 1.4170, G loss: 0.6060\n",
      "[1684/1762] D loss: 1.3145, G loss: 0.7880\n",
      "[1762/1762] D loss: 1.4054, G loss: 0.6281\n",
      "train error: \n",
      " D loss: 1.355919, G loss: 0.728859, D accuracy: 54.2%, cell accuracy: 99.8%, board accuracy: 81.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.346954, G loss: 0.741668, D accuracy: 55.5%, cell accuracy: 99.8%, board accuracy: 81.8% \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3900, G loss: 0.7801\n",
      "[84/1762] D loss: 1.4179, G loss: 0.6128\n",
      "[164/1762] D loss: 1.3887, G loss: 0.8081\n",
      "[244/1762] D loss: 1.3085, G loss: 0.5953\n",
      "[324/1762] D loss: 1.3398, G loss: 0.8339\n",
      "[404/1762] D loss: 1.3870, G loss: 0.8197\n",
      "[484/1762] D loss: 1.3239, G loss: 0.8167\n",
      "[564/1762] D loss: 1.3940, G loss: 0.8206\n",
      "[644/1762] D loss: 1.3950, G loss: 0.7681\n",
      "[724/1762] D loss: 1.4003, G loss: 0.7620\n",
      "[804/1762] D loss: 1.4087, G loss: 0.7396\n",
      "[884/1762] D loss: 1.3918, G loss: 0.7174\n",
      "[964/1762] D loss: 1.4017, G loss: 0.7439\n",
      "[1044/1762] D loss: 1.4329, G loss: 1.0141\n",
      "[1124/1762] D loss: 1.2949, G loss: 0.7174\n",
      "[1204/1762] D loss: 1.3960, G loss: 0.7218\n",
      "[1284/1762] D loss: 1.2345, G loss: 0.7028\n",
      "[1364/1762] D loss: 1.2410, G loss: 0.7579\n",
      "[1444/1762] D loss: 1.4114, G loss: 0.5527\n",
      "[1524/1762] D loss: 1.3951, G loss: 0.7218\n",
      "[1604/1762] D loss: 1.4099, G loss: 0.5614\n",
      "[1684/1762] D loss: 1.3732, G loss: 0.7739\n",
      "[1762/1762] D loss: 1.3859, G loss: 0.7412\n",
      "train error: \n",
      " D loss: 1.360994, G loss: 0.802916, D accuracy: 53.5%, cell accuracy: 99.8%, board accuracy: 84.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.350584, G loss: 0.819750, D accuracy: 54.9%, cell accuracy: 99.8%, board accuracy: 83.0% \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3915, G loss: 0.7583\n",
      "[84/1762] D loss: 1.1888, G loss: 0.6861\n",
      "[164/1762] D loss: 1.4118, G loss: 0.8195\n",
      "[244/1762] D loss: 1.3963, G loss: 0.6407\n",
      "[324/1762] D loss: 1.4105, G loss: 0.4930\n",
      "[404/1762] D loss: 1.3856, G loss: 0.7901\n",
      "[484/1762] D loss: 1.2723, G loss: 0.6438\n",
      "[564/1762] D loss: 1.3583, G loss: 0.7961\n",
      "[644/1762] D loss: 1.4187, G loss: 0.8338\n",
      "[724/1762] D loss: 1.3775, G loss: 0.8079\n",
      "[804/1762] D loss: 1.4345, G loss: 0.7793\n",
      "[884/1762] D loss: 1.3918, G loss: 0.7990\n",
      "[964/1762] D loss: 1.3839, G loss: 0.6869\n",
      "[1044/1762] D loss: 1.2013, G loss: 0.7621\n",
      "[1124/1762] D loss: 1.4216, G loss: 0.6457\n",
      "[1204/1762] D loss: 1.4084, G loss: 0.6592\n",
      "[1284/1762] D loss: 1.2383, G loss: 0.7072\n",
      "[1364/1762] D loss: 1.3807, G loss: 0.7990\n",
      "[1444/1762] D loss: 1.3161, G loss: 0.6807\n",
      "[1524/1762] D loss: 1.4531, G loss: 0.9428\n",
      "[1604/1762] D loss: 1.3061, G loss: 0.5875\n",
      "[1684/1762] D loss: 1.4507, G loss: 0.6916\n",
      "[1762/1762] D loss: 1.1984, G loss: 0.8995\n",
      "train error: \n",
      " D loss: 1.463551, G loss: 0.465553, D accuracy: 52.3%, cell accuracy: 99.6%, board accuracy: 70.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.448971, G loss: 0.477346, D accuracy: 54.0%, cell accuracy: 99.6%, board accuracy: 68.2% \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.5679, G loss: 0.3904\n",
      "[84/1762] D loss: 1.4013, G loss: 1.0020\n",
      "[164/1762] D loss: 1.4052, G loss: 0.6082\n",
      "[244/1762] D loss: 1.3189, G loss: 0.9142\n",
      "[324/1762] D loss: 1.2622, G loss: 0.7227\n",
      "[404/1762] D loss: 1.4040, G loss: 0.7786\n",
      "[484/1762] D loss: 1.3331, G loss: 0.8755\n",
      "[564/1762] D loss: 1.4004, G loss: 0.8065\n",
      "[644/1762] D loss: 1.4373, G loss: 0.5814\n",
      "[724/1762] D loss: 1.3337, G loss: 1.1357\n",
      "[804/1762] D loss: 1.5009, G loss: 0.5680\n",
      "[884/1762] D loss: 1.3056, G loss: 0.7145\n",
      "[964/1762] D loss: 1.3862, G loss: 0.6387\n",
      "[1044/1762] D loss: 1.1672, G loss: 0.8901\n",
      "[1124/1762] D loss: 1.3194, G loss: 0.5591\n",
      "[1204/1762] D loss: 1.4269, G loss: 0.8133\n",
      "[1284/1762] D loss: 1.4110, G loss: 0.7235\n",
      "[1364/1762] D loss: 1.3875, G loss: 0.6877\n",
      "[1444/1762] D loss: 1.4465, G loss: 0.6355\n",
      "[1524/1762] D loss: 1.3883, G loss: 0.9331\n",
      "[1604/1762] D loss: 1.3926, G loss: 0.6351\n",
      "[1684/1762] D loss: 1.4167, G loss: 0.7944\n",
      "[1762/1762] D loss: 1.1157, G loss: 0.6863\n",
      "train error: \n",
      " D loss: 1.402274, G loss: 0.556863, D accuracy: 53.8%, cell accuracy: 99.6%, board accuracy: 71.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.394513, G loss: 0.566568, D accuracy: 55.6%, cell accuracy: 99.6%, board accuracy: 69.8% \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2908, G loss: 0.5758\n",
      "[84/1762] D loss: 1.3560, G loss: 0.7047\n",
      "[164/1762] D loss: 1.4634, G loss: 0.8387\n",
      "[244/1762] D loss: 1.4120, G loss: 0.6051\n",
      "[324/1762] D loss: 1.3895, G loss: 0.7279\n",
      "[404/1762] D loss: 1.3987, G loss: 0.7542\n",
      "[484/1762] D loss: 1.3457, G loss: 0.5839\n",
      "[564/1762] D loss: 1.2355, G loss: 0.9333\n",
      "[644/1762] D loss: 1.3847, G loss: 0.6873\n",
      "[724/1762] D loss: 1.3763, G loss: 0.6494\n",
      "[804/1762] D loss: 1.4686, G loss: 0.7758\n",
      "[884/1762] D loss: 1.2701, G loss: 0.8339\n",
      "[964/1762] D loss: 1.4125, G loss: 0.9267\n",
      "[1044/1762] D loss: 1.4199, G loss: 0.5065\n",
      "[1124/1762] D loss: 1.4165, G loss: 0.6687\n",
      "[1204/1762] D loss: 1.3669, G loss: 0.6631\n",
      "[1284/1762] D loss: 1.2775, G loss: 0.6239\n",
      "[1364/1762] D loss: 1.3771, G loss: 1.0166\n",
      "[1444/1762] D loss: 1.4282, G loss: 0.6016\n",
      "[1524/1762] D loss: 1.3963, G loss: 0.5355\n",
      "[1604/1762] D loss: 1.4007, G loss: 0.8180\n",
      "[1684/1762] D loss: 1.3897, G loss: 0.5384\n",
      "[1762/1762] D loss: 1.4085, G loss: 0.8794\n",
      "train error: \n",
      " D loss: 1.399296, G loss: 0.935923, D accuracy: 52.1%, cell accuracy: 99.6%, board accuracy: 72.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.396926, G loss: 0.944621, D accuracy: 51.9%, cell accuracy: 99.6%, board accuracy: 71.4% \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4389, G loss: 0.8062\n",
      "[84/1762] D loss: 1.3900, G loss: 0.7091\n",
      "[164/1762] D loss: 1.3175, G loss: 0.7240\n",
      "[244/1762] D loss: 1.4549, G loss: 0.8923\n",
      "[324/1762] D loss: 1.2692, G loss: 0.7373\n",
      "[404/1762] D loss: 1.3921, G loss: 0.6712\n",
      "[484/1762] D loss: 1.3870, G loss: 0.6855\n",
      "[564/1762] D loss: 1.2935, G loss: 0.9104\n",
      "[644/1762] D loss: 1.1976, G loss: 0.7656\n",
      "[724/1762] D loss: 1.2790, G loss: 0.7679\n",
      "[804/1762] D loss: 1.1849, G loss: 0.9866\n",
      "[884/1762] D loss: 1.2821, G loss: 0.7805\n",
      "[964/1762] D loss: 1.4060, G loss: 0.6324\n",
      "[1044/1762] D loss: 1.3067, G loss: 0.6389\n",
      "[1124/1762] D loss: 1.4098, G loss: 0.7110\n",
      "[1204/1762] D loss: 1.4125, G loss: 0.6686\n",
      "[1284/1762] D loss: 1.3979, G loss: 0.6343\n",
      "[1364/1762] D loss: 1.3530, G loss: 0.7024\n",
      "[1444/1762] D loss: 1.3395, G loss: 0.7424\n",
      "[1524/1762] D loss: 1.3640, G loss: 0.6164\n",
      "[1604/1762] D loss: 1.4034, G loss: 0.7679\n",
      "[1684/1762] D loss: 1.3052, G loss: 0.6280\n",
      "[1762/1762] D loss: 1.2514, G loss: 0.8729\n",
      "train error: \n",
      " D loss: 1.362140, G loss: 0.759894, D accuracy: 53.5%, cell accuracy: 99.7%, board accuracy: 72.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.353471, G loss: 0.772281, D accuracy: 53.9%, cell accuracy: 99.6%, board accuracy: 70.0% \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3889, G loss: 0.7540\n",
      "[84/1762] D loss: 1.3920, G loss: 0.7139\n",
      "[164/1762] D loss: 1.4372, G loss: 0.6852\n",
      "[244/1762] D loss: 1.3859, G loss: 0.7430\n",
      "[324/1762] D loss: 1.2591, G loss: 0.7879\n",
      "[404/1762] D loss: 1.3977, G loss: 0.7481\n",
      "[484/1762] D loss: 1.2609, G loss: 0.7344\n",
      "[564/1762] D loss: 1.3801, G loss: 0.6335\n",
      "[644/1762] D loss: 1.4135, G loss: 0.7755\n",
      "[724/1762] D loss: 1.3909, G loss: 0.7141\n",
      "[804/1762] D loss: 1.3947, G loss: 0.6653\n",
      "[884/1762] D loss: 1.3922, G loss: 0.7764\n",
      "[964/1762] D loss: 1.3027, G loss: 0.6562\n",
      "[1044/1762] D loss: 1.2832, G loss: 0.7089\n",
      "[1124/1762] D loss: 1.4066, G loss: 0.7515\n",
      "[1204/1762] D loss: 1.2973, G loss: 0.7728\n",
      "[1284/1762] D loss: 1.3948, G loss: 0.7565\n",
      "[1364/1762] D loss: 1.3044, G loss: 0.6738\n",
      "[1444/1762] D loss: 1.3791, G loss: 0.8504\n",
      "[1524/1762] D loss: 1.3861, G loss: 0.7080\n",
      "[1604/1762] D loss: 1.3857, G loss: 0.6287\n",
      "[1684/1762] D loss: 1.1951, G loss: 0.7836\n",
      "[1762/1762] D loss: 1.4065, G loss: 0.6596\n",
      "train error: \n",
      " D loss: 1.360135, G loss: 0.701062, D accuracy: 54.3%, cell accuracy: 99.7%, board accuracy: 71.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.349969, G loss: 0.714600, D accuracy: 54.3%, cell accuracy: 99.6%, board accuracy: 70.0% \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3826, G loss: 0.6559\n",
      "[84/1762] D loss: 1.3797, G loss: 0.6256\n",
      "[164/1762] D loss: 1.4035, G loss: 0.6761\n",
      "[244/1762] D loss: 1.4174, G loss: 0.5900\n",
      "[324/1762] D loss: 1.2871, G loss: 0.9697\n",
      "[404/1762] D loss: 1.4092, G loss: 0.6961\n",
      "[484/1762] D loss: 1.3783, G loss: 0.7543\n",
      "[564/1762] D loss: 1.4063, G loss: 0.6266\n",
      "[644/1762] D loss: 1.3223, G loss: 0.7462\n",
      "[724/1762] D loss: 1.4474, G loss: 0.7522\n",
      "[804/1762] D loss: 1.4013, G loss: 0.7482\n",
      "[884/1762] D loss: 1.4014, G loss: 0.8604\n",
      "[964/1762] D loss: 1.3992, G loss: 0.9259\n",
      "[1044/1762] D loss: 1.2636, G loss: 0.7018\n",
      "[1124/1762] D loss: 1.3831, G loss: 0.8996\n",
      "[1204/1762] D loss: 1.1515, G loss: 1.0251\n",
      "[1284/1762] D loss: 1.3944, G loss: 0.6842\n",
      "[1364/1762] D loss: 1.3911, G loss: 0.7587\n",
      "[1444/1762] D loss: 1.4000, G loss: 0.5992\n",
      "[1524/1762] D loss: 1.3910, G loss: 0.6768\n",
      "[1604/1762] D loss: 1.2351, G loss: 0.7470\n",
      "[1684/1762] D loss: 1.3811, G loss: 0.7536\n",
      "[1762/1762] D loss: 1.1001, G loss: 0.9252\n",
      "train error: \n",
      " D loss: 1.359358, G loss: 0.752699, D accuracy: 53.7%, cell accuracy: 99.8%, board accuracy: 84.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.351343, G loss: 0.760031, D accuracy: 54.5%, cell accuracy: 99.8%, board accuracy: 83.6% \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3349, G loss: 0.7436\n",
      "[84/1762] D loss: 1.3914, G loss: 0.7454\n",
      "[164/1762] D loss: 1.3957, G loss: 0.5649\n",
      "[244/1762] D loss: 1.4027, G loss: 0.8296\n",
      "[324/1762] D loss: 1.3961, G loss: 0.6872\n",
      "[404/1762] D loss: 1.3905, G loss: 0.6190\n",
      "[484/1762] D loss: 1.3683, G loss: 1.0461\n",
      "[564/1762] D loss: 1.2425, G loss: 0.6681\n",
      "[644/1762] D loss: 1.4428, G loss: 0.8239\n",
      "[724/1762] D loss: 1.3486, G loss: 0.7368\n",
      "[804/1762] D loss: 1.3934, G loss: 0.6644\n",
      "[884/1762] D loss: 1.4249, G loss: 0.9282\n",
      "[964/1762] D loss: 1.4186, G loss: 0.7330\n",
      "[1044/1762] D loss: 1.3914, G loss: 0.7036\n",
      "[1124/1762] D loss: 1.4119, G loss: 0.6805\n",
      "[1204/1762] D loss: 1.3965, G loss: 0.7122\n",
      "[1284/1762] D loss: 1.3924, G loss: 0.8405\n",
      "[1364/1762] D loss: 1.4065, G loss: 0.5590\n",
      "[1444/1762] D loss: 1.2433, G loss: 0.7696\n",
      "[1524/1762] D loss: 1.3959, G loss: 0.7293\n",
      "[1604/1762] D loss: 1.3959, G loss: 0.6898\n",
      "[1684/1762] D loss: 1.2837, G loss: 0.6546\n",
      "[1762/1762] D loss: 1.3950, G loss: 0.8271\n",
      "train error: \n",
      " D loss: 1.378260, G loss: 0.892917, D accuracy: 53.0%, cell accuracy: 99.8%, board accuracy: 83.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.370711, G loss: 0.901144, D accuracy: 53.6%, cell accuracy: 99.8%, board accuracy: 83.0% \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4046, G loss: 0.8313\n",
      "[84/1762] D loss: 1.3723, G loss: 0.5670\n",
      "[164/1762] D loss: 1.3928, G loss: 0.7365\n",
      "[244/1762] D loss: 1.2164, G loss: 0.9018\n",
      "[324/1762] D loss: 1.3983, G loss: 0.7864\n",
      "[404/1762] D loss: 1.3965, G loss: 0.6641\n",
      "[484/1762] D loss: 1.4144, G loss: 0.7080\n",
      "[564/1762] D loss: 1.2186, G loss: 0.6950\n",
      "[644/1762] D loss: 1.4102, G loss: 0.6485\n",
      "[724/1762] D loss: 1.4042, G loss: 0.5351\n",
      "[804/1762] D loss: 1.3897, G loss: 0.6335\n",
      "[884/1762] D loss: 1.3991, G loss: 0.6065\n",
      "[964/1762] D loss: 1.3901, G loss: 0.7669\n",
      "[1044/1762] D loss: 1.2979, G loss: 0.6953\n",
      "[1124/1762] D loss: 1.3895, G loss: 0.6894\n",
      "[1204/1762] D loss: 1.4010, G loss: 0.6777\n",
      "[1284/1762] D loss: 1.3875, G loss: 0.6455\n",
      "[1364/1762] D loss: 1.2443, G loss: 0.6962\n",
      "[1444/1762] D loss: 1.4040, G loss: 0.8371\n",
      "[1524/1762] D loss: 1.2332, G loss: 0.7967\n",
      "[1604/1762] D loss: 1.2655, G loss: 0.8472\n",
      "[1684/1762] D loss: 1.2445, G loss: 0.9852\n",
      "[1762/1762] D loss: 1.4195, G loss: 0.8896\n",
      "train error: \n",
      " D loss: 1.364109, G loss: 0.844117, D accuracy: 53.2%, cell accuracy: 99.8%, board accuracy: 88.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.355824, G loss: 0.850398, D accuracy: 53.4%, cell accuracy: 99.8%, board accuracy: 86.8% \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2475, G loss: 0.9032\n",
      "[84/1762] D loss: 1.2664, G loss: 0.7371\n",
      "[164/1762] D loss: 1.4016, G loss: 0.8275\n",
      "[244/1762] D loss: 1.4074, G loss: 0.6409\n",
      "[324/1762] D loss: 1.3997, G loss: 0.7467\n",
      "[404/1762] D loss: 1.4216, G loss: 0.7055\n",
      "[484/1762] D loss: 1.4260, G loss: 0.6210\n",
      "[564/1762] D loss: 1.5308, G loss: 0.5785\n",
      "[644/1762] D loss: 1.4268, G loss: 0.7219\n",
      "[724/1762] D loss: 1.3433, G loss: 0.7060\n",
      "[804/1762] D loss: 1.2345, G loss: 0.8265\n",
      "[884/1762] D loss: 1.4057, G loss: 0.5960\n",
      "[964/1762] D loss: 1.4225, G loss: 0.6080\n",
      "[1044/1762] D loss: 1.3948, G loss: 0.6868\n",
      "[1124/1762] D loss: 1.4755, G loss: 0.5488\n",
      "[1204/1762] D loss: 1.4176, G loss: 0.7404\n",
      "[1284/1762] D loss: 1.4264, G loss: 0.6183\n",
      "[1364/1762] D loss: 1.4109, G loss: 0.7385\n",
      "[1444/1762] D loss: 1.3853, G loss: 0.6291\n",
      "[1524/1762] D loss: 1.3505, G loss: 0.8552\n",
      "[1604/1762] D loss: 1.4187, G loss: 0.7047\n",
      "[1684/1762] D loss: 1.3775, G loss: 0.9676\n",
      "[1762/1762] D loss: 1.4249, G loss: 0.5821\n",
      "train error: \n",
      " D loss: 1.397862, G loss: 0.576347, D accuracy: 50.7%, cell accuracy: 99.8%, board accuracy: 88.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.394169, G loss: 0.577945, D accuracy: 51.9%, cell accuracy: 99.8%, board accuracy: 86.4% \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4082, G loss: 0.6219\n",
      "[84/1762] D loss: 1.4052, G loss: 0.8587\n",
      "[164/1762] D loss: 1.3265, G loss: 0.7847\n",
      "[244/1762] D loss: 1.4096, G loss: 0.5647\n",
      "[324/1762] D loss: 1.3069, G loss: 0.8342\n",
      "[404/1762] D loss: 1.3152, G loss: 0.6932\n",
      "[484/1762] D loss: 1.3704, G loss: 0.8026\n",
      "[564/1762] D loss: 1.3395, G loss: 0.7243\n",
      "[644/1762] D loss: 1.3928, G loss: 0.7642\n",
      "[724/1762] D loss: 1.3031, G loss: 0.7167\n",
      "[804/1762] D loss: 1.4389, G loss: 0.7127\n",
      "[884/1762] D loss: 1.3907, G loss: 0.7458\n",
      "[964/1762] D loss: 1.3898, G loss: 0.7238\n",
      "[1044/1762] D loss: 1.3877, G loss: 0.7345\n",
      "[1124/1762] D loss: 1.3945, G loss: 0.6572\n",
      "[1204/1762] D loss: 1.3629, G loss: 0.7753\n",
      "[1284/1762] D loss: 1.3953, G loss: 0.7968\n",
      "[1364/1762] D loss: 1.2957, G loss: 0.7412\n",
      "[1444/1762] D loss: 1.3875, G loss: 0.7863\n",
      "[1524/1762] D loss: 1.3974, G loss: 0.6076\n",
      "[1604/1762] D loss: 1.3957, G loss: 0.8051\n",
      "[1684/1762] D loss: 1.4176, G loss: 0.5119\n",
      "[1762/1762] D loss: 1.3919, G loss: 0.8424\n",
      "train error: \n",
      " D loss: 1.381902, G loss: 0.861443, D accuracy: 52.8%, cell accuracy: 99.8%, board accuracy: 87.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.375634, G loss: 0.862242, D accuracy: 53.5%, cell accuracy: 99.8%, board accuracy: 83.6% \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3221, G loss: 0.8932\n",
      "[84/1762] D loss: 1.4092, G loss: 0.5232\n",
      "[164/1762] D loss: 1.2778, G loss: 0.8179\n",
      "[244/1762] D loss: 1.3013, G loss: 0.6810\n",
      "[324/1762] D loss: 1.2394, G loss: 0.7835\n",
      "[404/1762] D loss: 1.3894, G loss: 0.6219\n",
      "[484/1762] D loss: 1.3971, G loss: 0.7726\n",
      "[564/1762] D loss: 1.3975, G loss: 0.5316\n",
      "[644/1762] D loss: 1.2752, G loss: 0.7461\n",
      "[724/1762] D loss: 1.3913, G loss: 0.6872\n",
      "[804/1762] D loss: 1.4014, G loss: 0.7288\n",
      "[884/1762] D loss: 1.2989, G loss: 0.6379\n",
      "[964/1762] D loss: 1.2921, G loss: 0.8318\n",
      "[1044/1762] D loss: 1.4151, G loss: 0.7200\n",
      "[1124/1762] D loss: 1.3897, G loss: 0.6753\n",
      "[1204/1762] D loss: 1.3895, G loss: 0.7210\n",
      "[1284/1762] D loss: 1.2704, G loss: 0.7323\n",
      "[1364/1762] D loss: 1.3643, G loss: 0.8481\n",
      "[1444/1762] D loss: 1.3982, G loss: 0.6628\n",
      "[1524/1762] D loss: 1.3980, G loss: 0.5907\n",
      "[1604/1762] D loss: 1.3909, G loss: 0.7727\n",
      "[1684/1762] D loss: 1.2953, G loss: 0.6102\n",
      "[1762/1762] D loss: 1.4020, G loss: 0.7838\n",
      "train error: \n",
      " D loss: 1.370502, G loss: 0.839524, D accuracy: 53.2%, cell accuracy: 99.8%, board accuracy: 87.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.361887, G loss: 0.842948, D accuracy: 53.8%, cell accuracy: 99.8%, board accuracy: 85.9% \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2635, G loss: 0.7924\n",
      "[84/1762] D loss: 1.2513, G loss: 0.8544\n",
      "[164/1762] D loss: 1.4226, G loss: 0.6692\n",
      "[244/1762] D loss: 1.4590, G loss: 0.4775\n",
      "[324/1762] D loss: 1.3887, G loss: 0.7742\n",
      "[404/1762] D loss: 1.4341, G loss: 0.8635\n",
      "[484/1762] D loss: 1.2543, G loss: 0.7568\n",
      "[564/1762] D loss: 1.3972, G loss: 0.7213\n",
      "[644/1762] D loss: 1.3871, G loss: 0.6919\n",
      "[724/1762] D loss: 1.4004, G loss: 0.8086\n",
      "[804/1762] D loss: 1.3889, G loss: 0.6763\n",
      "[884/1762] D loss: 1.3875, G loss: 0.6242\n",
      "[964/1762] D loss: 1.3405, G loss: 0.6995\n",
      "[1044/1762] D loss: 1.3886, G loss: 0.7679\n",
      "[1124/1762] D loss: 1.2746, G loss: 0.6770\n",
      "[1204/1762] D loss: 1.3041, G loss: 0.7724\n",
      "[1284/1762] D loss: 1.3930, G loss: 0.6749\n",
      "[1364/1762] D loss: 1.3978, G loss: 0.8204\n",
      "[1444/1762] D loss: 1.2518, G loss: 0.7315\n",
      "[1524/1762] D loss: 1.3768, G loss: 0.8163\n",
      "[1604/1762] D loss: 1.5184, G loss: 0.5901\n",
      "[1684/1762] D loss: 1.4113, G loss: 0.9149\n",
      "[1762/1762] D loss: 1.1848, G loss: 0.8549\n",
      "train error: \n",
      " D loss: 1.393287, G loss: 0.544725, D accuracy: 53.4%, cell accuracy: 98.6%, board accuracy: 15.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.384577, G loss: 0.553016, D accuracy: 54.1%, cell accuracy: 98.6%, board accuracy: 15.2% \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4390, G loss: 0.4745\n",
      "[84/1762] D loss: 1.1983, G loss: 1.0838\n",
      "[164/1762] D loss: 1.3634, G loss: 0.6467\n",
      "[244/1762] D loss: 1.4156, G loss: 0.5777\n",
      "[324/1762] D loss: 1.3906, G loss: 0.6499\n",
      "[404/1762] D loss: 1.3944, G loss: 0.7570\n",
      "[484/1762] D loss: 1.2671, G loss: 0.6817\n",
      "[564/1762] D loss: 1.3821, G loss: 0.5794\n",
      "[644/1762] D loss: 1.3967, G loss: 0.7459\n",
      "[724/1762] D loss: 1.3993, G loss: 0.7792\n",
      "[804/1762] D loss: 1.3984, G loss: 0.6984\n",
      "[884/1762] D loss: 1.3892, G loss: 0.6095\n",
      "[964/1762] D loss: 1.3938, G loss: 0.7616\n",
      "[1044/1762] D loss: 1.3666, G loss: 0.6763\n",
      "[1124/1762] D loss: 1.3946, G loss: 0.7608\n",
      "[1204/1762] D loss: 1.3986, G loss: 0.6924\n",
      "[1284/1762] D loss: 1.3863, G loss: 0.7198\n",
      "[1364/1762] D loss: 1.1963, G loss: 0.7286\n",
      "[1444/1762] D loss: 1.4006, G loss: 0.6853\n",
      "[1524/1762] D loss: 1.4004, G loss: 0.7762\n",
      "[1604/1762] D loss: 1.3542, G loss: 0.7472\n",
      "[1684/1762] D loss: 1.3983, G loss: 0.6386\n",
      "[1762/1762] D loss: 1.2633, G loss: 0.7157\n",
      "train error: \n",
      " D loss: 1.363796, G loss: 0.694579, D accuracy: 53.7%, cell accuracy: 99.8%, board accuracy: 89.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.357114, G loss: 0.699394, D accuracy: 55.0%, cell accuracy: 99.8%, board accuracy: 86.4% \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3881, G loss: 0.7320\n",
      "[84/1762] D loss: 1.3951, G loss: 0.5984\n",
      "[164/1762] D loss: 1.3680, G loss: 0.9005\n",
      "[244/1762] D loss: 1.2791, G loss: 0.7888\n",
      "[324/1762] D loss: 1.3974, G loss: 0.5636\n",
      "[404/1762] D loss: 1.4322, G loss: 0.9799\n",
      "[484/1762] D loss: 1.3512, G loss: 0.5413\n",
      "[564/1762] D loss: 1.4009, G loss: 0.7249\n",
      "[644/1762] D loss: 1.3816, G loss: 0.7590\n",
      "[724/1762] D loss: 1.3957, G loss: 0.6867\n",
      "[804/1762] D loss: 1.3971, G loss: 0.6775\n",
      "[884/1762] D loss: 1.4034, G loss: 0.6645\n",
      "[964/1762] D loss: 1.3912, G loss: 0.7138\n",
      "[1044/1762] D loss: 1.2648, G loss: 0.8335\n",
      "[1124/1762] D loss: 1.3880, G loss: 0.7222\n",
      "[1204/1762] D loss: 1.3924, G loss: 0.6946\n",
      "[1284/1762] D loss: 1.3178, G loss: 0.8678\n",
      "[1364/1762] D loss: 1.2668, G loss: 0.8947\n",
      "[1444/1762] D loss: 1.2618, G loss: 0.7184\n",
      "[1524/1762] D loss: 1.4179, G loss: 0.7372\n",
      "[1604/1762] D loss: 1.3877, G loss: 0.7529\n",
      "[1684/1762] D loss: 1.3260, G loss: 0.7424\n",
      "[1762/1762] D loss: 1.3871, G loss: 0.7739\n",
      "train error: \n",
      " D loss: 1.365508, G loss: 0.805801, D accuracy: 53.2%, cell accuracy: 99.8%, board accuracy: 85.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.355750, G loss: 0.809993, D accuracy: 54.5%, cell accuracy: 99.8%, board accuracy: 82.7% \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2718, G loss: 0.9202\n",
      "[84/1762] D loss: 1.4060, G loss: 0.6114\n",
      "[164/1762] D loss: 1.3419, G loss: 0.8699\n",
      "[244/1762] D loss: 1.4031, G loss: 0.6987\n",
      "[324/1762] D loss: 1.4137, G loss: 0.5544\n",
      "[404/1762] D loss: 1.3885, G loss: 0.6986\n",
      "[484/1762] D loss: 1.3948, G loss: 0.6853\n",
      "[564/1762] D loss: 1.3889, G loss: 0.7438\n",
      "[644/1762] D loss: 1.3352, G loss: 0.6961\n",
      "[724/1762] D loss: 1.4036, G loss: 0.8411\n",
      "[804/1762] D loss: 1.2475, G loss: 0.9776\n",
      "[884/1762] D loss: 1.0912, G loss: 0.9463\n",
      "[964/1762] D loss: 1.3865, G loss: 0.6861\n",
      "[1044/1762] D loss: 1.3519, G loss: 0.7024\n",
      "[1124/1762] D loss: 1.4037, G loss: 0.6945\n",
      "[1204/1762] D loss: 1.3980, G loss: 0.6489\n",
      "[1284/1762] D loss: 1.3911, G loss: 0.6723\n",
      "[1364/1762] D loss: 1.4140, G loss: 0.5415\n",
      "[1444/1762] D loss: 1.3624, G loss: 0.7793\n",
      "[1524/1762] D loss: 1.3869, G loss: 0.5868\n",
      "[1604/1762] D loss: 1.3943, G loss: 0.7958\n",
      "[1684/1762] D loss: 1.4199, G loss: 0.8742\n",
      "[1762/1762] D loss: 1.3916, G loss: 0.7083\n",
      "train error: \n",
      " D loss: 1.370336, G loss: 0.670199, D accuracy: 53.3%, cell accuracy: 99.9%, board accuracy: 90.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.364078, G loss: 0.674349, D accuracy: 54.0%, cell accuracy: 99.8%, board accuracy: 86.6% \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2869, G loss: 0.7267\n",
      "[84/1762] D loss: 1.3887, G loss: 0.7992\n",
      "[164/1762] D loss: 1.3865, G loss: 0.6046\n",
      "[244/1762] D loss: 1.4314, G loss: 0.9876\n",
      "[324/1762] D loss: 1.3946, G loss: 0.6598\n",
      "[404/1762] D loss: 1.3901, G loss: 0.7702\n",
      "[484/1762] D loss: 1.4039, G loss: 0.7275\n",
      "[564/1762] D loss: 1.3206, G loss: 0.6463\n",
      "[644/1762] D loss: 1.3912, G loss: 0.6714\n",
      "[724/1762] D loss: 1.3908, G loss: 0.6873\n",
      "[804/1762] D loss: 1.4745, G loss: 0.5181\n",
      "[884/1762] D loss: 1.3920, G loss: 0.7405\n",
      "[964/1762] D loss: 1.3873, G loss: 0.6708\n",
      "[1044/1762] D loss: 1.3944, G loss: 0.7409\n",
      "[1124/1762] D loss: 1.3948, G loss: 0.6307\n",
      "[1204/1762] D loss: 1.4117, G loss: 0.6857\n",
      "[1284/1762] D loss: 1.3828, G loss: 0.6476\n",
      "[1364/1762] D loss: 1.1478, G loss: 0.8172\n",
      "[1444/1762] D loss: 1.4049, G loss: 0.6498\n",
      "[1524/1762] D loss: 1.4000, G loss: 0.8705\n",
      "[1604/1762] D loss: 1.4008, G loss: 0.7717\n",
      "[1684/1762] D loss: 1.1651, G loss: 0.8568\n",
      "[1762/1762] D loss: 1.3495, G loss: 0.6444\n",
      "train error: \n",
      " D loss: 1.355452, G loss: 0.696567, D accuracy: 53.7%, cell accuracy: 99.9%, board accuracy: 91.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.345521, G loss: 0.700771, D accuracy: 55.1%, cell accuracy: 99.8%, board accuracy: 87.3% \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3912, G loss: 0.6848\n",
      "[84/1762] D loss: 1.3908, G loss: 0.6798\n",
      "[164/1762] D loss: 1.3982, G loss: 0.6940\n",
      "[244/1762] D loss: 1.4222, G loss: 0.7717\n",
      "[324/1762] D loss: 1.3684, G loss: 0.7247\n",
      "[404/1762] D loss: 1.3736, G loss: 0.9770\n",
      "[484/1762] D loss: 1.2521, G loss: 0.7313\n",
      "[564/1762] D loss: 1.4525, G loss: 0.5436\n",
      "[644/1762] D loss: 1.3401, G loss: 0.7173\n",
      "[724/1762] D loss: 1.3956, G loss: 0.6944\n",
      "[804/1762] D loss: 1.3737, G loss: 0.6337\n",
      "[884/1762] D loss: 1.4558, G loss: 0.6098\n",
      "[964/1762] D loss: 1.4256, G loss: 0.7106\n",
      "[1044/1762] D loss: 1.4370, G loss: 0.7531\n",
      "[1124/1762] D loss: 1.3491, G loss: 0.6666\n",
      "[1204/1762] D loss: 1.4201, G loss: 0.6379\n",
      "[1284/1762] D loss: 1.3608, G loss: 0.7826\n",
      "[1364/1762] D loss: 1.3815, G loss: 0.8352\n",
      "[1444/1762] D loss: 1.3343, G loss: 0.6436\n",
      "[1524/1762] D loss: 1.3542, G loss: 0.8699\n",
      "[1604/1762] D loss: 1.4185, G loss: 0.8296\n",
      "[1684/1762] D loss: 1.3394, G loss: 0.8442\n",
      "[1762/1762] D loss: 1.5171, G loss: 0.9641\n",
      "train error: \n",
      " D loss: 1.409331, G loss: 0.790607, D accuracy: 50.0%, cell accuracy: 99.8%, board accuracy: 83.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.413756, G loss: 0.799000, D accuracy: 48.8%, cell accuracy: 99.8%, board accuracy: 78.2% \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3985, G loss: 0.6348\n",
      "[84/1762] D loss: 1.4933, G loss: 0.5746\n",
      "[164/1762] D loss: 1.3215, G loss: 0.8515\n",
      "[244/1762] D loss: 1.4330, G loss: 0.6463\n",
      "[324/1762] D loss: 1.4116, G loss: 0.5901\n",
      "[404/1762] D loss: 1.3914, G loss: 0.7163\n",
      "[484/1762] D loss: 1.4109, G loss: 0.7434\n",
      "[564/1762] D loss: 1.3914, G loss: 0.5858\n",
      "[644/1762] D loss: 1.4530, G loss: 0.5459\n",
      "[724/1762] D loss: 1.4090, G loss: 0.7392\n",
      "[804/1762] D loss: 1.4013, G loss: 0.6628\n",
      "[884/1762] D loss: 1.3884, G loss: 0.7612\n",
      "[964/1762] D loss: 1.3962, G loss: 0.7513\n",
      "[1044/1762] D loss: 1.3999, G loss: 0.7659\n",
      "[1124/1762] D loss: 1.3781, G loss: 0.7048\n",
      "[1204/1762] D loss: 1.3958, G loss: 0.6751\n",
      "[1284/1762] D loss: 1.4262, G loss: 0.6237\n",
      "[1364/1762] D loss: 1.4030, G loss: 0.7503\n",
      "[1444/1762] D loss: 1.3175, G loss: 0.6516\n",
      "[1524/1762] D loss: 1.3880, G loss: 0.6526\n",
      "[1604/1762] D loss: 1.2493, G loss: 0.7401\n",
      "[1684/1762] D loss: 1.3662, G loss: 0.7143\n",
      "[1762/1762] D loss: 1.2077, G loss: 0.7765\n",
      "train error: \n",
      " D loss: 1.398815, G loss: 0.785477, D accuracy: 49.7%, cell accuracy: 99.8%, board accuracy: 81.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.388230, G loss: 0.793320, D accuracy: 51.4%, cell accuracy: 99.8%, board accuracy: 80.2% \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4618, G loss: 0.7578\n",
      "[84/1762] D loss: 1.4046, G loss: 0.7419\n",
      "[164/1762] D loss: 1.4959, G loss: 0.6664\n",
      "[244/1762] D loss: 1.4941, G loss: 0.5819\n",
      "[324/1762] D loss: 1.3917, G loss: 0.7651\n",
      "[404/1762] D loss: 1.4788, G loss: 0.6295\n",
      "[484/1762] D loss: 1.3155, G loss: 0.6647\n",
      "[564/1762] D loss: 1.3351, G loss: 0.7952\n",
      "[644/1762] D loss: 1.3932, G loss: 0.6094\n",
      "[724/1762] D loss: 1.4115, G loss: 0.6313\n",
      "[804/1762] D loss: 1.3659, G loss: 0.6472\n",
      "[884/1762] D loss: 1.3993, G loss: 0.6611\n",
      "[964/1762] D loss: 1.3831, G loss: 0.7385\n",
      "[1044/1762] D loss: 1.3914, G loss: 0.6377\n",
      "[1124/1762] D loss: 1.4082, G loss: 0.7135\n",
      "[1204/1762] D loss: 1.5388, G loss: 0.7355\n",
      "[1284/1762] D loss: 1.3647, G loss: 0.6812\n",
      "[1364/1762] D loss: 1.4178, G loss: 0.5971\n",
      "[1444/1762] D loss: 1.3597, G loss: 0.7258\n",
      "[1524/1762] D loss: 1.3877, G loss: 0.6726\n",
      "[1604/1762] D loss: 1.3943, G loss: 0.7329\n",
      "[1684/1762] D loss: 1.3884, G loss: 0.6827\n",
      "[1762/1762] D loss: 1.3251, G loss: 0.8006\n",
      "train error: \n",
      " D loss: 1.381381, G loss: 0.742814, D accuracy: 50.9%, cell accuracy: 99.8%, board accuracy: 74.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.369377, G loss: 0.750742, D accuracy: 53.0%, cell accuracy: 99.7%, board accuracy: 73.6% \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2751, G loss: 0.7779\n",
      "[84/1762] D loss: 1.2649, G loss: 0.6513\n",
      "[164/1762] D loss: 1.4107, G loss: 0.8032\n",
      "[244/1762] D loss: 1.3974, G loss: 0.7041\n",
      "[324/1762] D loss: 1.2755, G loss: 0.7026\n",
      "[404/1762] D loss: 1.4345, G loss: 0.6742\n",
      "[484/1762] D loss: 1.3987, G loss: 0.6857\n",
      "[564/1762] D loss: 1.4184, G loss: 0.5439\n",
      "[644/1762] D loss: 1.3999, G loss: 0.8426\n",
      "[724/1762] D loss: 1.3904, G loss: 0.6442\n",
      "[804/1762] D loss: 1.4014, G loss: 0.8144\n",
      "[884/1762] D loss: 1.3865, G loss: 0.6542\n",
      "[964/1762] D loss: 1.3960, G loss: 0.6063\n",
      "[1044/1762] D loss: 1.4006, G loss: 0.8313\n",
      "[1124/1762] D loss: 1.4245, G loss: 0.8053\n",
      "[1204/1762] D loss: 1.2837, G loss: 0.5980\n",
      "[1284/1762] D loss: 1.3603, G loss: 0.8197\n",
      "[1364/1762] D loss: 1.5058, G loss: 0.6243\n",
      "[1444/1762] D loss: 1.3023, G loss: 0.8611\n",
      "[1524/1762] D loss: 1.3992, G loss: 0.6678\n",
      "[1604/1762] D loss: 1.4375, G loss: 0.8533\n",
      "[1684/1762] D loss: 1.3920, G loss: 0.7184\n",
      "[1762/1762] D loss: 1.2433, G loss: 0.7320\n",
      "train error: \n",
      " D loss: 1.344534, G loss: 0.669328, D accuracy: 56.6%, cell accuracy: 99.6%, board accuracy: 42.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.340907, G loss: 0.679006, D accuracy: 55.6%, cell accuracy: 99.6%, board accuracy: 37.3% \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2751, G loss: 0.6475\n",
      "[84/1762] D loss: 1.2769, G loss: 0.8373\n",
      "[164/1762] D loss: 1.3858, G loss: 0.6207\n",
      "[244/1762] D loss: 1.4039, G loss: 0.6028\n",
      "[324/1762] D loss: 1.3262, G loss: 0.7851\n",
      "[404/1762] D loss: 1.3928, G loss: 0.5794\n",
      "[484/1762] D loss: 1.3967, G loss: 0.7758\n",
      "[564/1762] D loss: 1.4107, G loss: 0.7089\n",
      "[644/1762] D loss: 1.3657, G loss: 0.7130\n",
      "[724/1762] D loss: 1.3669, G loss: 0.8149\n",
      "[804/1762] D loss: 1.3137, G loss: 0.7197\n",
      "[884/1762] D loss: 1.3891, G loss: 0.7188\n",
      "[964/1762] D loss: 1.4120, G loss: 0.7410\n",
      "[1044/1762] D loss: 1.3881, G loss: 0.6913\n",
      "[1124/1762] D loss: 1.3995, G loss: 0.6337\n",
      "[1204/1762] D loss: 1.2967, G loss: 0.7905\n",
      "[1284/1762] D loss: 1.4016, G loss: 0.6156\n",
      "[1364/1762] D loss: 1.3984, G loss: 0.7875\n",
      "[1444/1762] D loss: 1.3910, G loss: 0.7153\n",
      "[1524/1762] D loss: 1.4010, G loss: 0.7674\n",
      "[1604/1762] D loss: 1.3973, G loss: 0.7514\n",
      "[1684/1762] D loss: 1.3979, G loss: 0.7787\n",
      "[1762/1762] D loss: 1.3871, G loss: 0.7079\n",
      "train error: \n",
      " D loss: 1.365056, G loss: 0.757781, D accuracy: 53.2%, cell accuracy: 99.8%, board accuracy: 89.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.356496, G loss: 0.763801, D accuracy: 54.9%, cell accuracy: 99.8%, board accuracy: 86.1% \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4043, G loss: 0.7883\n",
      "[84/1762] D loss: 1.4183, G loss: 0.7622\n",
      "[164/1762] D loss: 1.2833, G loss: 0.7559\n",
      "[244/1762] D loss: 1.3845, G loss: 0.7934\n",
      "[324/1762] D loss: 1.4266, G loss: 0.6432\n",
      "[404/1762] D loss: 1.3913, G loss: 0.7875\n",
      "[484/1762] D loss: 1.4259, G loss: 0.6440\n",
      "[564/1762] D loss: 1.3938, G loss: 0.8251\n",
      "[644/1762] D loss: 1.2585, G loss: 0.7806\n",
      "[724/1762] D loss: 1.3916, G loss: 0.6741\n",
      "[804/1762] D loss: 1.4048, G loss: 0.7794\n",
      "[884/1762] D loss: 1.3964, G loss: 0.6338\n",
      "[964/1762] D loss: 1.4006, G loss: 0.7737\n",
      "[1044/1762] D loss: 1.2594, G loss: 0.7260\n",
      "[1124/1762] D loss: 1.4311, G loss: 0.6879\n",
      "[1204/1762] D loss: 1.1648, G loss: 0.7863\n",
      "[1284/1762] D loss: 1.3922, G loss: 0.7226\n",
      "[1364/1762] D loss: 1.3946, G loss: 0.6533\n",
      "[1444/1762] D loss: 1.4142, G loss: 0.7871\n",
      "[1524/1762] D loss: 1.4217, G loss: 0.7097\n",
      "[1604/1762] D loss: 1.3886, G loss: 0.7043\n",
      "[1684/1762] D loss: 1.2867, G loss: 0.7484\n",
      "[1762/1762] D loss: 1.3898, G loss: 0.6930\n",
      "train error: \n",
      " D loss: 1.356774, G loss: 0.672616, D accuracy: 53.4%, cell accuracy: 99.9%, board accuracy: 90.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.345598, G loss: 0.680429, D accuracy: 54.4%, cell accuracy: 99.8%, board accuracy: 87.5% \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3906, G loss: 0.6618\n",
      "[84/1762] D loss: 1.3892, G loss: 0.6473\n",
      "[164/1762] D loss: 1.2283, G loss: 0.7589\n",
      "[244/1762] D loss: 1.3471, G loss: 0.7720\n",
      "[324/1762] D loss: 1.3887, G loss: 0.5812\n",
      "[404/1762] D loss: 1.0863, G loss: 0.9253\n",
      "[484/1762] D loss: 1.3912, G loss: 0.7504\n",
      "[564/1762] D loss: 1.3996, G loss: 0.6411\n",
      "[644/1762] D loss: 1.3964, G loss: 0.7099\n",
      "[724/1762] D loss: 1.3938, G loss: 0.6420\n",
      "[804/1762] D loss: 1.3902, G loss: 0.8586\n",
      "[884/1762] D loss: 1.4158, G loss: 0.5157\n",
      "[964/1762] D loss: 1.3886, G loss: 0.7558\n",
      "[1044/1762] D loss: 1.2191, G loss: 0.7540\n",
      "[1124/1762] D loss: 1.4455, G loss: 0.6179\n",
      "[1204/1762] D loss: 1.4657, G loss: 0.6391\n",
      "[1284/1762] D loss: 1.3986, G loss: 0.6884\n",
      "[1364/1762] D loss: 1.3946, G loss: 0.8019\n",
      "[1444/1762] D loss: 1.2490, G loss: 0.7310\n",
      "[1524/1762] D loss: 1.4081, G loss: 0.8174\n",
      "[1604/1762] D loss: 1.2543, G loss: 0.8144\n",
      "[1684/1762] D loss: 1.3889, G loss: 0.6638\n",
      "[1762/1762] D loss: 1.0537, G loss: 0.7842\n",
      "train error: \n",
      " D loss: 1.349585, G loss: 0.697169, D accuracy: 53.5%, cell accuracy: 99.9%, board accuracy: 89.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.337212, G loss: 0.704433, D accuracy: 54.8%, cell accuracy: 99.8%, board accuracy: 87.3% \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3926, G loss: 0.6142\n",
      "[84/1762] D loss: 1.3895, G loss: 0.7435\n",
      "[164/1762] D loss: 1.4008, G loss: 0.6836\n",
      "[244/1762] D loss: 1.4031, G loss: 0.6404\n",
      "[324/1762] D loss: 1.3937, G loss: 0.7693\n",
      "[404/1762] D loss: 1.3934, G loss: 0.6899\n",
      "[484/1762] D loss: 1.3920, G loss: 0.6055\n",
      "[564/1762] D loss: 1.2274, G loss: 0.8085\n",
      "[644/1762] D loss: 1.1873, G loss: 0.8682\n",
      "[724/1762] D loss: 1.2465, G loss: 0.7959\n",
      "[804/1762] D loss: 1.3914, G loss: 0.7143\n",
      "[884/1762] D loss: 1.2175, G loss: 0.6362\n",
      "[964/1762] D loss: 1.3960, G loss: 0.7720\n",
      "[1044/1762] D loss: 1.3912, G loss: 0.6443\n",
      "[1124/1762] D loss: 1.3963, G loss: 0.6939\n",
      "[1204/1762] D loss: 1.3915, G loss: 0.6362\n",
      "[1284/1762] D loss: 1.2010, G loss: 0.9347\n",
      "[1364/1762] D loss: 1.2504, G loss: 0.6376\n",
      "[1444/1762] D loss: 1.3957, G loss: 0.7892\n",
      "[1524/1762] D loss: 1.3929, G loss: 0.6761\n",
      "[1604/1762] D loss: 1.3926, G loss: 0.6954\n",
      "[1684/1762] D loss: 2.0539, G loss: 0.5022\n",
      "[1762/1762] D loss: 2.3236, G loss: 0.2629\n",
      "train error: \n",
      " D loss: 1.826261, G loss: 0.529809, D accuracy: 14.1%, cell accuracy: 98.0%, board accuracy: 0.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.808055, G loss: 0.545137, D accuracy: 15.0%, cell accuracy: 98.1%, board accuracy: 1.1% \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.8800, G loss: 0.6067\n",
      "[84/1762] D loss: 1.6669, G loss: 0.5674\n",
      "[164/1762] D loss: 1.3930, G loss: 1.0324\n",
      "[244/1762] D loss: 1.3550, G loss: 0.8682\n",
      "[324/1762] D loss: 1.4262, G loss: 0.8385\n",
      "[404/1762] D loss: 1.4453, G loss: 0.5770\n",
      "[484/1762] D loss: 1.4388, G loss: 0.6235\n",
      "[564/1762] D loss: 1.3990, G loss: 0.6955\n",
      "[644/1762] D loss: 1.4094, G loss: 0.6766\n",
      "[724/1762] D loss: 1.3965, G loss: 0.6095\n",
      "[804/1762] D loss: 1.4130, G loss: 0.7727\n",
      "[884/1762] D loss: 1.3689, G loss: 0.6894\n",
      "[964/1762] D loss: 1.3896, G loss: 0.6359\n",
      "[1044/1762] D loss: 1.4135, G loss: 0.6951\n",
      "[1124/1762] D loss: 1.4151, G loss: 0.7786\n",
      "[1204/1762] D loss: 1.3914, G loss: 0.6617\n",
      "[1284/1762] D loss: 1.4111, G loss: 0.5951\n",
      "[1364/1762] D loss: 1.4012, G loss: 0.6104\n",
      "[1444/1762] D loss: 1.3909, G loss: 0.6509\n",
      "[1524/1762] D loss: 1.3898, G loss: 0.6084\n",
      "[1604/1762] D loss: 1.3544, G loss: 0.6836\n",
      "[1684/1762] D loss: 1.3885, G loss: 0.7229\n",
      "[1762/1762] D loss: 1.3934, G loss: 0.7260\n",
      "train error: \n",
      " D loss: 1.370396, G loss: 0.642918, D accuracy: 53.1%, cell accuracy: 99.9%, board accuracy: 91.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.364091, G loss: 0.646373, D accuracy: 54.2%, cell accuracy: 99.8%, board accuracy: 88.4% \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3408, G loss: 0.6623\n",
      "[84/1762] D loss: 1.2924, G loss: 0.7125\n",
      "[164/1762] D loss: 1.3872, G loss: 0.6901\n",
      "[244/1762] D loss: 1.3880, G loss: 0.7055\n",
      "[324/1762] D loss: 1.3240, G loss: 0.7395\n",
      "[404/1762] D loss: 1.2844, G loss: 0.6635\n",
      "[484/1762] D loss: 1.2702, G loss: 0.7262\n",
      "[564/1762] D loss: 1.2746, G loss: 0.7783\n",
      "[644/1762] D loss: 1.3907, G loss: 0.6565\n",
      "[724/1762] D loss: 1.4111, G loss: 0.8478\n",
      "[804/1762] D loss: 1.2529, G loss: 0.7005\n",
      "[884/1762] D loss: 1.3916, G loss: 0.7576\n",
      "[964/1762] D loss: 1.3900, G loss: 0.6911\n",
      "[1044/1762] D loss: 1.3935, G loss: 0.7051\n",
      "[1124/1762] D loss: 1.3907, G loss: 0.6198\n",
      "[1204/1762] D loss: 1.2432, G loss: 0.7480\n",
      "[1284/1762] D loss: 1.3636, G loss: 0.7675\n",
      "[1364/1762] D loss: 1.3926, G loss: 0.7350\n",
      "[1444/1762] D loss: 1.2375, G loss: 0.9160\n",
      "[1524/1762] D loss: 1.3427, G loss: 0.6269\n",
      "[1604/1762] D loss: 1.3881, G loss: 0.7388\n",
      "[1684/1762] D loss: 1.3928, G loss: 0.6208\n",
      "[1762/1762] D loss: 1.3936, G loss: 0.7327\n",
      "train error: \n",
      " D loss: 1.353411, G loss: 0.701088, D accuracy: 53.6%, cell accuracy: 99.9%, board accuracy: 91.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.344912, G loss: 0.706369, D accuracy: 54.4%, cell accuracy: 99.8%, board accuracy: 89.3% \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3916, G loss: 0.6787\n",
      "[84/1762] D loss: 1.3949, G loss: 0.6604\n",
      "[164/1762] D loss: 1.3116, G loss: 0.7803\n",
      "[244/1762] D loss: 1.2492, G loss: 0.8541\n",
      "[324/1762] D loss: 1.3914, G loss: 0.6190\n",
      "[404/1762] D loss: 1.3063, G loss: 0.8561\n",
      "[484/1762] D loss: 1.3118, G loss: 0.6939\n",
      "[564/1762] D loss: 1.4304, G loss: 0.7614\n",
      "[644/1762] D loss: 1.4028, G loss: 0.6846\n",
      "[724/1762] D loss: 1.4586, G loss: 0.6973\n",
      "[804/1762] D loss: 1.3755, G loss: 0.7493\n",
      "[884/1762] D loss: 1.3087, G loss: 0.7200\n",
      "[964/1762] D loss: 1.3102, G loss: 0.6720\n",
      "[1044/1762] D loss: 1.3066, G loss: 0.7871\n",
      "[1124/1762] D loss: 1.3928, G loss: 0.7459\n",
      "[1204/1762] D loss: 1.4045, G loss: 0.6486\n",
      "[1284/1762] D loss: 1.4165, G loss: 0.7438\n",
      "[1364/1762] D loss: 1.3969, G loss: 0.7775\n",
      "[1444/1762] D loss: 1.3969, G loss: 0.6035\n",
      "[1524/1762] D loss: 1.3995, G loss: 0.7150\n",
      "[1604/1762] D loss: 1.3912, G loss: 0.6220\n",
      "[1684/1762] D loss: 1.4005, G loss: 0.6628\n",
      "[1762/1762] D loss: 1.2003, G loss: 0.7543\n",
      "train error: \n",
      " D loss: 1.381564, G loss: 0.596102, D accuracy: 52.1%, cell accuracy: 99.8%, board accuracy: 88.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.374976, G loss: 0.604127, D accuracy: 53.2%, cell accuracy: 99.8%, board accuracy: 86.1% \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3996, G loss: 0.5884\n",
      "[84/1762] D loss: 1.3885, G loss: 0.7047\n",
      "[164/1762] D loss: 1.3878, G loss: 0.7073\n",
      "[244/1762] D loss: 1.4049, G loss: 0.6002\n",
      "[324/1762] D loss: 1.3889, G loss: 0.6852\n",
      "[404/1762] D loss: 1.2853, G loss: 0.7553\n",
      "[484/1762] D loss: 1.2932, G loss: 0.7699\n",
      "[564/1762] D loss: 1.3905, G loss: 0.6387\n",
      "[644/1762] D loss: 1.3891, G loss: 0.7304\n",
      "[724/1762] D loss: 1.4107, G loss: 0.5730\n",
      "[804/1762] D loss: 1.4043, G loss: 0.6865\n",
      "[884/1762] D loss: 1.3874, G loss: 0.7613\n",
      "[964/1762] D loss: 1.3933, G loss: 0.6142\n",
      "[1044/1762] D loss: 1.3870, G loss: 0.7252\n",
      "[1124/1762] D loss: 1.3905, G loss: 0.6859\n",
      "[1204/1762] D loss: 1.3906, G loss: 0.7062\n",
      "[1284/1762] D loss: 1.3230, G loss: 0.6721\n",
      "[1364/1762] D loss: 1.3903, G loss: 0.7698\n",
      "[1444/1762] D loss: 1.3880, G loss: 0.6819\n",
      "[1524/1762] D loss: 1.3929, G loss: 0.7060\n",
      "[1604/1762] D loss: 1.2646, G loss: 0.7107\n",
      "[1684/1762] D loss: 1.3864, G loss: 0.6712\n",
      "[1762/1762] D loss: 1.2228, G loss: 0.8354\n",
      "train error: \n",
      " D loss: 1.354171, G loss: 0.736664, D accuracy: 53.1%, cell accuracy: 99.8%, board accuracy: 89.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.343362, G loss: 0.745402, D accuracy: 54.2%, cell accuracy: 99.8%, board accuracy: 87.7% \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3802, G loss: 0.7409\n",
      "[84/1762] D loss: 1.3949, G loss: 0.6441\n",
      "[164/1762] D loss: 1.3661, G loss: 0.7538\n",
      "[244/1762] D loss: 1.3949, G loss: 0.6668\n",
      "[324/1762] D loss: 1.2974, G loss: 0.7280\n",
      "[404/1762] D loss: 1.2585, G loss: 0.7947\n",
      "[484/1762] D loss: 1.2444, G loss: 0.8508\n",
      "[564/1762] D loss: 1.3948, G loss: 0.7187\n",
      "[644/1762] D loss: 1.3912, G loss: 0.6824\n",
      "[724/1762] D loss: 1.4123, G loss: 0.5569\n",
      "[804/1762] D loss: 1.3938, G loss: 0.7156\n",
      "[884/1762] D loss: 1.6960, G loss: 0.5941\n",
      "[964/1762] D loss: 1.4711, G loss: 0.6155\n",
      "[1044/1762] D loss: 1.3654, G loss: 0.8314\n",
      "[1124/1762] D loss: 1.3240, G loss: 0.8296\n",
      "[1204/1762] D loss: 1.3060, G loss: 0.8149\n",
      "[1284/1762] D loss: 1.3910, G loss: 0.6257\n",
      "[1364/1762] D loss: 1.4054, G loss: 0.7498\n",
      "[1444/1762] D loss: 1.4222, G loss: 0.6186\n",
      "[1524/1762] D loss: 1.5035, G loss: 0.5411\n",
      "[1604/1762] D loss: 1.4346, G loss: 0.8689\n",
      "[1684/1762] D loss: 1.5161, G loss: 0.6033\n",
      "[1762/1762] D loss: 1.4044, G loss: 0.5166\n",
      "train error: \n",
      " D loss: 1.410593, G loss: 0.592013, D accuracy: 49.5%, cell accuracy: 99.8%, board accuracy: 89.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.413133, G loss: 0.594854, D accuracy: 49.4%, cell accuracy: 99.8%, board accuracy: 88.2% \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3880, G loss: 0.6154\n",
      "[84/1762] D loss: 1.3902, G loss: 0.6721\n",
      "[164/1762] D loss: 1.4039, G loss: 0.8213\n",
      "[244/1762] D loss: 1.4575, G loss: 0.8537\n",
      "[324/1762] D loss: 1.3972, G loss: 0.6019\n",
      "[404/1762] D loss: 1.3960, G loss: 0.6750\n",
      "[484/1762] D loss: 1.3958, G loss: 0.6923\n",
      "[564/1762] D loss: 1.3908, G loss: 0.6594\n",
      "[644/1762] D loss: 1.3959, G loss: 0.8226\n",
      "[724/1762] D loss: 1.3884, G loss: 0.6954\n",
      "[804/1762] D loss: 1.3988, G loss: 0.6315\n",
      "[884/1762] D loss: 1.3904, G loss: 0.6702\n",
      "[964/1762] D loss: 1.3963, G loss: 0.6356\n",
      "[1044/1762] D loss: 1.3896, G loss: 0.7040\n",
      "[1124/1762] D loss: 1.3070, G loss: 0.6823\n",
      "[1204/1762] D loss: 1.3904, G loss: 0.7450\n",
      "[1284/1762] D loss: 1.3895, G loss: 0.7244\n",
      "[1364/1762] D loss: 1.3955, G loss: 0.6583\n",
      "[1444/1762] D loss: 1.3912, G loss: 0.7184\n",
      "[1524/1762] D loss: 1.4029, G loss: 0.6938\n",
      "[1604/1762] D loss: 1.4036, G loss: 0.6683\n",
      "[1684/1762] D loss: 1.3107, G loss: 0.7134\n",
      "[1762/1762] D loss: 1.4012, G loss: 0.6804\n",
      "train error: \n",
      " D loss: 1.366362, G loss: 0.722929, D accuracy: 53.0%, cell accuracy: 99.8%, board accuracy: 90.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.363245, G loss: 0.726721, D accuracy: 53.4%, cell accuracy: 99.8%, board accuracy: 88.4% \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3887, G loss: 0.7773\n",
      "[84/1762] D loss: 1.3972, G loss: 0.7649\n",
      "[164/1762] D loss: 1.2938, G loss: 0.6830\n",
      "[244/1762] D loss: 1.4039, G loss: 0.7855\n",
      "[324/1762] D loss: 1.3976, G loss: 0.5956\n",
      "[404/1762] D loss: 1.3905, G loss: 0.6820\n",
      "[484/1762] D loss: 1.3873, G loss: 0.6978\n",
      "[564/1762] D loss: 1.4286, G loss: 0.7067\n",
      "[644/1762] D loss: 1.3943, G loss: 0.6218\n",
      "[724/1762] D loss: 1.3861, G loss: 0.7462\n",
      "[804/1762] D loss: 1.3864, G loss: 0.6050\n",
      "[884/1762] D loss: 1.3971, G loss: 0.7791\n",
      "[964/1762] D loss: 1.3869, G loss: 0.6737\n",
      "[1044/1762] D loss: 1.3902, G loss: 0.6564\n",
      "[1124/1762] D loss: 1.3890, G loss: 0.7204\n",
      "[1204/1762] D loss: 1.3935, G loss: 0.6769\n",
      "[1284/1762] D loss: 1.3984, G loss: 0.6060\n",
      "[1364/1762] D loss: 1.3928, G loss: 0.7066\n",
      "[1444/1762] D loss: 1.3975, G loss: 0.7170\n",
      "[1524/1762] D loss: 1.3899, G loss: 0.7197\n",
      "[1604/1762] D loss: 1.2365, G loss: 0.8138\n",
      "[1684/1762] D loss: 1.3884, G loss: 0.6772\n",
      "[1762/1762] D loss: 1.3949, G loss: 0.6720\n",
      "train error: \n",
      " D loss: 1.361054, G loss: 0.649909, D accuracy: 53.4%, cell accuracy: 99.9%, board accuracy: 90.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.354085, G loss: 0.654354, D accuracy: 54.5%, cell accuracy: 99.8%, board accuracy: 88.9% \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2429, G loss: 0.8226\n",
      "[84/1762] D loss: 1.3898, G loss: 0.5969\n",
      "[164/1762] D loss: 1.3883, G loss: 0.6952\n",
      "[244/1762] D loss: 1.3972, G loss: 0.6529\n",
      "[324/1762] D loss: 1.4008, G loss: 0.6453\n",
      "[404/1762] D loss: 1.3976, G loss: 0.7650\n",
      "[484/1762] D loss: 1.3887, G loss: 0.6954\n",
      "[564/1762] D loss: 1.3916, G loss: 0.6329\n",
      "[644/1762] D loss: 1.2197, G loss: 0.8117\n",
      "[724/1762] D loss: 1.3882, G loss: 0.6714\n",
      "[804/1762] D loss: 1.2357, G loss: 0.7944\n",
      "[884/1762] D loss: 1.4223, G loss: 0.8488\n",
      "[964/1762] D loss: 1.2216, G loss: 0.8693\n",
      "[1044/1762] D loss: 1.2214, G loss: 0.7468\n",
      "[1124/1762] D loss: 1.4123, G loss: 0.5952\n",
      "[1204/1762] D loss: 1.3937, G loss: 0.6663\n",
      "[1284/1762] D loss: 1.3955, G loss: 0.7676\n",
      "[1364/1762] D loss: 1.3936, G loss: 0.7135\n",
      "[1444/1762] D loss: 1.3946, G loss: 0.7997\n",
      "[1524/1762] D loss: 1.2683, G loss: 0.8374\n",
      "[1604/1762] D loss: 1.3920, G loss: 0.6903\n",
      "[1684/1762] D loss: 1.0721, G loss: 0.8384\n",
      "[1762/1762] D loss: 1.0723, G loss: 0.9779\n",
      "train error: \n",
      " D loss: 1.354956, G loss: 0.820471, D accuracy: 53.1%, cell accuracy: 99.8%, board accuracy: 90.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.344855, G loss: 0.830333, D accuracy: 53.9%, cell accuracy: 99.8%, board accuracy: 88.2% \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3961, G loss: 0.7416\n",
      "[84/1762] D loss: 1.2149, G loss: 0.7657\n",
      "[164/1762] D loss: 1.2182, G loss: 0.7821\n",
      "[244/1762] D loss: 1.4406, G loss: 0.7370\n",
      "[324/1762] D loss: 1.5367, G loss: 0.5813\n",
      "[404/1762] D loss: 1.3396, G loss: 0.5882\n",
      "[484/1762] D loss: 1.2308, G loss: 0.7593\n",
      "[564/1762] D loss: 1.3801, G loss: 0.8264\n",
      "[644/1762] D loss: 1.4166, G loss: 0.7390\n",
      "[724/1762] D loss: 1.3999, G loss: 0.7389\n",
      "[804/1762] D loss: 1.4007, G loss: 0.8106\n",
      "[884/1762] D loss: 1.3931, G loss: 0.7359\n",
      "[964/1762] D loss: 1.3032, G loss: 0.6947\n",
      "[1044/1762] D loss: 1.3887, G loss: 0.7147\n",
      "[1124/1762] D loss: 1.3955, G loss: 0.7219\n",
      "[1204/1762] D loss: 1.4141, G loss: 0.5204\n",
      "[1284/1762] D loss: 1.3871, G loss: 0.6919\n",
      "[1364/1762] D loss: 1.3067, G loss: 0.8767\n",
      "[1444/1762] D loss: 1.3906, G loss: 0.6847\n",
      "[1524/1762] D loss: 1.3900, G loss: 0.6030\n",
      "[1604/1762] D loss: 1.4021, G loss: 0.6642\n",
      "[1684/1762] D loss: 1.4024, G loss: 0.7916\n",
      "[1762/1762] D loss: 1.1406, G loss: 0.7610\n",
      "train error: \n",
      " D loss: 1.360630, G loss: 0.660363, D accuracy: 53.5%, cell accuracy: 99.9%, board accuracy: 90.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.354566, G loss: 0.667151, D accuracy: 54.7%, cell accuracy: 99.8%, board accuracy: 88.0% \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2760, G loss: 0.7515\n",
      "[84/1762] D loss: 1.2707, G loss: 0.8226\n",
      "[164/1762] D loss: 1.2137, G loss: 0.8091\n",
      "[244/1762] D loss: 1.3895, G loss: 0.6829\n",
      "[324/1762] D loss: 1.3982, G loss: 0.6985\n",
      "[404/1762] D loss: 1.2794, G loss: 0.7473\n",
      "[484/1762] D loss: 1.2547, G loss: 0.7499\n",
      "[564/1762] D loss: 1.4280, G loss: 0.6660\n",
      "[644/1762] D loss: 1.3876, G loss: 0.7215\n",
      "[724/1762] D loss: 1.3927, G loss: 0.5940\n",
      "[804/1762] D loss: 1.3878, G loss: 0.7564\n",
      "[884/1762] D loss: 1.2288, G loss: 0.8202\n",
      "[964/1762] D loss: 1.3874, G loss: 0.7536\n",
      "[1044/1762] D loss: 1.3886, G loss: 0.7248\n",
      "[1124/1762] D loss: 1.3911, G loss: 0.7259\n",
      "[1204/1762] D loss: 1.4070, G loss: 0.7281\n",
      "[1284/1762] D loss: 1.3915, G loss: 0.5605\n",
      "[1364/1762] D loss: 1.3914, G loss: 0.6571\n",
      "[1444/1762] D loss: 1.3881, G loss: 0.6772\n",
      "[1524/1762] D loss: 1.3933, G loss: 0.6419\n",
      "[1604/1762] D loss: 1.1127, G loss: 0.8475\n",
      "[1684/1762] D loss: 1.3987, G loss: 0.6216\n",
      "[1762/1762] D loss: 1.3864, G loss: 0.6717\n",
      "train error: \n",
      " D loss: 1.347805, G loss: 0.689493, D accuracy: 53.5%, cell accuracy: 99.8%, board accuracy: 89.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.338306, G loss: 0.697135, D accuracy: 54.9%, cell accuracy: 99.8%, board accuracy: 87.0% \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3867, G loss: 0.6574\n",
      "[84/1762] D loss: 1.3866, G loss: 0.7484\n",
      "[164/1762] D loss: 1.4064, G loss: 0.7775\n",
      "[244/1762] D loss: 1.3928, G loss: 0.6358\n",
      "[324/1762] D loss: 1.4180, G loss: 0.6407\n",
      "[404/1762] D loss: 1.3958, G loss: 0.7770\n",
      "[484/1762] D loss: 1.3895, G loss: 0.7000\n",
      "[564/1762] D loss: 1.4094, G loss: 0.6853\n",
      "[644/1762] D loss: 1.2661, G loss: 0.7146\n",
      "[724/1762] D loss: 1.3947, G loss: 0.6913\n",
      "[804/1762] D loss: 1.3926, G loss: 0.6816\n",
      "[884/1762] D loss: 1.0411, G loss: 0.8612\n",
      "[964/1762] D loss: 1.3884, G loss: 0.7425\n",
      "[1044/1762] D loss: 1.3902, G loss: 0.7706\n",
      "[1124/1762] D loss: 1.3921, G loss: 0.7653\n",
      "[1204/1762] D loss: 1.3886, G loss: 0.7132\n",
      "[1284/1762] D loss: 1.3904, G loss: 0.7337\n",
      "[1364/1762] D loss: 1.3913, G loss: 0.6411\n",
      "[1444/1762] D loss: 1.4027, G loss: 0.6208\n",
      "[1524/1762] D loss: 1.3946, G loss: 0.7493\n",
      "[1604/1762] D loss: 1.4121, G loss: 0.6921\n",
      "[1684/1762] D loss: 1.3905, G loss: 0.7720\n",
      "[1762/1762] D loss: 1.3898, G loss: 0.7420\n",
      "train error: \n",
      " D loss: 1.344879, G loss: 0.743096, D accuracy: 52.6%, cell accuracy: 99.8%, board accuracy: 86.6% \n",
      "\n",
      "test error: \n",
      " D loss: 1.329331, G loss: 0.757985, D accuracy: 54.1%, cell accuracy: 99.8%, board accuracy: 86.6% \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3880, G loss: 0.6902\n",
      "[84/1762] D loss: 1.2294, G loss: 0.8798\n",
      "[164/1762] D loss: 2.1268, G loss: 0.8779\n",
      "[244/1762] D loss: 1.8073, G loss: 0.5906\n",
      "[324/1762] D loss: 1.3335, G loss: 0.9922\n",
      "[404/1762] D loss: 1.2180, G loss: 0.7787\n",
      "[484/1762] D loss: 1.3358, G loss: 0.7105\n",
      "[564/1762] D loss: 1.4960, G loss: 0.6342\n",
      "[644/1762] D loss: 1.6068, G loss: 0.5380\n",
      "[724/1762] D loss: 1.3865, G loss: 0.6445\n",
      "[804/1762] D loss: 1.4130, G loss: 0.7530\n",
      "[884/1762] D loss: 1.4532, G loss: 0.5566\n",
      "[964/1762] D loss: 1.3895, G loss: 0.7158\n",
      "[1044/1762] D loss: 1.3903, G loss: 0.6718\n",
      "[1124/1762] D loss: 1.3897, G loss: 0.7432\n",
      "[1204/1762] D loss: 1.4553, G loss: 0.6888\n",
      "[1284/1762] D loss: 1.3917, G loss: 0.7221\n",
      "[1364/1762] D loss: 1.3917, G loss: 0.6888\n",
      "[1444/1762] D loss: 1.3299, G loss: 0.7063\n",
      "[1524/1762] D loss: 1.3885, G loss: 0.6751\n",
      "[1604/1762] D loss: 1.3881, G loss: 0.7257\n",
      "[1684/1762] D loss: 1.4065, G loss: 0.7619\n",
      "[1762/1762] D loss: 1.3149, G loss: 0.7456\n",
      "train error: \n",
      " D loss: 1.384423, G loss: 0.712894, D accuracy: 52.0%, cell accuracy: 99.9%, board accuracy: 91.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.387919, G loss: 0.718325, D accuracy: 52.7%, cell accuracy: 99.8%, board accuracy: 88.6% \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3878, G loss: 0.7092\n",
      "[84/1762] D loss: 1.2805, G loss: 0.6536\n",
      "[164/1762] D loss: 1.3873, G loss: 0.7320\n",
      "[244/1762] D loss: 1.3850, G loss: 0.6913\n",
      "[324/1762] D loss: 1.4390, G loss: 0.6202\n",
      "[404/1762] D loss: 1.3893, G loss: 0.6495\n",
      "[484/1762] D loss: 1.3871, G loss: 0.7123\n",
      "[564/1762] D loss: 1.4193, G loss: 0.7206\n",
      "[644/1762] D loss: 1.3855, G loss: 0.7008\n",
      "[724/1762] D loss: 1.3875, G loss: 0.6834\n",
      "[804/1762] D loss: 1.3145, G loss: 0.8827\n",
      "[884/1762] D loss: 1.2950, G loss: 0.7379\n",
      "[964/1762] D loss: 1.3863, G loss: 0.7303\n",
      "[1044/1762] D loss: 1.3914, G loss: 0.6610\n",
      "[1124/1762] D loss: 1.3904, G loss: 0.6715\n",
      "[1204/1762] D loss: 1.2024, G loss: 0.7527\n",
      "[1284/1762] D loss: 1.2706, G loss: 0.7284\n",
      "[1364/1762] D loss: 1.3350, G loss: 0.6664\n",
      "[1444/1762] D loss: 1.1894, G loss: 0.7733\n",
      "[1524/1762] D loss: 1.3888, G loss: 0.7336\n",
      "[1604/1762] D loss: 1.3877, G loss: 0.7114\n",
      "[1684/1762] D loss: 1.3846, G loss: 0.7244\n",
      "[1762/1762] D loss: 1.3906, G loss: 0.7084\n",
      "train error: \n",
      " D loss: 1.360153, G loss: 0.718030, D accuracy: 53.2%, cell accuracy: 99.9%, board accuracy: 92.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.355375, G loss: 0.724979, D accuracy: 54.1%, cell accuracy: 99.8%, board accuracy: 90.0% \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3884, G loss: 0.6991\n",
      "[84/1762] D loss: 1.2569, G loss: 0.8009\n",
      "[164/1762] D loss: 1.3439, G loss: 0.7905\n",
      "[244/1762] D loss: 1.3919, G loss: 0.7225\n",
      "[324/1762] D loss: 1.2658, G loss: 0.7863\n",
      "[404/1762] D loss: 1.3907, G loss: 0.6648\n",
      "[484/1762] D loss: 1.3969, G loss: 0.6294\n",
      "[564/1762] D loss: 1.3939, G loss: 0.7534\n",
      "[644/1762] D loss: 1.3908, G loss: 0.7285\n",
      "[724/1762] D loss: 1.3892, G loss: 0.6666\n",
      "[804/1762] D loss: 1.3901, G loss: 0.7674\n",
      "[884/1762] D loss: 1.3890, G loss: 0.7621\n",
      "[964/1762] D loss: 1.3895, G loss: 0.7015\n",
      "[1044/1762] D loss: 1.3871, G loss: 0.6819\n",
      "[1124/1762] D loss: 1.2782, G loss: 0.7458\n",
      "[1204/1762] D loss: 1.3944, G loss: 0.7185\n",
      "[1284/1762] D loss: 1.3883, G loss: 0.6662\n",
      "[1364/1762] D loss: 1.3981, G loss: 0.7390\n",
      "[1444/1762] D loss: 1.3951, G loss: 0.6602\n",
      "[1524/1762] D loss: 1.3602, G loss: 0.7224\n",
      "[1604/1762] D loss: 1.3055, G loss: 0.7805\n",
      "[1684/1762] D loss: 1.3871, G loss: 0.7462\n",
      "[1762/1762] D loss: 1.3780, G loss: 0.6894\n",
      "train error: \n",
      " D loss: 1.347184, G loss: 0.698041, D accuracy: 54.0%, cell accuracy: 99.8%, board accuracy: 89.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.337672, G loss: 0.707406, D accuracy: 55.3%, cell accuracy: 99.8%, board accuracy: 84.5% \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3980, G loss: 0.6545\n",
      "[84/1762] D loss: 1.3905, G loss: 0.7302\n",
      "[164/1762] D loss: 1.3881, G loss: 0.6892\n",
      "[244/1762] D loss: 1.3946, G loss: 0.7642\n",
      "[324/1762] D loss: 1.4044, G loss: 0.7957\n",
      "[404/1762] D loss: 1.3907, G loss: 0.6796\n",
      "[484/1762] D loss: 1.2291, G loss: 0.7807\n",
      "[564/1762] D loss: 1.3894, G loss: 0.6779\n",
      "[644/1762] D loss: 1.3904, G loss: 0.7533\n",
      "[724/1762] D loss: 1.3976, G loss: 0.7217\n",
      "[804/1762] D loss: 1.3892, G loss: 0.6378\n",
      "[884/1762] D loss: 1.3896, G loss: 0.6661\n",
      "[964/1762] D loss: 1.3932, G loss: 0.7070\n",
      "[1044/1762] D loss: 1.4058, G loss: 0.6139\n",
      "[1124/1762] D loss: 1.3845, G loss: 0.6817\n",
      "[1204/1762] D loss: 1.6802, G loss: 0.4916\n",
      "[1284/1762] D loss: 1.7342, G loss: 0.6284\n",
      "[1364/1762] D loss: 1.5386, G loss: 0.5317\n",
      "[1444/1762] D loss: 1.1085, G loss: 0.7947\n",
      "[1524/1762] D loss: 1.1609, G loss: 1.0027\n",
      "[1604/1762] D loss: 1.2910, G loss: 0.6300\n",
      "[1684/1762] D loss: 1.6666, G loss: 0.4946\n",
      "[1762/1762] D loss: 1.5266, G loss: 0.9773\n",
      "train error: \n",
      " D loss: 1.477161, G loss: 0.649027, D accuracy: 47.5%, cell accuracy: 99.8%, board accuracy: 80.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.495108, G loss: 0.662658, D accuracy: 47.0%, cell accuracy: 99.8%, board accuracy: 78.2% \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3757, G loss: 0.6305\n",
      "[84/1762] D loss: 1.4065, G loss: 0.6779\n",
      "[164/1762] D loss: 1.4585, G loss: 0.5453\n",
      "[244/1762] D loss: 1.3937, G loss: 0.7472\n",
      "[324/1762] D loss: 1.4623, G loss: 0.9137\n",
      "[404/1762] D loss: 1.3927, G loss: 0.8044\n",
      "[484/1762] D loss: 1.3952, G loss: 0.7170\n",
      "[564/1762] D loss: 1.4571, G loss: 0.6400\n",
      "[644/1762] D loss: 1.4480, G loss: 0.6177\n",
      "[724/1762] D loss: 1.3942, G loss: 0.6717\n",
      "[804/1762] D loss: 1.4120, G loss: 0.8343\n",
      "[884/1762] D loss: 1.3875, G loss: 0.7165\n",
      "[964/1762] D loss: 1.4001, G loss: 0.6309\n",
      "[1044/1762] D loss: 1.5385, G loss: 0.6582\n",
      "[1124/1762] D loss: 1.3925, G loss: 0.6316\n",
      "[1204/1762] D loss: 1.3916, G loss: 0.6350\n",
      "[1284/1762] D loss: 1.3961, G loss: 0.6224\n",
      "[1364/1762] D loss: 1.3952, G loss: 0.5972\n",
      "[1444/1762] D loss: 1.3963, G loss: 0.7459\n",
      "[1524/1762] D loss: 1.3901, G loss: 0.6401\n",
      "[1604/1762] D loss: 1.3875, G loss: 0.7251\n",
      "[1684/1762] D loss: 1.4132, G loss: 0.8227\n",
      "[1762/1762] D loss: 1.3882, G loss: 0.7081\n",
      "train error: \n",
      " D loss: 1.400079, G loss: 0.792950, D accuracy: 50.3%, cell accuracy: 99.8%, board accuracy: 89.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.405744, G loss: 0.794151, D accuracy: 50.6%, cell accuracy: 99.8%, board accuracy: 87.7% \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3594, G loss: 0.7886\n",
      "[84/1762] D loss: 1.3960, G loss: 0.6746\n",
      "[164/1762] D loss: 1.3897, G loss: 0.7062\n",
      "[244/1762] D loss: 1.3959, G loss: 0.6744\n",
      "[324/1762] D loss: 1.3941, G loss: 0.7523\n",
      "[404/1762] D loss: 1.3885, G loss: 0.7611\n",
      "[484/1762] D loss: 1.3884, G loss: 0.6940\n",
      "[564/1762] D loss: 1.3886, G loss: 0.6310\n",
      "[644/1762] D loss: 1.4016, G loss: 0.6141\n",
      "[724/1762] D loss: 1.3889, G loss: 0.6962\n",
      "[804/1762] D loss: 1.3908, G loss: 0.6698\n",
      "[884/1762] D loss: 1.3866, G loss: 0.7085\n",
      "[964/1762] D loss: 1.3911, G loss: 0.7309\n",
      "[1044/1762] D loss: 1.3920, G loss: 0.7004\n",
      "[1124/1762] D loss: 1.3871, G loss: 0.7320\n",
      "[1204/1762] D loss: 1.3874, G loss: 0.7013\n",
      "[1284/1762] D loss: 1.4006, G loss: 0.6383\n",
      "[1364/1762] D loss: 1.3880, G loss: 0.7193\n",
      "[1444/1762] D loss: 1.3934, G loss: 0.6579\n",
      "[1524/1762] D loss: 1.3921, G loss: 0.6929\n",
      "[1604/1762] D loss: 1.3940, G loss: 0.6330\n",
      "[1684/1762] D loss: 1.3187, G loss: 0.6983\n",
      "[1762/1762] D loss: 1.2235, G loss: 0.7105\n",
      "train error: \n",
      " D loss: 1.375398, G loss: 0.629306, D accuracy: 53.3%, cell accuracy: 99.9%, board accuracy: 91.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.372770, G loss: 0.632177, D accuracy: 53.3%, cell accuracy: 99.8%, board accuracy: 89.3% \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3913, G loss: 0.6733\n",
      "[84/1762] D loss: 1.3910, G loss: 0.7264\n",
      "[164/1762] D loss: 1.3926, G loss: 0.7529\n",
      "[244/1762] D loss: 1.3890, G loss: 0.7055\n",
      "[324/1762] D loss: 1.3876, G loss: 0.7602\n",
      "[404/1762] D loss: 1.3965, G loss: 0.6485\n",
      "[484/1762] D loss: 1.3871, G loss: 0.6463\n",
      "[564/1762] D loss: 1.4020, G loss: 0.6580\n",
      "[644/1762] D loss: 1.3882, G loss: 0.7626\n",
      "[724/1762] D loss: 1.3902, G loss: 0.7357\n",
      "[804/1762] D loss: 1.3896, G loss: 0.7294\n",
      "[884/1762] D loss: 1.3928, G loss: 0.6289\n",
      "[964/1762] D loss: 1.3984, G loss: 0.7799\n",
      "[1044/1762] D loss: 1.2604, G loss: 0.8245\n",
      "[1124/1762] D loss: 1.3937, G loss: 0.6247\n",
      "[1204/1762] D loss: 1.3345, G loss: 0.7224\n",
      "[1284/1762] D loss: 1.3901, G loss: 0.6784\n",
      "[1364/1762] D loss: 1.3895, G loss: 0.6978\n",
      "[1444/1762] D loss: 1.3886, G loss: 0.7160\n",
      "[1524/1762] D loss: 1.3869, G loss: 0.7149\n",
      "[1604/1762] D loss: 1.3886, G loss: 0.6778\n",
      "[1684/1762] D loss: 1.4020, G loss: 0.5967\n",
      "[1762/1762] D loss: 1.3865, G loss: 0.7508\n",
      "train error: \n",
      " D loss: 1.354192, G loss: 0.746735, D accuracy: 53.5%, cell accuracy: 99.9%, board accuracy: 91.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.347762, G loss: 0.751258, D accuracy: 54.5%, cell accuracy: 99.8%, board accuracy: 89.8% \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2527, G loss: 0.8312\n",
      "[84/1762] D loss: 1.3914, G loss: 0.6093\n",
      "[164/1762] D loss: 1.3883, G loss: 0.6706\n",
      "[244/1762] D loss: 1.3895, G loss: 0.6475\n",
      "[324/1762] D loss: 1.3906, G loss: 0.7547\n",
      "[404/1762] D loss: 1.2559, G loss: 0.7102\n",
      "[484/1762] D loss: 1.3901, G loss: 0.7205\n",
      "[564/1762] D loss: 1.2284, G loss: 0.8297\n",
      "[644/1762] D loss: 1.3981, G loss: 0.6518\n",
      "[724/1762] D loss: 1.3878, G loss: 0.7205\n",
      "[804/1762] D loss: 1.4012, G loss: 0.7905\n",
      "[884/1762] D loss: 1.3986, G loss: 0.7540\n",
      "[964/1762] D loss: 1.3910, G loss: 0.7686\n",
      "[1044/1762] D loss: 1.2151, G loss: 0.7702\n",
      "[1124/1762] D loss: 1.4053, G loss: 0.7449\n",
      "[1204/1762] D loss: 1.2008, G loss: 0.8311\n",
      "[1284/1762] D loss: 1.3960, G loss: 0.7829\n",
      "[1364/1762] D loss: 1.3922, G loss: 0.6395\n",
      "[1444/1762] D loss: 1.4007, G loss: 0.7167\n",
      "[1524/1762] D loss: 1.3889, G loss: 0.6699\n",
      "[1604/1762] D loss: 1.4034, G loss: 0.7340\n",
      "[1684/1762] D loss: 1.3874, G loss: 0.6853\n",
      "[1762/1762] D loss: 1.3992, G loss: 0.8130\n",
      "train error: \n",
      " D loss: 1.344870, G loss: 0.784225, D accuracy: 53.5%, cell accuracy: 99.8%, board accuracy: 90.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.334816, G loss: 0.789050, D accuracy: 54.7%, cell accuracy: 99.8%, board accuracy: 87.7% \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2132, G loss: 0.8230\n",
      "[84/1762] D loss: 1.3463, G loss: 0.6370\n",
      "[164/1762] D loss: 1.2157, G loss: 0.7615\n",
      "[244/1762] D loss: 1.2148, G loss: 0.7602\n",
      "[324/1762] D loss: 1.3915, G loss: 0.7067\n",
      "[404/1762] D loss: 1.3954, G loss: 0.6154\n",
      "[484/1762] D loss: 1.3891, G loss: 0.7776\n",
      "[564/1762] D loss: 1.3906, G loss: 0.6254\n",
      "[644/1762] D loss: 1.4016, G loss: 0.7596\n",
      "[724/1762] D loss: 1.1761, G loss: 0.9002\n",
      "[804/1762] D loss: 1.0573, G loss: 1.1220\n",
      "[884/1762] D loss: 1.3945, G loss: 0.5981\n",
      "[964/1762] D loss: 1.3986, G loss: 0.7076\n",
      "[1044/1762] D loss: 1.3922, G loss: 0.6116\n",
      "[1124/1762] D loss: 1.2010, G loss: 0.7887\n",
      "[1204/1762] D loss: 1.4321, G loss: 0.7711\n",
      "[1284/1762] D loss: 1.1821, G loss: 0.7774\n",
      "[1364/1762] D loss: 1.4183, G loss: 0.5604\n",
      "[1444/1762] D loss: 1.3946, G loss: 0.7776\n",
      "[1524/1762] D loss: 1.3898, G loss: 0.6811\n",
      "[1604/1762] D loss: 1.2350, G loss: 0.7322\n",
      "[1684/1762] D loss: 1.3944, G loss: 0.6199\n",
      "[1762/1762] D loss: 1.3895, G loss: 0.6282\n",
      "train error: \n",
      " D loss: 1.350509, G loss: 0.646123, D accuracy: 53.4%, cell accuracy: 99.8%, board accuracy: 86.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.337744, G loss: 0.653180, D accuracy: 53.8%, cell accuracy: 99.8%, board accuracy: 85.9% \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3954, G loss: 0.6426\n",
      "[84/1762] D loss: 1.4899, G loss: 0.6524\n",
      "[164/1762] D loss: 1.4566, G loss: 0.8511\n",
      "[244/1762] D loss: 2.1110, G loss: 0.4132\n",
      "[324/1762] D loss: 2.0530, G loss: 0.2963\n",
      "[404/1762] D loss: 1.4219, G loss: 0.9026\n",
      "[484/1762] D loss: 1.3724, G loss: 0.9302\n",
      "[564/1762] D loss: 1.2301, G loss: 0.8912\n",
      "[644/1762] D loss: 1.3582, G loss: 0.8274\n",
      "[724/1762] D loss: 1.4511, G loss: 0.5737\n",
      "[804/1762] D loss: 1.4334, G loss: 0.5689\n",
      "[884/1762] D loss: 1.4045, G loss: 0.7328\n",
      "[964/1762] D loss: 1.4050, G loss: 0.6409\n",
      "[1044/1762] D loss: 1.4630, G loss: 0.6839\n",
      "[1124/1762] D loss: 1.3963, G loss: 0.7340\n",
      "[1204/1762] D loss: 1.4007, G loss: 0.7548\n",
      "[1284/1762] D loss: 1.3956, G loss: 0.7178\n",
      "[1364/1762] D loss: 1.4351, G loss: 0.7253\n",
      "[1444/1762] D loss: 1.4167, G loss: 0.6813\n",
      "[1524/1762] D loss: 1.4041, G loss: 0.7606\n",
      "[1604/1762] D loss: 1.4061, G loss: 0.7596\n",
      "[1684/1762] D loss: 1.4001, G loss: 0.7958\n",
      "[1762/1762] D loss: 1.3892, G loss: 0.6553\n",
      "train error: \n",
      " D loss: 1.393203, G loss: 0.688176, D accuracy: 50.1%, cell accuracy: 99.9%, board accuracy: 90.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.396149, G loss: 0.693558, D accuracy: 50.6%, cell accuracy: 99.8%, board accuracy: 87.5% \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3793, G loss: 0.7625\n",
      "[84/1762] D loss: 1.3930, G loss: 0.6285\n",
      "[164/1762] D loss: 1.3877, G loss: 0.6709\n",
      "[244/1762] D loss: 1.4440, G loss: 0.7100\n",
      "[324/1762] D loss: 1.3884, G loss: 0.6704\n",
      "[404/1762] D loss: 1.3875, G loss: 0.6647\n",
      "[484/1762] D loss: 1.3878, G loss: 0.6644\n",
      "[564/1762] D loss: 1.3893, G loss: 0.6920\n",
      "[644/1762] D loss: 1.3932, G loss: 0.6421\n",
      "[724/1762] D loss: 1.4212, G loss: 0.7497\n",
      "[804/1762] D loss: 1.3865, G loss: 0.7125\n",
      "[884/1762] D loss: 1.3868, G loss: 0.6903\n",
      "[964/1762] D loss: 1.3891, G loss: 0.6710\n",
      "[1044/1762] D loss: 1.3888, G loss: 0.6713\n",
      "[1124/1762] D loss: 1.2851, G loss: 0.7396\n",
      "[1204/1762] D loss: 1.3154, G loss: 0.7111\n",
      "[1284/1762] D loss: 1.3921, G loss: 0.7263\n",
      "[1364/1762] D loss: 1.3736, G loss: 0.7026\n",
      "[1444/1762] D loss: 1.3080, G loss: 0.7009\n",
      "[1524/1762] D loss: 1.3062, G loss: 0.7579\n",
      "[1604/1762] D loss: 1.3022, G loss: 0.7185\n",
      "[1684/1762] D loss: 1.3821, G loss: 0.7429\n",
      "[1762/1762] D loss: 1.3863, G loss: 0.6985\n",
      "train error: \n",
      " D loss: 1.367284, G loss: 0.669380, D accuracy: 53.4%, cell accuracy: 99.9%, board accuracy: 92.0% \n",
      "\n",
      "test error: \n",
      " D loss: 1.364050, G loss: 0.672072, D accuracy: 54.1%, cell accuracy: 99.8%, board accuracy: 90.0% \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3094, G loss: 0.6786\n",
      "[84/1762] D loss: 1.3064, G loss: 0.6886\n",
      "[164/1762] D loss: 1.2363, G loss: 0.8239\n",
      "[244/1762] D loss: 1.3898, G loss: 0.6609\n",
      "[324/1762] D loss: 1.3870, G loss: 0.6886\n",
      "[404/1762] D loss: 1.3895, G loss: 0.6511\n",
      "[484/1762] D loss: 1.3915, G loss: 0.6573\n",
      "[564/1762] D loss: 1.2814, G loss: 0.7494\n",
      "[644/1762] D loss: 1.3892, G loss: 0.6948\n",
      "[724/1762] D loss: 1.3952, G loss: 0.6428\n",
      "[804/1762] D loss: 1.3961, G loss: 0.6259\n",
      "[884/1762] D loss: 1.3896, G loss: 0.6561\n",
      "[964/1762] D loss: 1.3884, G loss: 0.6977\n",
      "[1044/1762] D loss: 1.3868, G loss: 0.6721\n",
      "[1124/1762] D loss: 1.3906, G loss: 0.6427\n",
      "[1204/1762] D loss: 1.3899, G loss: 0.7470\n",
      "[1284/1762] D loss: 1.3877, G loss: 0.7039\n",
      "[1364/1762] D loss: 1.3886, G loss: 0.7328\n",
      "[1444/1762] D loss: 1.3897, G loss: 0.7578\n",
      "[1524/1762] D loss: 1.3868, G loss: 0.6817\n",
      "[1604/1762] D loss: 1.3933, G loss: 0.7680\n",
      "[1684/1762] D loss: 1.3890, G loss: 0.7144\n",
      "[1762/1762] D loss: 1.3868, G loss: 0.6798\n",
      "train error: \n",
      " D loss: 1.356200, G loss: 0.664127, D accuracy: 53.3%, cell accuracy: 99.9%, board accuracy: 91.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.349919, G loss: 0.668614, D accuracy: 54.2%, cell accuracy: 99.8%, board accuracy: 89.1% \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3871, G loss: 0.6901\n",
      "[84/1762] D loss: 1.3854, G loss: 0.7023\n",
      "[164/1762] D loss: 1.3969, G loss: 0.7080\n",
      "[244/1762] D loss: 1.3484, G loss: 0.6912\n",
      "[324/1762] D loss: 1.4719, G loss: 0.7759\n",
      "[404/1762] D loss: 1.4635, G loss: 0.7901\n",
      "[484/1762] D loss: 1.4902, G loss: 0.6476\n",
      "[564/1762] D loss: 1.3924, G loss: 0.6457\n",
      "[644/1762] D loss: 1.2785, G loss: 0.7671\n",
      "[724/1762] D loss: 1.2325, G loss: 0.6958\n",
      "[804/1762] D loss: 1.1643, G loss: 0.8462\n",
      "[884/1762] D loss: 1.2463, G loss: 0.8808\n",
      "[964/1762] D loss: 1.0239, G loss: 0.8349\n",
      "[1044/1762] D loss: 1.2151, G loss: 0.8274\n",
      "[1124/1762] D loss: 1.4267, G loss: 0.4835\n",
      "[1204/1762] D loss: 1.4731, G loss: 0.5549\n",
      "[1284/1762] D loss: 1.3944, G loss: 0.6920\n",
      "[1364/1762] D loss: 1.3735, G loss: 0.6600\n",
      "[1444/1762] D loss: 1.4172, G loss: 0.7729\n",
      "[1524/1762] D loss: 1.4220, G loss: 0.7449\n",
      "[1604/1762] D loss: 1.5259, G loss: 0.6345\n",
      "[1684/1762] D loss: 1.4254, G loss: 0.5603\n",
      "[1762/1762] D loss: 1.4098, G loss: 0.7725\n",
      "train error: \n",
      " D loss: 1.403093, G loss: 0.733049, D accuracy: 49.6%, cell accuracy: 99.8%, board accuracy: 86.5% \n",
      "\n",
      "test error: \n",
      " D loss: 1.407987, G loss: 0.738556, D accuracy: 48.9%, cell accuracy: 99.8%, board accuracy: 83.2% \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4504, G loss: 0.6515\n",
      "[84/1762] D loss: 1.3956, G loss: 0.7087\n",
      "[164/1762] D loss: 1.3898, G loss: 0.7421\n",
      "[244/1762] D loss: 1.3943, G loss: 0.6898\n",
      "[324/1762] D loss: 1.3912, G loss: 0.7056\n",
      "[404/1762] D loss: 1.3869, G loss: 0.7031\n",
      "[484/1762] D loss: 1.3888, G loss: 0.7121\n",
      "[564/1762] D loss: 1.3882, G loss: 0.6976\n",
      "[644/1762] D loss: 1.3689, G loss: 0.7649\n",
      "[724/1762] D loss: 1.3341, G loss: 0.7544\n",
      "[804/1762] D loss: 1.3407, G loss: 0.7064\n",
      "[884/1762] D loss: 1.3928, G loss: 0.7543\n",
      "[964/1762] D loss: 1.3884, G loss: 0.6887\n",
      "[1044/1762] D loss: 1.3832, G loss: 0.7180\n",
      "[1124/1762] D loss: 1.3266, G loss: 0.7073\n",
      "[1204/1762] D loss: 1.3880, G loss: 0.7106\n",
      "[1284/1762] D loss: 1.3174, G loss: 0.7250\n",
      "[1364/1762] D loss: 1.3953, G loss: 0.6300\n",
      "[1444/1762] D loss: 1.3548, G loss: 0.6954\n",
      "[1524/1762] D loss: 1.3950, G loss: 0.6751\n",
      "[1604/1762] D loss: 1.3284, G loss: 0.7290\n",
      "[1684/1762] D loss: 1.3867, G loss: 0.6675\n",
      "[1762/1762] D loss: 1.3886, G loss: 0.6463\n",
      "train error: \n",
      " D loss: 1.366437, G loss: 0.692321, D accuracy: 53.4%, cell accuracy: 99.9%, board accuracy: 92.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.363375, G loss: 0.696060, D accuracy: 54.5%, cell accuracy: 99.8%, board accuracy: 89.3% \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2891, G loss: 0.7185\n",
      "[84/1762] D loss: 1.3967, G loss: 0.7472\n",
      "[164/1762] D loss: 1.3752, G loss: 0.7935\n",
      "[244/1762] D loss: 1.3899, G loss: 0.6807\n",
      "[324/1762] D loss: 1.3892, G loss: 0.7087\n",
      "[404/1762] D loss: 1.3923, G loss: 0.7464\n",
      "[484/1762] D loss: 1.2758, G loss: 0.7661\n",
      "[564/1762] D loss: 1.2749, G loss: 0.7820\n",
      "[644/1762] D loss: 1.2178, G loss: 0.7689\n",
      "[724/1762] D loss: 1.3914, G loss: 0.7419\n",
      "[804/1762] D loss: 1.3993, G loss: 0.6793\n",
      "[884/1762] D loss: 1.3959, G loss: 0.7844\n",
      "[964/1762] D loss: 1.2566, G loss: 0.7434\n",
      "[1044/1762] D loss: 1.3888, G loss: 0.7022\n",
      "[1124/1762] D loss: 1.3188, G loss: 0.7784\n",
      "[1204/1762] D loss: 1.3890, G loss: 0.6927\n",
      "[1284/1762] D loss: 1.3923, G loss: 0.7476\n",
      "[1364/1762] D loss: 1.3996, G loss: 0.7218\n",
      "[1444/1762] D loss: 1.3886, G loss: 0.6660\n",
      "[1524/1762] D loss: 1.3893, G loss: 0.7247\n",
      "[1604/1762] D loss: 1.3875, G loss: 0.7154\n",
      "[1684/1762] D loss: 1.3928, G loss: 0.7478\n",
      "[1762/1762] D loss: 1.0752, G loss: 0.9265\n",
      "train error: \n",
      " D loss: 1.354751, G loss: 0.792688, D accuracy: 53.5%, cell accuracy: 99.8%, board accuracy: 86.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.347671, G loss: 0.798098, D accuracy: 54.4%, cell accuracy: 99.8%, board accuracy: 85.9% \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3943, G loss: 0.7883\n",
      "[84/1762] D loss: 1.3991, G loss: 0.7648\n",
      "[164/1762] D loss: 1.3905, G loss: 0.7333\n",
      "[244/1762] D loss: 1.1316, G loss: 0.8496\n",
      "[324/1762] D loss: 1.2665, G loss: 0.7034\n",
      "[404/1762] D loss: 1.4095, G loss: 0.5825\n",
      "[484/1762] D loss: 1.4171, G loss: 0.6829\n",
      "[564/1762] D loss: 1.3699, G loss: 0.7939\n",
      "[644/1762] D loss: 1.2294, G loss: 0.7572\n",
      "[724/1762] D loss: 1.3887, G loss: 0.6862\n",
      "[804/1762] D loss: 1.3898, G loss: 0.6763\n",
      "[884/1762] D loss: 1.3694, G loss: 0.6386\n",
      "[964/1762] D loss: 1.2245, G loss: 0.7539\n",
      "[1044/1762] D loss: 1.2152, G loss: 0.8341\n",
      "[1124/1762] D loss: 1.3884, G loss: 0.7141\n",
      "[1204/1762] D loss: 1.2237, G loss: 0.7730\n",
      "[1284/1762] D loss: 1.2073, G loss: 0.7860\n",
      "[1364/1762] D loss: 1.3670, G loss: 0.7145\n",
      "[1444/1762] D loss: 1.3880, G loss: 0.6916\n",
      "[1524/1762] D loss: 1.2049, G loss: 0.9404\n",
      "[1604/1762] D loss: 1.3985, G loss: 0.7465\n",
      "[1684/1762] D loss: 1.3964, G loss: 0.7104\n",
      "[1762/1762] D loss: 1.3928, G loss: 0.6470\n",
      "train error: \n",
      " D loss: 1.351708, G loss: 0.617233, D accuracy: 53.9%, cell accuracy: 99.8%, board accuracy: 78.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.339783, G loss: 0.624952, D accuracy: 55.2%, cell accuracy: 99.7%, board accuracy: 75.5% \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3990, G loss: 0.5354\n",
      "[84/1762] D loss: 1.2416, G loss: 0.7774\n",
      "[164/1762] D loss: 1.3878, G loss: 0.6296\n",
      "[244/1762] D loss: 1.3968, G loss: 0.7476\n",
      "[324/1762] D loss: 1.3884, G loss: 0.7185\n",
      "[404/1762] D loss: 1.3889, G loss: 0.6722\n",
      "[484/1762] D loss: 1.3877, G loss: 0.6072\n",
      "[564/1762] D loss: 1.1936, G loss: 0.7133\n",
      "[644/1762] D loss: 1.3866, G loss: 0.7121\n",
      "[724/1762] D loss: 1.3997, G loss: 0.6336\n",
      "[804/1762] D loss: 1.1941, G loss: 0.8578\n",
      "[884/1762] D loss: 0.9640, G loss: 1.0397\n",
      "[964/1762] D loss: 1.3890, G loss: 0.7257\n",
      "[1044/1762] D loss: 1.2198, G loss: 0.7756\n",
      "[1124/1762] D loss: 1.1876, G loss: 0.7785\n",
      "[1204/1762] D loss: 1.1992, G loss: 0.7555\n",
      "[1284/1762] D loss: 1.3879, G loss: 0.6569\n",
      "[1364/1762] D loss: 1.3955, G loss: 0.6260\n",
      "[1444/1762] D loss: 1.5675, G loss: 0.6461\n",
      "[1524/1762] D loss: 1.9479, G loss: 0.3774\n",
      "[1604/1762] D loss: 1.4152, G loss: 0.7520\n",
      "[1684/1762] D loss: 1.3600, G loss: 0.6379\n",
      "[1762/1762] D loss: 1.2360, G loss: 0.6729\n",
      "train error: \n",
      " D loss: 1.264677, G loss: 0.663277, D accuracy: 59.5%, cell accuracy: 98.8%, board accuracy: 20.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.268207, G loss: 0.679569, D accuracy: 60.9%, cell accuracy: 98.8%, board accuracy: 19.5% \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.2932, G loss: 0.5856\n",
      "[84/1762] D loss: 1.2335, G loss: 0.7475\n",
      "[164/1762] D loss: 1.1958, G loss: 0.8315\n",
      "[244/1762] D loss: 1.4032, G loss: 0.7503\n",
      "[324/1762] D loss: 1.4159, G loss: 0.5786\n",
      "[404/1762] D loss: 1.4686, G loss: 0.8266\n",
      "[484/1762] D loss: 1.5438, G loss: 0.6971\n",
      "[564/1762] D loss: 1.4285, G loss: 0.6402\n",
      "[644/1762] D loss: 1.3865, G loss: 0.7568\n",
      "[724/1762] D loss: 1.3983, G loss: 0.6239\n",
      "[804/1762] D loss: 1.4000, G loss: 0.5663\n",
      "[884/1762] D loss: 1.3893, G loss: 0.6547\n",
      "[964/1762] D loss: 1.4104, G loss: 0.7857\n",
      "[1044/1762] D loss: 1.4014, G loss: 0.6647\n",
      "[1124/1762] D loss: 1.5262, G loss: 0.6955\n",
      "[1204/1762] D loss: 1.5128, G loss: 0.7291\n",
      "[1284/1762] D loss: 1.4593, G loss: 0.6427\n",
      "[1364/1762] D loss: 1.3914, G loss: 0.7833\n",
      "[1444/1762] D loss: 1.3804, G loss: 0.7110\n",
      "[1524/1762] D loss: 1.4050, G loss: 0.7892\n",
      "[1604/1762] D loss: 1.3923, G loss: 0.7140\n",
      "[1684/1762] D loss: 1.4172, G loss: 0.5399\n",
      "[1762/1762] D loss: 1.2270, G loss: 0.7549\n",
      "train error: \n",
      " D loss: 1.382488, G loss: 0.684647, D accuracy: 52.5%, cell accuracy: 99.9%, board accuracy: 91.3% \n",
      "\n",
      "test error: \n",
      " D loss: 1.383628, G loss: 0.687335, D accuracy: 52.2%, cell accuracy: 99.8%, board accuracy: 88.2% \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4472, G loss: 0.7016\n",
      "[84/1762] D loss: 1.3980, G loss: 0.7091\n",
      "[164/1762] D loss: 1.3898, G loss: 0.7475\n",
      "[244/1762] D loss: 1.3875, G loss: 0.7239\n",
      "[324/1762] D loss: 1.3827, G loss: 0.7541\n",
      "[404/1762] D loss: 1.3895, G loss: 0.6501\n",
      "[484/1762] D loss: 1.3876, G loss: 0.6801\n",
      "[564/1762] D loss: 1.3952, G loss: 0.6400\n",
      "[644/1762] D loss: 1.3879, G loss: 0.6711\n",
      "[724/1762] D loss: 1.3878, G loss: 0.6558\n",
      "[804/1762] D loss: 1.3872, G loss: 0.6803\n",
      "[884/1762] D loss: 1.3880, G loss: 0.6617\n",
      "[964/1762] D loss: 1.3914, G loss: 0.7385\n",
      "[1044/1762] D loss: 1.3945, G loss: 0.7116\n",
      "[1124/1762] D loss: 1.3875, G loss: 0.7200\n",
      "[1204/1762] D loss: 1.3905, G loss: 0.6965\n",
      "[1284/1762] D loss: 1.3903, G loss: 0.6640\n",
      "[1364/1762] D loss: 1.1126, G loss: 0.8534\n",
      "[1444/1762] D loss: 1.3881, G loss: 0.6576\n",
      "[1524/1762] D loss: 1.3887, G loss: 0.6332\n",
      "[1604/1762] D loss: 1.2615, G loss: 0.6805\n",
      "[1684/1762] D loss: 1.3897, G loss: 0.7092\n",
      "[1762/1762] D loss: 1.3890, G loss: 0.7079\n",
      "train error: \n",
      " D loss: 1.353855, G loss: 0.722726, D accuracy: 53.6%, cell accuracy: 99.9%, board accuracy: 91.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.347273, G loss: 0.726435, D accuracy: 54.9%, cell accuracy: 99.8%, board accuracy: 89.3% \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3845, G loss: 0.6517\n",
      "[84/1762] D loss: 1.3867, G loss: 0.6495\n",
      "[164/1762] D loss: 1.3889, G loss: 0.6144\n",
      "[244/1762] D loss: 1.3868, G loss: 0.6871\n",
      "[324/1762] D loss: 1.2975, G loss: 0.6851\n",
      "[404/1762] D loss: 1.3933, G loss: 0.6919\n",
      "[484/1762] D loss: 1.3317, G loss: 0.7352\n",
      "[564/1762] D loss: 1.4107, G loss: 0.6129\n",
      "[644/1762] D loss: 1.3896, G loss: 0.7182\n",
      "[724/1762] D loss: 1.3902, G loss: 0.7085\n",
      "[804/1762] D loss: 1.3879, G loss: 0.6939\n",
      "[884/1762] D loss: 1.3768, G loss: 0.6999\n",
      "[964/1762] D loss: 1.3974, G loss: 0.7789\n",
      "[1044/1762] D loss: 1.2209, G loss: 0.7969\n",
      "[1124/1762] D loss: 1.3904, G loss: 0.6760\n",
      "[1204/1762] D loss: 1.3890, G loss: 0.6847\n",
      "[1284/1762] D loss: 1.3956, G loss: 0.6404\n",
      "[1364/1762] D loss: 1.3976, G loss: 0.6370\n",
      "[1444/1762] D loss: 1.3921, G loss: 0.7258\n",
      "[1524/1762] D loss: 1.4006, G loss: 0.7243\n",
      "[1604/1762] D loss: 1.2227, G loss: 0.7756\n",
      "[1684/1762] D loss: 1.3958, G loss: 0.8140\n",
      "[1762/1762] D loss: 1.3875, G loss: 0.6829\n",
      "train error: \n",
      " D loss: 1.343075, G loss: 0.712923, D accuracy: 53.7%, cell accuracy: 99.9%, board accuracy: 91.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.332945, G loss: 0.717718, D accuracy: 54.8%, cell accuracy: 99.8%, board accuracy: 89.5% \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3913, G loss: 0.6951\n",
      "[84/1762] D loss: 1.3887, G loss: 0.7490\n",
      "[164/1762] D loss: 1.2115, G loss: 0.8182\n",
      "[244/1762] D loss: 1.2911, G loss: 0.7640\n",
      "[324/1762] D loss: 1.4055, G loss: 0.8414\n",
      "[404/1762] D loss: 1.3787, G loss: 0.6916\n",
      "[484/1762] D loss: 1.3902, G loss: 0.6840\n",
      "[564/1762] D loss: 1.2392, G loss: 0.7816\n",
      "[644/1762] D loss: 1.3872, G loss: 0.7302\n",
      "[724/1762] D loss: 1.3872, G loss: 0.7560\n",
      "[804/1762] D loss: 1.3886, G loss: 0.6321\n",
      "[884/1762] D loss: 1.3921, G loss: 0.6204\n",
      "[964/1762] D loss: 1.3877, G loss: 0.6733\n",
      "[1044/1762] D loss: 1.2151, G loss: 0.9425\n",
      "[1124/1762] D loss: 1.2097, G loss: 0.8803\n",
      "[1204/1762] D loss: 1.3911, G loss: 0.6242\n",
      "[1284/1762] D loss: 1.3884, G loss: 0.6878\n",
      "[1364/1762] D loss: 1.3881, G loss: 0.6356\n",
      "[1444/1762] D loss: 1.3919, G loss: 0.7344\n",
      "[1524/1762] D loss: 1.4012, G loss: 0.7210\n",
      "[1604/1762] D loss: 1.4028, G loss: 0.6317\n",
      "[1684/1762] D loss: 1.3942, G loss: 0.7269\n",
      "[1762/1762] D loss: 1.3954, G loss: 0.7306\n",
      "train error: \n",
      " D loss: 1.337453, G loss: 0.747981, D accuracy: 53.5%, cell accuracy: 99.9%, board accuracy: 90.7% \n",
      "\n",
      "test error: \n",
      " D loss: 1.325114, G loss: 0.753781, D accuracy: 54.4%, cell accuracy: 99.8%, board accuracy: 88.4% \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3876, G loss: 0.6654\n",
      "[84/1762] D loss: 1.3832, G loss: 0.6973\n",
      "[164/1762] D loss: 1.1881, G loss: 0.8288\n",
      "[244/1762] D loss: 1.4084, G loss: 0.8180\n",
      "[324/1762] D loss: 1.3921, G loss: 0.7326\n",
      "[404/1762] D loss: 1.3855, G loss: 0.7688\n",
      "[484/1762] D loss: 1.3965, G loss: 0.7005\n",
      "[564/1762] D loss: 1.2720, G loss: 0.6978\n",
      "[644/1762] D loss: 1.6107, G loss: 0.6972\n",
      "[724/1762] D loss: 1.4375, G loss: 0.5595\n",
      "[804/1762] D loss: 1.2105, G loss: 0.8032\n",
      "[884/1762] D loss: 1.1249, G loss: 0.7933\n",
      "[964/1762] D loss: 1.3605, G loss: 0.7508\n",
      "[1044/1762] D loss: 1.4671, G loss: 0.8220\n",
      "[1124/1762] D loss: 1.4011, G loss: 0.7207\n",
      "[1204/1762] D loss: 1.3751, G loss: 0.6121\n",
      "[1284/1762] D loss: 1.4678, G loss: 0.7385\n",
      "[1364/1762] D loss: 1.3875, G loss: 0.6700\n",
      "[1444/1762] D loss: 1.3907, G loss: 0.6368\n",
      "[1524/1762] D loss: 1.2816, G loss: 0.7477\n",
      "[1604/1762] D loss: 1.3867, G loss: 0.7082\n",
      "[1684/1762] D loss: 1.3954, G loss: 0.6411\n",
      "[1762/1762] D loss: 1.3902, G loss: 0.6955\n",
      "train error: \n",
      " D loss: 1.361149, G loss: 0.724342, D accuracy: 53.4%, cell accuracy: 99.8%, board accuracy: 90.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.357638, G loss: 0.730120, D accuracy: 54.2%, cell accuracy: 99.8%, board accuracy: 87.7% \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3918, G loss: 0.7026\n",
      "[84/1762] D loss: 1.4090, G loss: 0.7443\n",
      "[164/1762] D loss: 1.2945, G loss: 0.8124\n",
      "[244/1762] D loss: 1.3887, G loss: 0.7147\n",
      "[324/1762] D loss: 1.2556, G loss: 0.7332\n",
      "[404/1762] D loss: 1.3877, G loss: 0.7102\n",
      "[484/1762] D loss: 1.3865, G loss: 0.7112\n",
      "[564/1762] D loss: 1.2497, G loss: 0.8092\n",
      "[644/1762] D loss: 1.3909, G loss: 0.7176\n",
      "[724/1762] D loss: 1.3876, G loss: 0.7166\n",
      "[804/1762] D loss: 1.3868, G loss: 0.6750\n",
      "[884/1762] D loss: 1.2328, G loss: 0.7590\n",
      "[964/1762] D loss: 1.3890, G loss: 0.7255\n",
      "[1044/1762] D loss: 1.2708, G loss: 0.8124\n",
      "[1124/1762] D loss: 1.3917, G loss: 0.7356\n",
      "[1204/1762] D loss: 1.2031, G loss: 0.8960\n",
      "[1284/1762] D loss: 1.3876, G loss: 0.6845\n",
      "[1364/1762] D loss: 1.3962, G loss: 0.8035\n",
      "[1444/1762] D loss: 1.4242, G loss: 0.8337\n",
      "[1524/1762] D loss: 1.3891, G loss: 0.7188\n",
      "[1604/1762] D loss: 1.3965, G loss: 0.6460\n",
      "[1684/1762] D loss: 1.3904, G loss: 0.6580\n",
      "[1762/1762] D loss: 1.3961, G loss: 0.6748\n",
      "train error: \n",
      " D loss: 1.344594, G loss: 0.638708, D accuracy: 53.8%, cell accuracy: 99.9%, board accuracy: 91.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.333254, G loss: 0.645524, D accuracy: 54.9%, cell accuracy: 99.8%, board accuracy: 90.0% \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3913, G loss: 0.6716\n",
      "[84/1762] D loss: 1.2004, G loss: 0.7212\n",
      "[164/1762] D loss: 1.3974, G loss: 0.6535\n",
      "[244/1762] D loss: 1.4070, G loss: 0.8370\n",
      "[324/1762] D loss: 1.3967, G loss: 0.8333\n",
      "[404/1762] D loss: 1.1774, G loss: 0.8661\n",
      "[484/1762] D loss: 1.3946, G loss: 0.7830\n",
      "[564/1762] D loss: 1.1708, G loss: 0.7894\n",
      "[644/1762] D loss: 1.3886, G loss: 0.6832\n",
      "[724/1762] D loss: 1.3889, G loss: 0.6359\n",
      "[804/1762] D loss: 1.3923, G loss: 0.7234\n",
      "[884/1762] D loss: 1.3886, G loss: 0.7356\n",
      "[964/1762] D loss: 1.3966, G loss: 0.7909\n",
      "[1044/1762] D loss: 1.1946, G loss: 0.8711\n",
      "[1124/1762] D loss: 1.3975, G loss: 0.8041\n",
      "[1204/1762] D loss: 1.1651, G loss: 0.8666\n",
      "[1284/1762] D loss: 1.3881, G loss: 0.7833\n",
      "[1364/1762] D loss: 1.1587, G loss: 1.0872\n",
      "[1444/1762] D loss: 1.3873, G loss: 0.7289\n",
      "[1524/1762] D loss: 1.1665, G loss: 0.8245\n",
      "[1604/1762] D loss: 1.4120, G loss: 0.6871\n",
      "[1684/1762] D loss: 1.4256, G loss: 0.7101\n",
      "[1762/1762] D loss: 1.3974, G loss: 0.7792\n",
      "train error: \n",
      " D loss: 1.333884, G loss: 0.810341, D accuracy: 53.3%, cell accuracy: 99.8%, board accuracy: 82.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.319270, G loss: 0.818386, D accuracy: 54.7%, cell accuracy: 99.7%, board accuracy: 79.5% \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3911, G loss: 0.7408\n",
      "[84/1762] D loss: 1.1822, G loss: 0.8763\n",
      "[164/1762] D loss: 1.3932, G loss: 0.6740\n",
      "[244/1762] D loss: 1.3839, G loss: 0.7005\n",
      "[324/1762] D loss: 1.3941, G loss: 0.7534\n",
      "[404/1762] D loss: 1.4553, G loss: 0.9366\n",
      "[484/1762] D loss: 1.4404, G loss: 0.9322\n",
      "[564/1762] D loss: 1.3912, G loss: 0.7173\n",
      "[644/1762] D loss: 1.4945, G loss: 0.5042\n",
      "[724/1762] D loss: 1.3939, G loss: 0.7028\n",
      "[804/1762] D loss: 1.3945, G loss: 0.7368\n",
      "[884/1762] D loss: 1.3927, G loss: 0.7138\n",
      "[964/1762] D loss: 1.3917, G loss: 0.8153\n",
      "[1044/1762] D loss: 1.3877, G loss: 0.7305\n",
      "[1124/1762] D loss: 1.3963, G loss: 0.7179\n",
      "[1204/1762] D loss: 1.3819, G loss: 0.6479\n",
      "[1284/1762] D loss: 1.1558, G loss: 0.8441\n",
      "[1364/1762] D loss: 1.1621, G loss: 0.8394\n",
      "[1444/1762] D loss: 1.3872, G loss: 0.6648\n",
      "[1524/1762] D loss: 1.3940, G loss: 0.6579\n",
      "[1604/1762] D loss: 1.4066, G loss: 0.6826\n",
      "[1684/1762] D loss: 1.3927, G loss: 0.6646\n",
      "[1762/1762] D loss: 1.4030, G loss: 0.7194\n",
      "train error: \n",
      " D loss: 1.336720, G loss: 0.725336, D accuracy: 52.4%, cell accuracy: 99.8%, board accuracy: 88.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.319111, G loss: 0.736200, D accuracy: 53.3%, cell accuracy: 99.8%, board accuracy: 86.1% \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.1663, G loss: 0.7113\n",
      "[84/1762] D loss: 2.5327, G loss: 0.4520\n",
      "[164/1762] D loss: 1.5399, G loss: 0.6157\n",
      "[244/1762] D loss: 1.5246, G loss: 0.5173\n",
      "[324/1762] D loss: 1.3293, G loss: 0.7303\n",
      "[404/1762] D loss: 1.2664, G loss: 0.8677\n",
      "[484/1762] D loss: 1.2069, G loss: 0.8011\n",
      "[564/1762] D loss: 1.1199, G loss: 1.0466\n",
      "[644/1762] D loss: 1.0287, G loss: 0.8650\n",
      "[724/1762] D loss: 0.7481, G loss: 1.8115\n",
      "[804/1762] D loss: 1.3813, G loss: 0.7502\n",
      "[884/1762] D loss: 1.3946, G loss: 0.4878\n",
      "[964/1762] D loss: 1.4033, G loss: 0.8835\n",
      "[1044/1762] D loss: 1.4471, G loss: 0.5211\n",
      "[1124/1762] D loss: 1.3990, G loss: 0.7304\n",
      "[1204/1762] D loss: 1.3996, G loss: 0.6596\n",
      "[1284/1762] D loss: 1.3971, G loss: 0.6732\n",
      "[1364/1762] D loss: 1.5855, G loss: 0.5856\n",
      "[1444/1762] D loss: 1.3827, G loss: 0.7019\n",
      "[1524/1762] D loss: 1.7126, G loss: 0.7098\n",
      "[1604/1762] D loss: 1.3857, G loss: 0.7364\n",
      "[1684/1762] D loss: 1.5582, G loss: 0.8455\n",
      "[1762/1762] D loss: 1.3953, G loss: 0.6563\n",
      "train error: \n",
      " D loss: 1.439910, G loss: 0.723263, D accuracy: 49.4%, cell accuracy: 99.8%, board accuracy: 85.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.459716, G loss: 0.723417, D accuracy: 48.3%, cell accuracy: 99.8%, board accuracy: 81.8% \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3990, G loss: 0.7699\n",
      "[84/1762] D loss: 1.4015, G loss: 0.7431\n",
      "[164/1762] D loss: 1.3321, G loss: 0.7312\n",
      "[244/1762] D loss: 1.4833, G loss: 0.5886\n",
      "[324/1762] D loss: 1.3945, G loss: 0.6604\n",
      "[404/1762] D loss: 1.3849, G loss: 0.6691\n",
      "[484/1762] D loss: 1.5833, G loss: 0.6709\n",
      "[564/1762] D loss: 1.3961, G loss: 0.7549\n",
      "[644/1762] D loss: 1.3959, G loss: 0.6279\n",
      "[724/1762] D loss: 1.3886, G loss: 0.7117\n",
      "[804/1762] D loss: 1.3901, G loss: 0.7384\n",
      "[884/1762] D loss: 1.3953, G loss: 0.6599\n",
      "[964/1762] D loss: 1.3887, G loss: 0.7295\n",
      "[1044/1762] D loss: 1.3898, G loss: 0.6897\n",
      "[1124/1762] D loss: 1.3952, G loss: 0.6331\n",
      "[1204/1762] D loss: 1.3897, G loss: 0.7195\n",
      "[1284/1762] D loss: 1.3900, G loss: 0.6917\n",
      "[1364/1762] D loss: 1.3941, G loss: 0.6654\n",
      "[1444/1762] D loss: 1.4146, G loss: 0.6155\n",
      "[1524/1762] D loss: 1.3888, G loss: 0.7121\n",
      "[1604/1762] D loss: 1.4394, G loss: 0.7048\n",
      "[1684/1762] D loss: 1.3920, G loss: 0.7660\n",
      "[1762/1762] D loss: 1.3958, G loss: 0.6193\n",
      "train error: \n",
      " D loss: 1.376844, G loss: 0.668243, D accuracy: 52.5%, cell accuracy: 99.8%, board accuracy: 89.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.378004, G loss: 0.667870, D accuracy: 53.2%, cell accuracy: 99.8%, board accuracy: 86.8% \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4042, G loss: 0.6371\n",
      "[84/1762] D loss: 1.3037, G loss: 0.7658\n",
      "[164/1762] D loss: 1.3867, G loss: 0.6736\n",
      "[244/1762] D loss: 1.2326, G loss: 0.7605\n",
      "[324/1762] D loss: 1.3895, G loss: 0.6410\n",
      "[404/1762] D loss: 1.3957, G loss: 0.7190\n",
      "[484/1762] D loss: 1.3937, G loss: 0.7024\n",
      "[564/1762] D loss: 1.3890, G loss: 0.6594\n",
      "[644/1762] D loss: 1.3890, G loss: 0.6809\n",
      "[724/1762] D loss: 1.2956, G loss: 0.7073\n",
      "[804/1762] D loss: 1.3879, G loss: 0.7311\n",
      "[884/1762] D loss: 1.2723, G loss: 0.7876\n",
      "[964/1762] D loss: 1.3864, G loss: 0.7039\n",
      "[1044/1762] D loss: 1.3889, G loss: 0.6616\n",
      "[1124/1762] D loss: 1.3933, G loss: 0.7871\n",
      "[1204/1762] D loss: 1.2563, G loss: 0.7554\n",
      "[1284/1762] D loss: 1.4122, G loss: 0.7126\n",
      "[1364/1762] D loss: 1.3855, G loss: 0.6217\n",
      "[1444/1762] D loss: 1.3889, G loss: 0.6610\n",
      "[1524/1762] D loss: 1.3863, G loss: 0.6705\n",
      "[1604/1762] D loss: 1.3898, G loss: 0.7012\n",
      "[1684/1762] D loss: 1.3913, G loss: 0.7383\n",
      "[1762/1762] D loss: 1.3874, G loss: 0.6536\n",
      "train error: \n",
      " D loss: 1.361521, G loss: 0.651729, D accuracy: 53.5%, cell accuracy: 99.9%, board accuracy: 91.9% \n",
      "\n",
      "test error: \n",
      " D loss: 1.358201, G loss: 0.652772, D accuracy: 54.3%, cell accuracy: 99.8%, board accuracy: 89.1% \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3946, G loss: 0.6634\n",
      "[84/1762] D loss: 1.3870, G loss: 0.6922\n",
      "[164/1762] D loss: 1.3903, G loss: 0.6813\n",
      "[244/1762] D loss: 1.3281, G loss: 0.6300\n",
      "[324/1762] D loss: 1.3931, G loss: 0.7429\n",
      "[404/1762] D loss: 1.3944, G loss: 0.6223\n",
      "[484/1762] D loss: 1.3888, G loss: 0.7502\n",
      "[564/1762] D loss: 1.2404, G loss: 0.7182\n",
      "[644/1762] D loss: 1.3890, G loss: 0.6295\n",
      "[724/1762] D loss: 1.3872, G loss: 0.6379\n",
      "[804/1762] D loss: 1.3929, G loss: 0.7610\n",
      "[884/1762] D loss: 1.3870, G loss: 0.7089\n",
      "[964/1762] D loss: 1.2055, G loss: 0.8315\n",
      "[1044/1762] D loss: 1.3884, G loss: 0.6999\n",
      "[1124/1762] D loss: 1.3972, G loss: 0.6272\n",
      "[1204/1762] D loss: 1.3930, G loss: 0.7305\n",
      "[1284/1762] D loss: 1.3943, G loss: 0.7094\n",
      "[1364/1762] D loss: 1.3922, G loss: 0.6976\n",
      "[1444/1762] D loss: 1.3920, G loss: 0.7264\n",
      "[1524/1762] D loss: 1.3884, G loss: 0.7248\n",
      "[1604/1762] D loss: 1.3885, G loss: 0.6908\n",
      "[1684/1762] D loss: 1.3948, G loss: 0.6303\n",
      "[1762/1762] D loss: 1.3932, G loss: 0.8161\n",
      "train error: \n",
      " D loss: 1.358472, G loss: 0.853293, D accuracy: 53.3%, cell accuracy: 99.9%, board accuracy: 91.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.349074, G loss: 0.854965, D accuracy: 54.1%, cell accuracy: 99.8%, board accuracy: 89.8% \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.4201, G loss: 0.8656\n",
      "[84/1762] D loss: 1.3913, G loss: 0.6882\n",
      "[164/1762] D loss: 1.2259, G loss: 0.7508\n",
      "[244/1762] D loss: 1.3916, G loss: 0.6425\n",
      "[324/1762] D loss: 1.3947, G loss: 0.7412\n",
      "[404/1762] D loss: 1.4025, G loss: 0.7907\n",
      "[484/1762] D loss: 1.3981, G loss: 0.7036\n",
      "[564/1762] D loss: 1.3926, G loss: 0.7469\n",
      "[644/1762] D loss: 1.3987, G loss: 0.6137\n",
      "[724/1762] D loss: 1.3939, G loss: 0.7533\n",
      "[804/1762] D loss: 1.3894, G loss: 0.6555\n",
      "[884/1762] D loss: 1.3876, G loss: 0.7477\n",
      "[964/1762] D loss: 1.3894, G loss: 0.6351\n",
      "[1044/1762] D loss: 1.1907, G loss: 0.7703\n",
      "[1124/1762] D loss: 1.2527, G loss: 0.7210\n",
      "[1204/1762] D loss: 1.3949, G loss: 0.7391\n",
      "[1284/1762] D loss: 1.3974, G loss: 0.6614\n",
      "[1364/1762] D loss: 1.3930, G loss: 0.7352\n",
      "[1444/1762] D loss: 1.3867, G loss: 0.6478\n",
      "[1524/1762] D loss: 1.3903, G loss: 0.7263\n",
      "[1604/1762] D loss: 1.3896, G loss: 0.6933\n",
      "[1684/1762] D loss: 1.2390, G loss: 0.9644\n",
      "[1762/1762] D loss: 1.3982, G loss: 0.6415\n",
      "train error: \n",
      " D loss: 1.333588, G loss: 0.739035, D accuracy: 53.5%, cell accuracy: 99.9%, board accuracy: 92.2% \n",
      "\n",
      "test error: \n",
      " D loss: 1.321748, G loss: 0.741044, D accuracy: 54.4%, cell accuracy: 99.8%, board accuracy: 90.0% \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3918, G loss: 0.7708\n",
      "[84/1762] D loss: 1.3880, G loss: 0.6726\n",
      "[164/1762] D loss: 1.4067, G loss: 0.7055\n",
      "[244/1762] D loss: 1.3941, G loss: 0.7673\n",
      "[324/1762] D loss: 1.3891, G loss: 0.6756\n",
      "[404/1762] D loss: 1.3947, G loss: 0.7533\n",
      "[484/1762] D loss: 1.3894, G loss: 0.7437\n",
      "[564/1762] D loss: 1.4061, G loss: 0.7629\n",
      "[644/1762] D loss: 1.5466, G loss: 0.6448\n",
      "[724/1762] D loss: 1.8662, G loss: 0.4884\n",
      "[804/1762] D loss: 1.4899, G loss: 0.6775\n",
      "[884/1762] D loss: 1.2213, G loss: 0.8908\n",
      "[964/1762] D loss: 1.1324, G loss: 1.0564\n",
      "[1044/1762] D loss: 0.9479, G loss: 0.8067\n",
      "[1124/1762] D loss: 1.0417, G loss: 0.9896\n",
      "[1204/1762] D loss: 1.4137, G loss: 0.4993\n",
      "[1284/1762] D loss: 1.3907, G loss: 0.7383\n",
      "[1364/1762] D loss: 1.4090, G loss: 0.7508\n",
      "[1444/1762] D loss: 1.3898, G loss: 0.6969\n",
      "[1524/1762] D loss: 1.3815, G loss: 0.6767\n",
      "[1604/1762] D loss: 1.3853, G loss: 0.6204\n",
      "[1684/1762] D loss: 1.4017, G loss: 0.7464\n",
      "[1762/1762] D loss: 1.3881, G loss: 0.6826\n",
      "train error: \n",
      " D loss: 1.434148, G loss: 0.633357, D accuracy: 47.7%, cell accuracy: 99.8%, board accuracy: 89.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.448025, G loss: 0.637235, D accuracy: 47.2%, cell accuracy: 99.8%, board accuracy: 86.8% \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.5124, G loss: 0.6799\n",
      "[84/1762] D loss: 1.4013, G loss: 0.6396\n",
      "[164/1762] D loss: 1.3915, G loss: 0.7570\n",
      "[244/1762] D loss: 1.3896, G loss: 0.6263\n",
      "[324/1762] D loss: 1.3963, G loss: 0.7270\n",
      "[404/1762] D loss: 1.3929, G loss: 0.7556\n",
      "[484/1762] D loss: 1.3924, G loss: 0.6608\n",
      "[564/1762] D loss: 1.4040, G loss: 0.7241\n",
      "[644/1762] D loss: 1.3951, G loss: 0.6216\n",
      "[724/1762] D loss: 1.3921, G loss: 0.6506\n",
      "[804/1762] D loss: 1.3518, G loss: 0.6355\n",
      "[884/1762] D loss: 1.3900, G loss: 0.7290\n",
      "[964/1762] D loss: 1.4020, G loss: 0.6867\n",
      "[1044/1762] D loss: 1.3885, G loss: 0.7209\n",
      "[1124/1762] D loss: 1.3771, G loss: 0.7297\n",
      "[1204/1762] D loss: 1.3874, G loss: 0.7109\n",
      "[1284/1762] D loss: 1.2974, G loss: 0.7568\n",
      "[1364/1762] D loss: 1.3860, G loss: 0.7018\n",
      "[1444/1762] D loss: 1.3866, G loss: 0.7170\n",
      "[1524/1762] D loss: 1.3305, G loss: 0.6709\n",
      "[1604/1762] D loss: 1.3898, G loss: 0.7686\n",
      "[1684/1762] D loss: 1.3870, G loss: 0.7097\n",
      "[1762/1762] D loss: 1.3881, G loss: 0.6547\n",
      "train error: \n",
      " D loss: 1.377327, G loss: 0.640498, D accuracy: 52.6%, cell accuracy: 99.9%, board accuracy: 91.8% \n",
      "\n",
      "test error: \n",
      " D loss: 1.379783, G loss: 0.641940, D accuracy: 52.0%, cell accuracy: 99.8%, board accuracy: 88.9% \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3921, G loss: 0.6186\n",
      "[84/1762] D loss: 1.3171, G loss: 0.7797\n",
      "[164/1762] D loss: 1.3880, G loss: 0.6443\n",
      "[244/1762] D loss: 1.3344, G loss: 0.7836\n",
      "[324/1762] D loss: 1.3874, G loss: 0.7105\n",
      "[404/1762] D loss: 1.3866, G loss: 0.6836\n",
      "[484/1762] D loss: 1.1779, G loss: 0.7887\n",
      "[564/1762] D loss: 1.3868, G loss: 0.7068\n",
      "[644/1762] D loss: 1.4035, G loss: 0.7082\n",
      "[724/1762] D loss: 1.3872, G loss: 0.7027\n",
      "[804/1762] D loss: 1.3876, G loss: 0.7014\n",
      "[884/1762] D loss: 1.3865, G loss: 0.6915\n",
      "[964/1762] D loss: 1.2675, G loss: 0.6903\n",
      "[1044/1762] D loss: 1.2632, G loss: 0.7335\n",
      "[1124/1762] D loss: 1.3880, G loss: 0.7079\n",
      "[1204/1762] D loss: 1.2106, G loss: 0.8475\n",
      "[1284/1762] D loss: 1.3858, G loss: 0.6989\n",
      "[1364/1762] D loss: 1.3889, G loss: 0.6966\n",
      "[1444/1762] D loss: 1.3874, G loss: 0.6937\n",
      "[1524/1762] D loss: 1.3892, G loss: 0.6920\n",
      "[1604/1762] D loss: 1.3993, G loss: 0.6531\n",
      "[1684/1762] D loss: 1.3877, G loss: 0.6509\n",
      "[1762/1762] D loss: 1.3866, G loss: 0.6696\n",
      "train error: \n",
      " D loss: 1.351257, G loss: 0.677811, D accuracy: 53.7%, cell accuracy: 99.9%, board accuracy: 92.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.346126, G loss: 0.679962, D accuracy: 54.5%, cell accuracy: 99.8%, board accuracy: 89.8% \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3982, G loss: 0.6102\n",
      "[84/1762] D loss: 1.2666, G loss: 0.8210\n",
      "[164/1762] D loss: 1.3927, G loss: 0.7381\n",
      "[244/1762] D loss: 1.3902, G loss: 0.7179\n",
      "[324/1762] D loss: 1.2213, G loss: 0.7932\n",
      "[404/1762] D loss: 1.3868, G loss: 0.6806\n",
      "[484/1762] D loss: 1.4049, G loss: 0.6155\n",
      "[564/1762] D loss: 1.3922, G loss: 0.7249\n",
      "[644/1762] D loss: 1.3910, G loss: 0.7354\n",
      "[724/1762] D loss: 1.3869, G loss: 0.7119\n",
      "[804/1762] D loss: 1.3881, G loss: 0.6743\n",
      "[884/1762] D loss: 1.0473, G loss: 0.8054\n",
      "[964/1762] D loss: 1.2019, G loss: 0.8058\n",
      "[1044/1762] D loss: 1.3882, G loss: 0.6897\n",
      "[1124/1762] D loss: 1.3905, G loss: 0.7180\n",
      "[1204/1762] D loss: 1.4157, G loss: 0.8389\n",
      "[1284/1762] D loss: 1.3897, G loss: 0.7233\n",
      "[1364/1762] D loss: 1.3926, G loss: 0.7380\n",
      "[1444/1762] D loss: 1.3896, G loss: 0.7523\n",
      "[1524/1762] D loss: 0.9829, G loss: 0.9573\n",
      "[1604/1762] D loss: 1.4015, G loss: 0.6361\n",
      "[1684/1762] D loss: 0.9735, G loss: 1.0113\n",
      "[1762/1762] D loss: 1.3640, G loss: 0.8305\n",
      "train error: \n",
      " D loss: 1.358520, G loss: 0.639971, D accuracy: 52.7%, cell accuracy: 99.8%, board accuracy: 80.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.349930, G loss: 0.642669, D accuracy: 53.9%, cell accuracy: 99.8%, board accuracy: 78.9% \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3964, G loss: 0.6076\n",
      "[84/1762] D loss: 1.4316, G loss: 0.6623\n",
      "[164/1762] D loss: 1.4937, G loss: 0.6594\n",
      "[244/1762] D loss: 1.3176, G loss: 0.7630\n",
      "[324/1762] D loss: 1.4501, G loss: 0.6844\n",
      "[404/1762] D loss: 1.2379, G loss: 0.8477\n",
      "[484/1762] D loss: 1.3252, G loss: 0.8913\n",
      "[564/1762] D loss: 1.2257, G loss: 0.7836\n",
      "[644/1762] D loss: 1.3350, G loss: 0.6323\n",
      "[724/1762] D loss: 1.2628, G loss: 0.8559\n",
      "[804/1762] D loss: 1.3876, G loss: 0.7102\n",
      "[884/1762] D loss: 1.3885, G loss: 0.7258\n",
      "[964/1762] D loss: 1.2431, G loss: 0.7581\n",
      "[1044/1762] D loss: 1.3137, G loss: 0.6369\n",
      "[1124/1762] D loss: 1.2464, G loss: 0.7004\n",
      "[1204/1762] D loss: 1.3910, G loss: 0.7400\n",
      "[1284/1762] D loss: 1.3906, G loss: 0.6495\n",
      "[1364/1762] D loss: 1.3877, G loss: 0.7280\n",
      "[1444/1762] D loss: 1.3932, G loss: 0.7819\n",
      "[1524/1762] D loss: 1.3883, G loss: 0.7237\n",
      "[1604/1762] D loss: 1.3902, G loss: 0.6798\n",
      "[1684/1762] D loss: 1.3881, G loss: 0.6385\n",
      "[1762/1762] D loss: 1.3811, G loss: 0.6875\n",
      "train error: \n",
      " D loss: 1.344431, G loss: 0.687242, D accuracy: 53.6%, cell accuracy: 99.8%, board accuracy: 73.4% \n",
      "\n",
      "test error: \n",
      " D loss: 1.336295, G loss: 0.693225, D accuracy: 55.2%, cell accuracy: 99.7%, board accuracy: 73.6% \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "[4/1762] D loss: 1.3882, G loss: 0.6307\n",
      "[84/1762] D loss: 1.2639, G loss: 0.7288\n",
      "[164/1762] D loss: 1.3901, G loss: 0.6588\n",
      "[244/1762] D loss: 0.9948, G loss: 0.9191\n",
      "[324/1762] D loss: 1.3909, G loss: 0.7082\n",
      "[404/1762] D loss: 1.4031, G loss: 0.8170\n",
      "[484/1762] D loss: 1.3876, G loss: 0.6904\n",
      "[564/1762] D loss: 1.3899, G loss: 0.6582\n",
      "[644/1762] D loss: 1.2026, G loss: 0.8089\n",
      "[724/1762] D loss: 1.4121, G loss: 0.6206\n",
      "[804/1762] D loss: 1.3931, G loss: 0.6946\n",
      "[884/1762] D loss: 1.4073, G loss: 0.8045\n",
      "[964/1762] D loss: 1.3942, G loss: 0.7855\n",
      "[1044/1762] D loss: 1.1990, G loss: 0.7535\n",
      "[1124/1762] D loss: 1.3947, G loss: 0.7249\n",
      "[1204/1762] D loss: 1.3910, G loss: 0.7695\n",
      "[1284/1762] D loss: 1.2520, G loss: 0.7099\n",
      "[1364/1762] D loss: 1.3891, G loss: 0.6942\n",
      "[1444/1762] D loss: 1.2485, G loss: 0.7081\n",
      "[1524/1762] D loss: 1.3913, G loss: 0.6847\n",
      "[1604/1762] D loss: 1.3938, G loss: 0.7327\n",
      "[1684/1762] D loss: 1.3894, G loss: 0.7617\n",
      "[1762/1762] D loss: 1.3938, G loss: 0.7966\n",
      "train error: \n",
      " D loss: 1.333707, G loss: 0.807903, D accuracy: 53.6%, cell accuracy: 99.9%, board accuracy: 90.1% \n",
      "\n",
      "test error: \n",
      " D loss: 1.318953, G loss: 0.815490, D accuracy: 54.7%, cell accuracy: 99.8%, board accuracy: 87.7% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train(run_name=\"normal_noise\", noise_type=\"normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The board accuracy and spawn recall curves look basically the same with Gaussian noise as they do with uniform random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "There seems to be little, if any, advantage to using Gaussian random noise instead of uniform noise in the generator, so let's stick with uniform.\n",
    "\n",
    "Another thing to try varying in a later experiment would be the number of random noise inputs to the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
